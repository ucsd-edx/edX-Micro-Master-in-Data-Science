{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" style=\"color:red;\"> **IMPORTANT: ** Only modify cells which have the following comment</font>\n",
    "\n",
    "```python\n",
    "# modify this cell\n",
    "```\n",
    "\n",
    "<font style=\"color:red;\"> Do not add any new cells when submitting homework. To test out new code, use the coding **scratchpad** by clicking the triangular icon in the bottom right corner of the screen. (**hotkey:** control-B)  </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regressing to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import arange,array,ones,linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This notebook is a an intuitive introduction to the concept of linear regression. Partially based on Chapter 10 in the book [**Statistics** by Freedman, Pisani and Purves](https://www.amazon.com/Statistics-4th-David-Freedman/dp/0393929728) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A real-life example\n",
    "http://www.math.uah.edu/stat/data/Pearson.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HH=pd.read_csv('../data/Pearson.csv')\n",
    "HH.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Copute some basic descritive statistics.\n",
    "HH.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array(HH['Father'])\n",
    "A=np.array([ones(len(A)),A])\n",
    "y=np.array(HH['Son'])\n",
    "A.shape,y.shape\n",
    "\n",
    "w1 = linalg.lstsq(A.T,y)[0] # finding the optimal parameters\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,w):\n",
    "    return w[0]+w[1]*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax=HH.plot(kind='scatter',s=1,x='Father',y='Son',figsize=[10,8]);\n",
    "x0,x1=xlim()\n",
    "ax.plot([x0,x1],[f(x0,w1),f(x1,w1)],'r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HH['Son-Father']=HH['Son']-HH['Father']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array(HH['Father'])\n",
    "A=np.array([ones(len(A)),A])\n",
    "y=np.array(HH['Son-Father'])\n",
    "A.shape,y.shape\n",
    "\n",
    "w2 = linalg.lstsq(A.T,y)[0] # finding the optimal parameters\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=HH.plot(kind='scatter',s=1,x='Father',y='Son-Father',figsize=[10,8]);\n",
    "x0,x1=xlim()\n",
    "ax.plot([x0,x1],[f(x0,w2),f(x1,w2)],'r');\n",
    "grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Excercises\n",
    "\n",
    "It is now your turn to do regression on a dataset. Consider $1000$ datapoints with random variables $x$ and $y$ both sampled from a normal (gaussian) distribution:\n",
    "$$ x,y \\text{ ~ } \\mathcal{N}(0,1)  $$\n",
    "\n",
    "Notice that the graph below looks like a blob, with no clear relationship between  $x$ and $y$. This leads intuitively to the idea that $x$ and $y$ are independent from each other. In this particualr case (not always true) this intuition is correct, $x$ and $y$ are mathematically independent variables.\n",
    "\n",
    "If we were to try to perform linear regression on this data (the <font style=\"color:red\">red line</font>), we would find rather boring results:\n",
    "$$ y = 0 + 0 \\cdot x = 0 $$\n",
    "\n",
    "This is due to the fact that $y$ has mean 0 and is independent of $x$. What do you think would happen if we did:\n",
    "$$ y-x = w_0 + w_1 \\cdot x $$\n",
    "\n",
    "Is $y$ independent of $x$? Will the graph look like a blob? Will $y-x = 0 ?$ Lets find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW=pd.read_csv('../data/hw/gauss_R2.csv')\n",
    "\n",
    "ax= HW.plot(kind='scatter',s=1.4,x=\"x\",y=\"y\");\n",
    "x0,x1=xlim()\n",
    "ax.plot([x0,x1],[0,0],'r');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Put $\\;y-x\\;$ in Dataframe\n",
    "\n",
    "This first exercise's goal is to familarize you with the [pandas](http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html) package.\n",
    "\n",
    "Your task is to make a new column on the pandas dataframe `HW` where each element of the new column is equal to $y-x$. Write a function **y_minus_x** that will create the new column, make sure the column's name is \"y-x\" . Notice that the function DOES NOT return any output, this is because the `HW` dataframe will be edited \"in-place\".\n",
    "\n",
    "<font  style=\"color:blue\"> * **Code:** *</font>\n",
    "```python\n",
    "print y_minus_x(HW)\n",
    "print HW.shape\n",
    "print HW.columns\n",
    "HW.head()\n",
    "```\n",
    "\n",
    "<font  style=\"color:magenta\"> * **Output:** *</font>\n",
    "```\n",
    "None\n",
    "(1000, 3)\n",
    "Index([u'x', u'y', u'y-x'], dtype='object')\n",
    "```\n",
    "<img src=\"../data/hw/reg_mean_pic.png\"  style=\"width: 220px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3cb6aac4ad5033a8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def y_minus_x(HW):\n",
    "    # input: the HW's dataset\n",
    "    # output: there is NO OUTPUT\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    HW[\"y-x\"] = HW[\"y\"] - HW[\"x\"]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2c10f592fe63462c",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check Answer\n",
    "HW=pd.read_csv('../data/hw/gauss_R2.csv')\n",
    "assert y_minus_x(HW) == None \n",
    "assert HW.shape == (1000, 3)\n",
    "assert all( HW.columns.values == array(['x', 'y', 'y-x']) )\n",
    "assert sum(abs( array(HW[\"y-x\"])[0:10] - \\\n",
    "               array([0.36236278,-2.37726552,-0.36501525,0.05449746,-0.27705517,1.80807383,-2.07001209,\n",
    "                      -0.67536514,0.67519959,  0.97277652])   )) < 10**-5\n",
    "\n",
    "# Create Graph\n",
    "ax= HW.plot(kind='scatter',s=1.4,x=\"x\",y=\"y-x\",figsize=[10,8]);\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert sum(abs( array(HW[\"y-x\"])[60:70] - \\\n",
    "                array([-1.46231473, -0.35256032,  0.50213927, -2.98962366, -1.98911633,\n",
    "                       1.19767907, -0.2104474 ,  1.71587283,  1.50967089, -0.37887511])    )) < 10**-5\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the graph that there is a clear negative relationship between $y-x$ and $x$. As you might have guessed, $y-x$ and $x$ are NOT independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use linear regression to find the relationship between $y-x$ and $x$ but before we do that, we can derive this mathematicaly. Remember from before, we showed:\n",
    "$$ y = 0 + 0 \\cdot x $$\n",
    "\n",
    "Now, we're doing linear regression to find:\n",
    "$$ \\;\\;\\;\\;\\;\\;y-x = w_0 + w_1 \\cdot x $$\n",
    "$$ \\iff y = w_0 + (1+w_1) \\cdot x $$\n",
    "\n",
    "Comparing the two eqations for $y$, you can deduce that $w_0=0$ and $w_1=-1$ . Now lets see how close our sampled data is to the true equation for the distribution!\n",
    "\n",
    "$\\;$\n",
    "<font style=\"color:red\">*Technical Note:*</font> The derivation is mathematically sound only because $y-x$ is also a gaussian random variable. The sum of two gaussians is a gaussian. Under the laws of linear regression this ensures both the regressors of $y$ and $y-x$ have zero expected error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Simple Linear Regression\n",
    "\n",
    "Write a function, **do_regression**, that performs linear regression to return \n",
    "$ \\;\n",
    " {\\bf w} = \\begin{pmatrix}\n",
    " w_0\\\\\n",
    " w_1\n",
    " \\end{pmatrix}\n",
    "$\n",
    "for the equation:\n",
    "\n",
    "$$\n",
    "\\text{y-x}= w_0 + w_1 \\cdot \\text{x}\n",
    "$$\n",
    "\n",
    "\n",
    "<font  style=\"color:blue\"> * **Code:** *</font>\n",
    "```python\n",
    "w = do_regression(HW)\n",
    "print type(w)\n",
    "print w.shape\n",
    "```\n",
    "\n",
    "<font  style=\"color:magenta\"> * **Output:** *</font>\n",
    "```\n",
    "<type 'numpy.ndarray'>\n",
    "(2,)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-39abbe5dc9828437",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# modify this cell\n",
    "\n",
    "def do_regression(HW):\n",
    "    # input: the HW's dataset\n",
    "    # output: a numpy array yielding w=(w0,w1) from linear regression\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    A= array(HW['x'])\n",
    "    A= array([ones(len(A)),A])\n",
    "    y= array(HW['y-x'])\n",
    "    return linalg.lstsq(A.T,y)[0]\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-03243a26bbfa4f6f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Check Answer\n",
    "w_hw = do_regression(HW)\n",
    "assert str(type(w_hw)) == \"<type 'numpy.ndarray'>\"\n",
    "assert w_hw.shape == (2,)\n",
    "assert sum(abs(w_hw[0] - 0.02173387)) < 10**-5\n",
    "\n",
    "# Plot Graph\n",
    "print \"LINEAR REGRESSION EQUATION:\"\n",
    "print \"y-x = {:.2f} + {:.2f} x\".format(w_hw[0],w_hw[1])\n",
    "ax= HW.plot(kind='scatter',s=1.4,x=\"x\",y=\"y-x\",figsize=[10,8]);\n",
    "x0,x1=xlim()\n",
    "ax.plot([x0,x1],[f(x0,w_hw),f(x1,w_hw)],'k');\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "assert sum(abs(w_hw[1] - -1.01071279)) < 10**-5\n",
    "### END HIDDEN TESTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-02f8256cb6d416ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "67px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
