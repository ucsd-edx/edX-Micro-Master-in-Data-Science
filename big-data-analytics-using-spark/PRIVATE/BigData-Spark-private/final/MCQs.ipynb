{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-1: Spark Transformations\n",
    "\n",
    "CSE and DSE\n",
    "\n",
    "Which of the following are properties of Spark transformations?\n",
    "1. They are not computed right away\n",
    "2. They are computed right away\n",
    "3. They are vulnerable to machine failures\n",
    "4. They define an execution plan, rather than a data structure in memory.\n",
    "\n",
    "Correct: 1,4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-2: Resilient Distributed Datasets\n",
    "\n",
    "CSE\n",
    "\n",
    "Which of the following is **not** a property of RDDs?\n",
    "1. They can be changed after they are constructed \n",
    "2. They can be created by transformations applied to existing RDDs\n",
    "3. They enable parallel operations on collections of distributed data\n",
    "4. They track lineage information to enable efficient recomputation of lost data.\n",
    "5. They always reside in memory.\n",
    "\n",
    "Correct: 1,5\n",
    "\n",
    "\n",
    "RDDs cannot be changed once they are created - they are immutable. You can create RDDs by applying transformations to existing RDDs and Spark automatically tracks how you create and manipulate RDDs (their lineage) so that it can reconstruct any data that is lost due to slow or failed machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-3: Spark Actions\n",
    "\n",
    "DSE\n",
    "\n",
    "Which of the following is not a property of Spark Actions?\n",
    "1. They cause Spark to execute the recipe to transform the source data\n",
    "2. They are the primary mechanism for getting results out of Spark\n",
    "3. They are lazily evaluated\n",
    "4. The results are returned to the driver\n",
    "\n",
    "Correct: 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-4: Broadcasting variables\n",
    "\n",
    "Which of the following is true?\n",
    "1. In iterative or repeated computations, broadcasted variables avoid the problem of repeatedly sending the same data to workers.\n",
    "2. Workers can modify broadcasted variables and can communicate the change to all other nodes.\n",
    "3. Broadcasting caches the data on workers in deserialized form.\n",
    "4. The value of a broadcasted variable `broadcastVar` can be accessed on workers using `broadcastVar.value`\n",
    "\n",
    "Correct: 1,3,4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-5: Hadoop Map Reduce and Spark Differences\n",
    "\n",
    "Spark is often faster than a traditional MapReduce implementation because:\n",
    "1. It sends less data over the network\n",
    "2. Results do not need to be written to disk\n",
    "3. It detects machine failures more quickly\n",
    "4. It replicates the output of each task to recover from failures quickly\n",
    "5. Results do not need to be serialized\n",
    "\n",
    "Correct: 2,5\n",
    "\n",
    "Spark keeps results in memory so they do not need to be serialized (converted into a format that can be stored on disk) and they do not need to be written to disk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-6: Gradient-Boosted Trees vs. Random Forests\n",
    "\n",
    "Which of the following is true?\n",
    "1. Gradient-Boosted Trees are faster as they train trees in parallel while Random Forests train trees sequentially.\n",
    "2. Random Forests can be less prone to overfitting. Training more trees in a Random Forest reduces the likelihood of overfitting, but training more trees with GBTs increases the likelihood of overfitting.\n",
    "3. Random Forests can be easier to tune since performance improves monotonically with the number of trees (whereas performance can start to decrease for GBTs if the number of trees grows too large).\n",
    "4. If we compare the time required to construct a single tree on a single node, then Random Forests will be faster than Gradient-Boosted Trees.\n",
    "\n",
    "Correct: 2,3,4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-7: PCA distance metric\n",
    "\n",
    "CSE and DSE\n",
    "\n",
    "When working with two dimensional data, if we project data points onto the top principal component \n",
    "(which is a line in 2D space), the distance between the projected points and the original points minimizes \n",
    "which distance?\n",
    "\n",
    "1. vertical distance\n",
    "2. euclidean distance\n",
    "3. manhattan distance\n",
    "4. horizontal distance\n",
    "\n",
    "Correct: 2\n",
    "\n",
    "PCA minimizes the euclidean distance between points and their projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CSE and DSE\n",
    "\n",
    "Consider the following methods for computing the variance of the elements of an RDD X:\n",
    "\n",
    "```python\n",
    "N,S,S2=X.map(lambda x: np.array(1,x,x*x)).reduce(lambda a,b:a+b)\n",
    "print 'variance=',(S2/N)-(S/N)**2\n",
    "\n",
    "N,S = X.map(lambda x: np.array(1,x)).reduce(lambda a,b:a+b)\n",
    "mean=S/N\n",
    "S2 = X.map(lambda x: (x-mean)*(x-mean)).reduce(lambda a,b:a+b)\n",
    "print 'variance=',S2/N\n",
    "```\n",
    "\n",
    "Which of these methods is faster? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-1 What is lazy evaluation and why is it more efficient?\n",
    "\n",
    "The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently â€“ for example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-2 Consider the following methods of estimating pi. Which one is more efficient and why?\n",
    "\n",
    "sc refers to SparkContext object\n",
    "\n",
    "#### Method 1\n",
    "\n",
    "\n",
    "```python\n",
    "from random import random\n",
    "def sampleAndSum(p):\n",
    "    points = [(random(),random()) for i in xrange(p)]\n",
    "    return sum([1 for (a,b) in points if a*a + b*b < 1])\n",
    "\n",
    "def calculate_pi(sc, NUM_SAMPLES):\n",
    "    tasks = sc.defaultParallelism\n",
    "    count = sc.parallelize([NUM_SAMPLES/tasks]*tasks) \\\n",
    "            .map(sampleAndSum) \\\n",
    "            .reduce(lambda a, b: a + b)\n",
    "    return count\n",
    "\n",
    "\n",
    "NUM_SAMPLES=10000000\n",
    "count = calculate_pi(sc, n)\n",
    "print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n",
    "```\n",
    "\n",
    "#### Method 2\n",
    "```python\n",
    "def sample(p):\n",
    "    x, y = random(), random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample) \\\n",
    "             .reduce(lambda a, b: a + b)\n",
    "print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**Answer:** Method 1 is more efficient because it is generating random numbers in parallel at worker nodes while method 2 is generating random samples at driver and then distributing them to workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-3 In assignment 5, you might have got different results by running Gradient Boosted Trees multiple times (with exactly the same training/test sets and same set of hyper-parameters). What might be introducing randomness in the algorithm?\n",
    "\n",
    "It is because Stochastic Gradient Boosting is implemented in Spark. In Stochastic Gradient Boosting, at each iteration a subsample of training data is drawn at random (without replacement) and the loss gradient of this subsample is used to build the next tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
