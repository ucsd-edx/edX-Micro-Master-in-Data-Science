{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataframes \n",
    "Dataframes are a special type of RDDs. They are similar to, but not the same as, pandas dataframes. They are used to store two dimensional data, similar to the type of data stored in a spreadsheet. Each column in a dataframe can have a different type and each row contains a `record`.\n",
    "\n",
    "Spark DataFrames are similar to `pandas` DataFrames. With the important difference that spark DataFrames are **distributed** data structures, based on RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType, BinaryType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x1087174d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, name=u'John'),\n",
       " Row(age=23, name=u'Smith'),\n",
       " Row(age=18, name=u'Sarah')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to create a DataFrame is to first define an RDD from a list of rows\n",
    "some_rdd = sc.parallelize([Row(name=u\"John\", age=19),\n",
    "                           Row(name=u\"Smith\", age=23),\n",
    "                           Row(name=u\"Sarah\", age=18)])\n",
    "some_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The DataFrame is created from the RDD or Rows\n",
    "# Infer schema from the first row, create a DataFrame and print the schema\n",
    "some_df = sqlContext.createDataFrame(some_rdd)\n",
    "some_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'> <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "some_df = [Row(age=19, name=u'John'), Row(age=23, name=u'Smith'), Row(age=18, name=u'Sarah')]\n",
      "some_rdd= [Row(age=19, name=u'John'), Row(age=23, name=u'Smith'), Row(age=18, name=u'Sarah')]\n"
     ]
    }
   ],
   "source": [
    "# A dataframe is an RDD of rows plus information on the schema.\n",
    "# In our case, the content of the RDD is the same as the content of the dataframe. \n",
    "print type(some_rdd),type(some_df)\n",
    "print 'some_df =',some_df.collect()\n",
    "print 'some_rdd=',some_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example of using an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_name: string (nullable = false)\n",
      " |-- person_age: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly\n",
    "another_rdd = sc.parallelize([(\"John\", 19), (\"Smith\", 23), (\"Sarah\", 18)])\n",
    "# Schema with two fields - person_name and person_age\n",
    "schema = StructType([StructField(\"person_name\", StringType(), False),\n",
    "                     StructField(\"person_age\", IntegerType(), False)])\n",
    "\n",
    "# Create a DataFrame by applying the schema to the RDD and print the schema\n",
    "another_df = sqlContext.createDataFrame(another_rdd, schema)\n",
    "another_df.printSchema()\n",
    "# root\n",
    "#  |-- age: binteger (nullable = true)\n",
    "#  |-- name: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What formats does spark-sql support?\n",
    "\n",
    "According to [this post](https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html), spark supports many formats. However, I could not find definite documentation on which \n",
    "formats are supported.\n",
    "\n",
    "In terms of syntax, you can use the format \n",
    "```python\n",
    "sqlContext.read.format('json').load('python/test_support/sql/people.json')\n",
    "```\n",
    "Where instead of `json` you can use `parquet`,`text` and suppposedly other formats, but I could not find an authoritative list of formats. It seems that `csv` is not supported at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Col1, Col2, Col3', u'11,12,13', u'21,22,23', u'31,32,33']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD=sc.textFile('../../Data/example.csv')\n",
    "RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parquet files\n",
    "[Parquet](http://parquet.apache.org/) is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  10 yoavfreund  staff  340 Apr 24 11:05 \u001b[34m../../Data/namesAndFavColors.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 yoavfreund  staff  615 Apr 24 11:05 ../../Data/users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "dir='../../Data'\n",
    "parquet_file=dir+\"/users.parquet\"\n",
    "!ls -ld $dir/*.parquet\n",
    "!rm -rf ../../Data/namesAndFavColors.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load a Parquet file\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- favorite_color: string (nullable = true)\n",
      " |-- favorite_numbers: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|  name|favorite_color|\n",
      "+------+--------------+\n",
      "|Alyssa|          null|\n",
      "|   Ben|           red|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.select(\"name\", \"favorite_color\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  10 yoavfreund  staff  340 Aug  8 14:15 \u001b[34m../../Data/namesAndFavColors.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 yoavfreund  staff  615 Apr 24 11:05 ../../Data/users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "df2.write.save(dir+\"/namesAndFavColors.parquet\")\n",
    "!ls -ld $dir/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing numpy arrays as strings inside a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\"Code for packing and unpacking a numpy array into a byte array.\n",
    "   the array is flattened if it is not 1D.\n",
    "   This is intended to be used as the interface for storing \n",
    "   \n",
    "   This code is intended to be used to store numpy array as fields in a dataframe and then store the \n",
    "   dataframes in a parquet file.\n",
    "\"\"\"\n",
    "\n",
    "def packArray(a):\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "def unpackArray(x,data_type=np.int16):\n",
    "    return np.frombuffer(x,dtype=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytearray(b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packArray(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 697.0  495.0  211.0 ...,  705.0  932.0  481.0]\n",
      " [ 979.0  250.0  797.0 ...,  305.0  617.0  457.0]\n",
      " [ 482.0  224.0  911.0 ...,  727.0  295.0  70.0]\n",
      " ..., \n",
      " [ 648.0  120.0  596.0 ...,  231.0  780.0  284.0]\n",
      " [ 426.0  486.0  325.0 ...,  526.0  377.0  395.0]\n",
      " [ 353.0  714.0  806.0 ...,  255.0  81.0  374.0]]\n",
      "length of buffer= 160000\n",
      "type of buffer= <type 'bytearray'>\n",
      "mapping buffer back: [ 697.0  495.0  211.0 ...,  255.0  81.0  374.0]\n",
      "number of elements in RDD= 100\n",
      "number of elements in dataframe= 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n=100; m=100\n",
    "L=[]\n",
    "data_type=np.float128\n",
    "for i in range(m):\n",
    "    A=np.round(np.random.random([n,n])*1000)\n",
    "    A=np.array(A,dtype=data_type)\n",
    "    B=packArray(A)\n",
    "    L.append(Row(image=B,j=i))\n",
    "\n",
    "    if i==0:\n",
    "        print A\n",
    "        print 'length of buffer=',len(B)\n",
    "        print 'type of buffer=',type(B)\n",
    "\n",
    "        print 'mapping buffer back:',unpackArray(B,data_type=data_type)\n",
    "\n",
    "schema = StructType([StructField(\"image\", BinaryType(), True),\n",
    "                     StructField(\"j\", IntegerType(), False)])\n",
    "\n",
    "RDD=sc.parallelize(L)\n",
    "print 'number of elements in RDD=',RDD.count()\n",
    "df=sqlContext.createDataFrame(RDD, schema)\n",
    "print 'number of elements in dataframe=',df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16128\r\n",
      "-rw-r--r--  1 yoavfreund  staff        0 Aug  8 19:00 _SUCCESS\r\n",
      "-rw-r--r--  1 yoavfreund  staff      284 Aug  8 19:00 _common_metadata\r\n",
      "-rw-r--r--  1 yoavfreund  staff   640749 Aug  8 19:00 _metadata\r\n",
      "-rw-r--r--  1 yoavfreund  staff  3801957 Aug  8 19:00 part-r-00000-3fe7db4e-7767-4175-a51c-6b3309096125.gz.parquet\r\n",
      "-rw-r--r--  1 yoavfreund  staff  3801927 Aug  8 19:00 part-r-00001-3fe7db4e-7767-4175-a51c-6b3309096125.gz.parquet\r\n"
     ]
    }
   ],
   "source": [
    "parquet_file=dir+\"dataFrameWithNumpy.parquet\"\n",
    "!rm -rf $parquet_file\n",
    "df.write.save(parquet_file)\n",
    "\n",
    "!ls -l $parquet_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "160000\n",
      "<type 'bytearray'>\n",
      "<type 'bytearray'>\n",
      "[ 697.0  495.0  211.0 ...,  255.0  81.0  374.0]\n",
      "1\n",
      "160000\n",
      "<type 'bytearray'>\n",
      "<type 'bytearray'>\n",
      "[ 411.0  155.0  330.0 ...,  158.0  878.0  281.0]\n",
      "2\n",
      "160000\n",
      "<type 'bytearray'>\n",
      "<type 'bytearray'>\n",
      "[ 880.0  280.0  404.0 ...,  821.0  880.0  375.0]\n"
     ]
    }
   ],
   "source": [
    "df2 = sqlContext.read.load(parquet_file)\n",
    "\n",
    "LX=df2.take(3)\n",
    "\n",
    "for X in LX:\n",
    "    C=X.image\n",
    "    print X.j\n",
    "    print len(C)\n",
    "    print type(C)\n",
    "    print type(B)\n",
    "    print unpackArray(C,data_type=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
