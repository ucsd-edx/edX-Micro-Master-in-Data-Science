{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from DistributedBoosting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  0  0  0  0  0  0  0  0  0 \n",
      "0  1  1  0  0  0  0  0  0  1 \n",
      "0  0  0  0  1  1  1  1  1  0 \n",
      "0  0  0  0  1  0  1  1  0  0 \n",
      "0  0  0  0  1  1  1  1  0  0 \n",
      "1  0  0  0  1  1  1  1  1  1 \n",
      "0  0  0  0  1  0  1  1  1  0 \n",
      "0  0  0  0  0  0  0  0  0  0 \n",
      "0  0  0  0  0  0  0  0  1  0 \n",
      "0  0  1  0  0  0  0  0  1  0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc=SparkContext()\n",
    "\n",
    "from numpy.random import rand\n",
    "p=0.9\n",
    "data=[]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if np.abs(i-4)<3 and np.abs(j-6)<3:\n",
    "            y=2*(rand()<p)-1\n",
    "        else:\n",
    "            y=2*(rand()>p)-1\n",
    "        print \"%1.0f \"%((1+y)/2),\n",
    "        data.append(LabeledPoint(y,[i,j]))\n",
    "    print\n",
    "\n",
    "dataRDD=sc.parallelize(data,numSlices=2)\n",
    "dataRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load DistributedBoosting.py\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "from string import split,strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"A simple service class to log run time and pretty-print it.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.T=[]\n",
    "    def stamp(self,name):\n",
    "        self.T.append((name,time()))\n",
    "    def str(self):\n",
    "        T=self.T\n",
    "        return '\\n'.join(['%6.2f : %s'%(T[i+1][1]-T[i][1],T[i+1][0]) for i in range(len(T)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Globals\n",
    "global T,iteration,GR,proposals,Strong_Classifier, feature_no, partition_no, Splits_Table\n",
    "global Strong_Classifier,global_best_splitter,PS\n",
    "\n",
    "T=Timer()\n",
    "feature_no=None                 # Tracks processing time\n",
    "global_feature_no=None\n",
    "partition_no=0\n",
    "iteration=0                     # Boosting iteration\n",
    "PS=[None]                       # RDD that hold state of boosting process for each partition.\n",
    "proposals=[]                    # proposed splits for each feature\n",
    "Strong_Classifier=[]            # Combined weak classifiers\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Partition fundctions\n",
    "def Prepare_partition_data_structure(A):\n",
    "\n",
    "    rows=len(A[1])\n",
    "\n",
    "    columns=np.empty([feature_no,rows])\n",
    "    columns[:]=np.NaN\n",
    "    print 'Prepare_partition_data_structure',feature_no,np.shape(columns)\n",
    "    \n",
    "    labels=np.empty(rows)\n",
    "    labels[:]=np.NaN\n",
    "\n",
    "    for j in range(rows):\n",
    "        LP=A[1][j]\n",
    "        labels[j]=LP.label\n",
    "        for i in range(feature_no):\n",
    "            columns[i,j]=LP.features[i]\n",
    "    return {'index':A[0],\\\n",
    "            'labels':labels,\\\n",
    "            'weights':np.ones(len(labels)),\\\n",
    "            'feature_values':columns}\n",
    "\n",
    "def Add_weak_learner_matrix(A):\n",
    "    \"\"\" This procedure adds to each partition the matrix that will be \n",
    "        used to efficiently find the best weak classifier \"\"\"\n",
    "\n",
    "    try:\n",
    "        feature_no\n",
    "    except:\n",
    "        feature_no=global_feature_no.value\n",
    "\n",
    "    index=A['index']%feature_no\n",
    "    SP=Splits_Table.value[index]\n",
    "\n",
    "    Col=A['feature_values'][index,:]\n",
    "\n",
    "    ### The matrix M is organized as follows: \n",
    "    # * There are as many rows as there are thresholds in SP (last one is inf)\n",
    "    # * There are as many columns as there are examples in this partition.\n",
    "    # For threshold i, the i'th rw of M is +1 if Col is smaller than the trehold SP[i] and -1 otherwise\n",
    "\n",
    "    M=np.empty([len(SP),len(Col)])\n",
    "    M[:]=np.NaN\n",
    "\n",
    "    for i in range(len(SP)):\n",
    "        M[i,:]=2*(Col<SP[i])-1\n",
    "\n",
    "    A['M']=M # add M matrix to the data structure.\n",
    "    return A\n",
    "\n",
    "\n",
    "def Find_weak(A):\n",
    "    \"\"\"Find the best split for a single feature on a single partition\"\"\"\n",
    "\n",
    "    try:\n",
    "        feature_no\n",
    "    except:\n",
    "        feature_no=global_feature_no.value\n",
    "\n",
    "    index=A['index']%feature_no\n",
    "    SP=Splits_Table.value[index]\n",
    "\n",
    "    M=A['M']\n",
    "    weights=A['weights']\n",
    "    weighted_Labels=weights*A['labels']\n",
    "    SS=np.dot(M,weighted_Labels)/np.sum(weights)\n",
    "    i_max=np.argmax(np.abs(SS))\n",
    "    answer={'Feature_index':A['index']%feature_no,\\\n",
    "            'Threshold_index':i_max,\\\n",
    "            'Threshold':SP[i_max],\\\n",
    "            'Correlation':SS[i_max],\\\n",
    "            'SS':SS}\n",
    "    return answer\n",
    "\n",
    "# update weights. New splitter is shipped to partition as one of the referenced\n",
    "# Variables\n",
    "\n",
    "def update_weights(A):\n",
    "    \"\"\"Update the weights of the exammples belonging to this \n",
    "    partition according to the new splitter\"\"\"\n",
    "    best_splitter=global_best_splitter\n",
    "    F_index=best_splitter['Feature_index']\n",
    "    Thr=best_splitter['Threshold']\n",
    "    alpha=best_splitter['alpha']\n",
    "    y_hat=2*(A['feature_values'][F_index,:]<Thr)-1\n",
    "    y=A['labels']\n",
    "    weights=A['weights']*exp(-alpha*y_hat*y)\n",
    "    weights /= sum(weights)\n",
    "    A['weights']=weights\n",
    "    return A\n",
    "\n",
    "def calc_scores(Strong_Classifier,Columns,Lbl):\n",
    "    \n",
    "    Scores=np.zeros(len(Lbl))\n",
    "\n",
    "    for h in Strong_Classifier:\n",
    "        index=h['Feature_index']\n",
    "        Thr=h['Threshold']\n",
    "        alpha=h['alpha']\n",
    "        y_hat=2*(Columns[index,:]<Thr)-1\n",
    "        Scores += alpha*y_hat*Lbl\n",
    "    return Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating directory higgs\n",
      "/Users/yoavfreund/academic.papers/Courses/BigDataAnalytics/BigData_spring2016/src/pyspark_notebooks/TreesAndBoosting/higgs\n",
      "downloading HIGGS.csv.gz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  2 2685M    2 75.5M    0     0  6537k      0  0:07:00  0:00:11  0:06:49 6800k^C\n",
      "decompressing HIGGS.csv.gz --- May take 5-10 minutes\n",
      "gunzip: HIGGS.csv.gz: unexpected end of file\n",
      "gunzip: HIGGS.csv.gz: uncompress failed\n",
      "total 168472\n",
      "-rw-r--r--  1 yoavfreund  staff  86257664 May 24 07:03 HIGGS.csv.gz\n",
      "/bin/sh: /root/ephemeral-hdfs/bin/hdfs: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from os.path import exists\n",
    "    if not exists('higgs'):\n",
    "        print \"creating directory higgs\"\n",
    "        get_ipython().system(u'mkdir higgs')\n",
    "    get_ipython().magic(u'cd higgs')\n",
    "    if not exists('HIGGS.csv'):\n",
    "        if not exists('HIGGS.csv.gz'):\n",
    "            print 'downloading HIGGS.csv.gz'\n",
    "            get_ipython().system(u'curl -O http://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')\n",
    "        print 'decompressing HIGGS.csv.gz --- May take 5-10 minutes'\n",
    "        get_ipython().system(u'gunzip -f HIGGS.csv.gz')\n",
    "    get_ipython().system(u'ls -l')\n",
    "\n",
    "    #copy file to HDFS - when runnnig on AWS cluster\n",
    "    get_ipython().system(u'/root/ephemeral-hdfs/bin/hdfs dfs -cp file:///mnt/higgs/HIGGS.csv /HIGGS.csv')\n",
    "\n",
    "def test_globals():\n",
    "    return globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Head-Node functions\n",
    "def init(sc,Data):\n",
    "    \"\"\" Given an RDD with labeled Points, create the RDD of data structures used for boosting\n",
    "    \"\"\"\n",
    "\n",
    "    global T,iteration,GR,proposals,Strong_Classifier, feature_no, partition_no, Splits_Table\n",
    "    global Strong_Classifier,global_best_splitter\n",
    "\n",
    "    T=Timer()\n",
    "    T.stamp('Started')\n",
    "\n",
    "    X=Data.first()\n",
    "    feature_no=len(X.features)\n",
    "#    print 'global_feature_no = sc.broadcast(feature_no)',feature_no\n",
    "    partition_no=Data.getNumPartitions()\n",
    "    if partition_no != feature_no:\n",
    "        Data=Data.repartition(feature_no).cache()\n",
    "    print 'number of features=',feature_no,'number of partitions=',Data.getNumPartitions()\n",
    "\n",
    "    # Split data into training and test\n",
    "    (trainingData,testData)=Data.randomSplit([0.7,0.3])\n",
    "    print 'Sizes: Data1=%d, trainingData=%d, testData=%d'%      (Data.count(),trainingData.cache().count(),testData.cache().count())\n",
    "    T.stamp('Split into train and test')\n",
    "    # Glom each partition into a local array\n",
    "    G=trainingData.glom()\n",
    "    GTest=testData.glom()  \n",
    "    T.stamp('glom')\n",
    "\n",
    "    # Add an index to each partition to identify it.\n",
    "    def f(splitIndex, iterator): yield splitIndex,iterator.next()\n",
    "    GI=G.mapPartitionsWithIndex(f)\n",
    "    GTI=GTest.mapPartitionsWithIndex(f)\n",
    "    T.stamp('add partition index')\n",
    "\n",
    "    return GI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features= 2 number of partitions= 2\n",
      "Sizes: Data1=100, trainingData=67, testData=33\n"
     ]
    }
   ],
   "source": [
    "GI=init(sc,dataRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init2(GI):\n",
    "    # Prepare the data structure for each partition.\n",
    "    GR=GI.map(Prepare_partition_data_structure)\n",
    "    print 'number of elements in GR=', GR.cache().count()\n",
    "    T.stamp('Prepare_partition_data_structure')\n",
    "\n",
    "    #compute the split points for each feature\n",
    "    Splits=find_splits(GR)\n",
    "    print 'Split points=',Splits\n",
    "    T.stamp('Compute Split points')\n",
    "\n",
    "    #broadcast split points\n",
    "    global Splits_Table\n",
    "    Splits_Table=sc.broadcast(Splits)\n",
    "    T.stamp('Broadcast split points')\n",
    "\n",
    "    # Create matrix for each partition to make finding the weak rules correlation a matter of taking a matrix product\n",
    "\n",
    "    iteration=0\n",
    "    global PS\n",
    "    PS[0]=GR.map(Add_weak_learner_matrix)\n",
    "    print 'number of partitions in PS=',PS[0].cache().count()\n",
    "    T.stamp('Add_weak_learner_matrix')\n",
    "\n",
    "    return PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements in GR= 2\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 10, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"DistributedBoosting.py\", line 254, in find_split_points\n    feature_no=global_feature_no.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"DistributedBoosting.py\", line 254, in find_split_points\n    feature_no=global_feature_no.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4686ce57ac69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f736fe8cd55d>\u001b[0m in \u001b[0;36minit2\u001b[0;34m(GI)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#compute the split points for each feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mSplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Split points='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSplits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Compute Split points'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yoavfreund/academic.papers/Courses/BigDataAnalytics/BigData_spring2016/src/pyspark_notebooks/TreesAndBoosting/DistributedBoosting.pyc\u001b[0m in \u001b[0;36mfind_splits\u001b[0;34m(GR, number_of_bins, debug)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mpartition_no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mSplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_split_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0mmax_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yoavfreund/spark-latest/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \"\"\"\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yoavfreund/spark-latest/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yoavfreund/spark-latest/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 10, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"DistributedBoosting.py\", line 254, in find_split_points\n    feature_no=global_feature_no.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/yoavfreund/spark-latest/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"DistributedBoosting.py\", line 254, in find_split_points\n    feature_no=global_feature_no.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "init2(GI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boosting_iteration(k=1):\n",
    "    \"\"\" perform k boosting iterations \"\"\"\n",
    "    for i in range(iteration,iteration+k):\n",
    "        T.stamp('Start main loop %d'%i)\n",
    "\n",
    "        prop=PS[i].map(Find_weak).collect()\n",
    "        proposals.append(prop)\n",
    "        corrs=[p['Correlation'] for p in prop]\n",
    "        best_splitter_index=np.argmax(np.abs(corrs))\n",
    "        best_splitter = prop[best_splitter_index]\n",
    "        Strong_Classifier.append(best_splitter)\n",
    "        global global_Strong_Classifier\n",
    "        global_Strong_Classifier=sc.broadcast(Strong_Classifier)\n",
    "        T.stamp('found best splitter %d'%i)\n",
    "\n",
    "        corr=best_splitter['Correlation']\n",
    "        best_splitter['alpha']=0.5*np.log((1+corr)/(1-corr))\n",
    "        global global_best_splitter\n",
    "        global_best_splitter = sc.broadcast(best_splitter)\n",
    "        PS.append(PS[i].map(update_weights))\n",
    "        T.stamp('Updated Weights %d'%i)\n",
    "    iteration+=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_splits(GR,number_of_bins=10,debug=False):\n",
    "    \"\"\"Compute the split points for each feature to create number_of_bins bins\"\"\"\n",
    "    def find_split_points(A):\n",
    "\n",
    "        try:\n",
    "            feature_no\n",
    "        except:\n",
    "            feature_no=global_feature_no.value\n",
    "\n",
    "        j=A['index'] % feature_no\n",
    "        S=np.sort(A['feature_values'][j,:       ])\n",
    "        L=len(S) \n",
    "        step=L/number_of_bins+2*number_of_bins\n",
    "        return (j,S[range(0,L,step)])\n",
    "\n",
    "    global partition_no\n",
    "    Splits=GR.map(find_split_points).collect()\n",
    "    max_no=np.array([np.finfo(float).max])\n",
    "\n",
    "    # Average the split points across the partitions corresponding to the same feature.\n",
    "    Splits1=[]\n",
    "    for i in range(feature_no):\n",
    "        S=Splits[i][1]\n",
    "        if debug:\n",
    "            print 'no. ',i,' = ',Splits[i]\n",
    "        n=1  # number of copies (for averaging)\n",
    "        j=i+feature_no\n",
    "        while j<partition_no:\n",
    "            if debug:\n",
    "                print 'j=',j\n",
    "            S+=Splits[j][1]\n",
    "            if debug:\n",
    "                print 'no. ',j,' = ',Splits[j]\n",
    "            n+=1.0\n",
    "            j+=feature_no\n",
    "        Splits1.append(np.concatenate([S/n,max_no]))\n",
    "        if debug:\n",
    "            print n\n",
    "            print Splits1[i]\n",
    "            print '='*60\n",
    "\n",
    "    return Splits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
