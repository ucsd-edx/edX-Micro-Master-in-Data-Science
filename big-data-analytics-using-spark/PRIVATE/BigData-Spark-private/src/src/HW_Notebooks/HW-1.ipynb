{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HomeWork 1\n",
    "\n",
    "Unigrams, bigrams, and in general n-grams are 1,2 or n words that appear consecutively in a single sentence. Consider the sentence:\n",
    "\n",
    "    \"to know you is to love you.\"\n",
    "\n",
    "This sentence contains:\n",
    "\n",
    "    Unigrams(single words): to(2 times), know(1 time), you(2 times), is(1 time), love(1 time)\n",
    "    Bigrams: \"to know\",\"know you\",\"you is\", \"is to\",\"to love\", \"love you\" (all 1 time)\n",
    "    Trigrams: \"to know you\", \"know you is\", \"you is to\", \"is to love\", \"to love you\" (all 1 time)\n",
    "\n",
    "The goal of this HW is to find the most common n-grams in the text of Moby Dick.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "* Convert all text to lower case, remove all punctuations. (Finally, the text should contain only letters, numbers and spaces)\n",
    "* Count the occurance of each word and of each 2,3,4,5 - gram\n",
    "* List the 5 most common elements for each order (word, bigram, trigram...). For each element, list the sequence of words and the number of occurances.\n",
    "\n",
    "Basically, you need to change all punctuations to a space and define as a word anything that is between whitespace or at the beginning or the end of a sentence, and does not consist of whitespace (strings consisiting of only white spaces should not be considered as words). The important thing here is to be simple, not to be 100% correct in terms of parsing English. Evaluation will be primarily based on identifying the 5 most frequent n-grams in correct order for all values of n. Some slack will be allowed in the values of frequency of ngrams to allow flexibility in text processing.   \n",
    "\n",
    "This text is short enough to process on a single core using standard python. However, you are required to solve it using RDD's for the whole process. At the very end you can use `.take(5)` to bring the results to the central node for printing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for reading the file and splitting it into sentences is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textRDD = sc.newAPIHadoopFile('../Data/Moby-Dick.txt',\n",
    "                              'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "                              'org.apache.hadoop.io.LongWritable',\n",
    "                              'org.apache.hadoop.io.Text',\n",
    "                               conf={'textinputformat.record.delimiter': \"\\r\\n\\r\\n\"}) \\\n",
    "            .map(lambda x: x[1])\n",
    "\n",
    "sentences=textRDD.flatMap(lambda x: x.split(\". \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "By default, the delimiter string in Spark is \"\\n\". Thus, values in each partition of textRDD describe lines from the file rather than sentences. As a result, sentences may be split over multiple lines. For this input file, a good approach will be to delimit by paragraph so that each value in the RDD is one paragraph (instead of one line). Then we can split the paragraphs into sentences. \n",
    "\n",
    "This is done by setting the `textinputformat.record.delimiter` parameter to `\"\\r\\n\\r\\n\"` in the configuration file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `freq_ngramRDD` be the final result RDD containing the n-grams sorted by their frequency in descending order. Use the following function to print your final output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printOutput(n,freq_ngramRDD):\n",
    "    top=freq_ngramRDD.take(5)\n",
    "    print '\\n============ %d most frequent %d-grams'%(5,n)\n",
    "    print '\\nindex\\tcount\\tngram'\n",
    "    for i in range(5):\n",
    "        print '%d.\\t%d: \\t\"%s\"'%(i+1,top[i][0],' '.join(top[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output for unigrams should look like:\n",
    "```\n",
    "============ 5 most frequent 1-grams\n",
    "\n",
    "index\tcount\tngram\n",
    "1.       40: \t \"a\"\n",
    "2.\t   25: \t \"the\"\n",
    "3.\t   21: \t \"and\"\n",
    "4.\t   16: \t \"to\"\n",
    "5.\t   9:  \t \"of\"\n",
    "\n",
    "```\n",
    "Note: This is just a sample output and does not resemble the actual results in any manner.\n",
    "\n",
    "Your final program should generate an output using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in range(1,6):\n",
    "    # Put your logic for generating the sorted n-gram RDD here and store it in freq_ngramRDD variable\n",
    "    printOutput(n,freq_ngramRDD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
