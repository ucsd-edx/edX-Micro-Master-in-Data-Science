\documentclass[10pt]{article}

\newcommand{\littleblank}{\rule{.5in}{.01in}}
\newcommand{\blank}{\rule{1in}{.01in}}
\newcommand{\bigblank}{\rule{2in}{.01in}}
\newcommand{\bigbigblank}{\rule{3in}{.01in}}
\newcommand{\CUT}[0]{}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subfig} 
\usepackage{bm}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\rhead{PID: \underline{\hspace{1in}}}

\vspace{.1in}
\begin{document}

\setlength\parindent{0pt}
\thispagestyle{empty}

{\textbf \Large Quiz 6} \hfill CSE 255, Spring 2017
\\

\vspace{.1in}

Name: \underline{\hspace{3in}}
\\

PID: \underline{\hspace{3.15in}}

\vspace{.1in}

{\small \setlength\parindent{20pt}This is the sixth quiz of CSE255/DSE230

On your desk you should have only the exam paper and writing tools.
No hats or hoods allowed (unless religious items).
There is one question in this quiz.

You have 10 minutes to complete the exam.

Start by writing your name and PID on this page.

Good Luck!}\\
\underline{\hspace{6in}}

\newcommand{\uu}{\vec{u}}
\newcommand{\xx}{\vec{x}}
\newcommand{\yy}{\vec{y}}
\newcommand{\vmu}{\vec{\mu}}
~\\
~\\
\noindent
    {\bf Problem I (50 points)}\\
    Algorithms for learning Decision trees use a ``purity measure''
    $f(p)$ which is a function from $[0,1]$ to $[0,1]$, the next
    split is the one which minimizes the average of $f(p)$. A good
    purity measure is one that guarantees progress of the tree
    learning algorithm as long as at least one of the leaves is not
    pure (all of the labels are +1 or all of the labels are -1)\\
    ~\\
    Mark all of the correct statements
    \begin{enumerate}
    \item The minima of a purity measure must be at $p=0$ and $p=1$
    \item The maximum of a purity measure must be at $p=1/2$.
    \item The function $f(p)=\min(p,1-p)$ is a good purity measure.
    \item The function $f(p) = p(1-p)$ is a good purity measure.
    \item A good purity measure must be strictly concave.
    \end{enumerate}
    ~\\
    ~\\

\noindent
    {\bf Problem II (50 points)}\\
    The following statements are about
    the Adaboost algorithm for binary classification. Mark all of the
    correct statements.
    \begin{enumerate}
    \item Adaboost reduces the training error of the combined
      rule on each iteration.
      \item A weak rule is one which is slightly better than random
        guessing with respect the the uniform distribution over the
        training examples.
      \item A weak rule whose error rate is 60\% is better than a weak
        rule whose error rate is 45\%.
      \item If the errors of the  weak rules are all smaller than
        48\%, Adaboost will generate a strong rule whose training error is
        zero.
      \item Performing classification using an alternating
        decision tree requires summing the prediction nodes along a
        path from the root to a leaf.
    \end{enumerate}

\end{document}   




 
