{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from string import split,strip\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cover Type\n",
    "\n",
    "Classify geographical locations according to their predicted tree cover:\n",
    "\n",
    "* **URL:** http://archive.ics.uci.edu/ml/datasets/Covertype\n",
    "* **Abstract:** Forest CoverType dataset\n",
    "* **Data Set Description:** http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Cover Types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1.0: 'Spruce/Fir',\n",
       " 2.0: 'Lodgepole Pine',\n",
       " 3.0: 'Ponderosa Pine',\n",
       " 4.0: 'Cottonwood/Willow',\n",
       " 5.0: 'Aspen',\n",
       " 6.0: 'Douglas-fir',\n",
       " 7.0: 'Krummholz'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a dictionary of cover types\n",
    "CoverTypes={1.0: 'Spruce/Fir',\n",
    "            2.0: 'Lodgepole Pine',\n",
    "            3.0: 'Ponderosa Pine',\n",
    "            4.0: 'Cottonwood/Willow',\n",
    "            5.0: 'Aspen',\n",
    "            6.0: 'Douglas-fir',\n",
    "            7.0: 'Krummholz' }\n",
    "print 'Tree Cover Types:'\n",
    "CoverTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# creating a directory called covtype, download and decompress covtype.data.gz into it\n",
    "\n",
    "from os.path import exists\n",
    "if not exists('covtype'):\n",
    "    print \"creating directory covtype\"\n",
    "    !mkdir covtype\n",
    "%cd covtype\n",
    "if not exists('covtype.data'):\n",
    "    if not exists('covtype.data.gz'):\n",
    "        print 'downloading covtype.data.gz'\n",
    "        !curl -O http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
    "    print 'decompressing covtype.data.gz'\n",
    "    !gunzip -f covtype.data.gz\n",
    "!ls -l\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the feature names\n",
    "cols_txt=\"\"\"\n",
    "Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology,\n",
    "Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways,\n",
    "Hillshade_9am, Hillshade_Noon, Hillshade_3pm,\n",
    "Horizontal_Distance_To_Fire_Points, Wilderness_Area (4 binarycolumns), \n",
    "Soil_Type (40 binary columns), Cover_Type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'WA_0', 'WA_1', 'WA_2', 'WA_3', 'ST_0', 'ST_1', 'ST_2', 'ST_3', 'ST_4', 'ST_5', 'ST_6', 'ST_7', 'ST_8', 'ST_9', 'ST_10', 'ST_11', 'ST_12', 'ST_13', 'ST_14', 'ST_15', 'ST_16', 'ST_17', 'ST_18', 'ST_19', 'ST_20', 'ST_21', 'ST_22', 'ST_23', 'ST_24', 'ST_25', 'ST_26', 'ST_27', 'ST_28', 'ST_29', 'ST_30', 'ST_31', 'ST_32', 'ST_33', 'ST_34', 'ST_35', 'ST_36', 'ST_37', 'ST_38', 'ST_39', 'Cover_Type']\n"
     ]
    }
   ],
   "source": [
    "# Break up features that are made out of several binary features.\n",
    "from string import split,strip\n",
    "cols=[strip(a) for a in split(cols_txt,',')]\n",
    "colDict={a:[a] for a in cols}\n",
    "colDict['Soil_Type (40 binary columns)'] = ['ST_'+str(i) for i in range(40)]\n",
    "colDict['Wilderness_Area (4 binarycolumns)'] = ['WA_'+str(i) for i in range(4)]\n",
    "Columns=[]\n",
    "for item in cols:\n",
    "    Columns=Columns+colDict[item]\n",
    "print Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\r\n",
      "2590,56,2,212,-6,390,220,235,151,6225,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\r\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first two lines of the data file\n",
    "!head -2 covtype/covtype.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the file into an RDD\n",
    "# If doing this on a real cluster, you need the file to be available on all nodes, ideally in HDFS.\n",
    "path='covtype/covtype.data'\n",
    "inputRDD=sc.textFile(path)\n",
    "inputRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(5.0, [2596.0,51.0,3.0,258.0,0.0,510.0,221.0,232.0,148.0,6279.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the text RDD into an RDD of LabeledPoints\n",
    "Data=inputRDD.map(lambda line: [float(strip(x)) for x in line.split(',')])\\\n",
    "     .map(## FILLIN ##)\n",
    "Data.first()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size= 581012\n",
      "              type (label):   percent of total\n",
      "---------------------------------------------------------\n",
      "      Lodgepole Pine (2.0):\t48.76\n",
      "          Spruce/Fir (1.0):\t36.46\n",
      "      Ponderosa Pine (3.0):\t6.15\n",
      "           Krummholz (7.0):\t3.53\n",
      "         Douglas-fir (6.0):\t2.99\n",
      "               Aspen (5.0):\t1.63\n",
      "   Cottonwood/Willow (4.0):\t0.47\n"
     ]
    }
   ],
   "source": [
    "# count the number of examples of each type\n",
    "total=Data.cache().count()\n",
    "print 'total data size=',total\n",
    "counts=Data.  ## Fillin ##\n",
    "\n",
    "counts.sort(key=lambda x:x[1],reverse=True)\n",
    "print '              type (label):   percent of total'\n",
    "print '---------------------------------------------------------'\n",
    "print '\\n'.join(['%20s (%3.1f):\\t%4.2f'%(CoverTypes[a[0]],a[0],100.0*a[1]/float(total)) for a in counts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Making the problem binary\n",
    "\n",
    "The implementation of BoostedGradientTrees in MLLib supports only binary problems. the `CovTYpe` problem has\n",
    "7 classes. To make the problem binary we choose the `Lodgepole Pine` (label = 2.0). We therefor transform the dataset to a new dataset where the label is `1.0` is the class is `Lodgepole Pine` and is `0.0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Label=2.0\n",
    "Data=inputRDD.map(lambda line: [float(x) for x in line.split(',')])\\\n",
    "    .map(lambda V:LabeledPoint(### FILLIN ###))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reducing data size\n",
    "In order to see the effects of overfitting more clearly, we reduce the size of the data by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: Data1=58022, trainingData=40674, testData=17348\n"
     ]
    }
   ],
   "source": [
    "Data1=Data.sample(False,0.1).cache()\n",
    "(trainingData,testData)=Data1.randomSplit([0.7,0.3])\n",
    "\n",
    "print 'Sizes: Data1=%d, trainingData=%d, testData=%d'%(Data1.count(),trainingData.cache().count(),testData.cache().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 8881), (1.0, 8467)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts=testData.map(lambda lp:(lp.label,1)).reduceByKey(lambda x,y:x+y).collect()\n",
    "counts.sort(key=lambda x:x[1],reverse=True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosted Trees\n",
    "\n",
    "* Following [this example](http://spark.apache.org/docs/latest/mllib-ensembles.html#classification) from the mllib documentation\n",
    "\n",
    "* [pyspark.mllib.trees documentation](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.tree)\n",
    "\n",
    "#### Main classes and methods\n",
    "\n",
    "* `GradientBoostedTrees` is the class that implements the learning trainClassifier,\n",
    "   * It's main method is `trainClassifier(trainingData)` which takes as input a training set and generates an instance of `GradientBoostedTreesModel`\n",
    "   * The main parameter from train Classifier are:\n",
    "      * **data** – Training dataset: RDD of LabeledPoint. Labels should take values {0, 1}.\n",
    "      * categoricalFeaturesInfo – Map storing arity of categorical features. E.g., an entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.\n",
    "      * **loss** – Loss function used for minimization during gradient boosting. Supported: {“logLoss” (default), “leastSquaresError”, “leastAbsoluteError”}.\n",
    "      * **numIterations** – Number of iterations of boosting. (default: 100)\n",
    "      * **learningRate** – Learning rate for shrinking the contribution of each estimator. The learning rate should be between in the interval (0, 1]. (default: 0.1)\n",
    "      * **maxDepth** – Maximum depth of the tree. E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 3)\n",
    "      * **maxBins** – maximum number of bins used for splitting features (default: 32) DecisionTree requires maxBins >= max categories\n",
    "      \n",
    "      \n",
    "* `GradientBoostedTreesModel` represents the output of the boosting process: a linear combination of classification trees. The methods supported by this class are:\n",
    "   * `save(sc, path)` : save the tree to a given filename, sc is the Spark Context.\n",
    "   * `load(sc,path)` : The counterpart to save - load classifier from file.\n",
    "   * `predict(X)` : predict on a single datapoint (the `.features` field of a `LabeledPont`) or an RDD of datapoints.\n",
    "   * `toDebugString()` : print the classifier in a human readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "errors={}\n",
    "for depth in [1,3,6,10]:\n",
    "    start=time()\n",
    "    model=GradientBoostedTrees.trainClassifier(##FILLIN to generate 10 trees ##)\n",
    "    #print model.toDebugString()\n",
    "    errors[depth]={}\n",
    "    dataSets={'train':trainingData,'test':testData}\n",
    "    for name in dataSets.keys():  # Calculate errors on train and test sets\n",
    "        data=dataSets[name]\n",
    "        Predicted=model.predict(data.map(lambda x: x.features))\n",
    "        LabelsAndPredictions=data. ### FILLIN ###\n",
    "        Err = LabelsAndPredictions.filter(lambda (v,p):v != p).count()/float(data.count())\n",
    "        errors[depth][name]=Err\n",
    "    print depth,errors[depth],int(time()-start),'seconds'\n",
    "print errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B10 = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Train/test accuracy vs Depth of trees graph\n",
    "%pylab inline\n",
    "from plot_utils import *\n",
    "make_figure([B10],['10Trees'],Title='Boosting using 10% of data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
