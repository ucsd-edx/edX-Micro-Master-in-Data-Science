{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from DistributedBoosting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0  0  0  0  0  0  0  0  0 \n",
      "1  0  0  0  0  0  0  0  0  0 \n",
      "0  0  0  0  1  1  1  1  0  0 \n",
      "0  0  0  0  1  1  1  1  1  0 \n",
      "0  0  0  0  1  1  1  1  1  0 \n",
      "0  0  0  0  1  1  1  1  1  0 \n",
      "0  0  0  0  1  1  1  1  1  0 \n",
      "0  0  0  0  0  1  0  0  0  0 \n",
      "0  0  0  0  0  0  0  0  0  0 \n",
      "0  0  0  0  0  1  0  0  0  0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import rand\n",
    "p=0.9\n",
    "data=[]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if np.abs(i-4)<3 and np.abs(j-6)<3:\n",
    "            y=2*(rand()<p)-1\n",
    "        else:\n",
    "            y=2*(rand()>p)-1\n",
    "        print \"%1.0f \"%((1+y)/2),\n",
    "        data.append(LabeledPoint(y,[i,j]))\n",
    "    print\n",
    "\n",
    "dataRDD=sc.parallelize(data,numSlices=2)\n",
    "dataRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load DistributedBoosting.py\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "from string import split,strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"A simple service class to log run time and pretty-print it.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.T=[]\n",
    "    def stamp(self,name):\n",
    "        self.T.append((name,time()))\n",
    "    def str(self):\n",
    "        T=self.T\n",
    "        return '\\n'.join(['%6.2f : %s'%(T[i+1][1]-T[i][1],T[i+1][0]) for i in range(len(T)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Globals\n",
    "global T,iteration,GR,proposals,Strong_Classifier, feature_no, partition_no, Splits_Table\n",
    "global Strong_Classifier,global_best_splitter,PS\n",
    "\n",
    "T=Timer()\n",
    "feature_no=None                 # Tracks processing time\n",
    "global_feature_no=None\n",
    "partition_no=0\n",
    "iteration=0                     # Boosting iteration\n",
    "PS=[None]                       # RDD that hold state of boosting process for each partition.\n",
    "proposals=[]                    # proposed splits for each feature\n",
    "Strong_Classifier=[]            # Combined weak classifiers\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Partition fundctions\n",
    "def Prepare_partition_data_structure(A):\n",
    "\n",
    "    rows=len(A[1])\n",
    "\n",
    "    columns=np.empty([feature_no,rows])\n",
    "    columns[:]=np.NaN\n",
    "    print 'Prepare_partition_data_structure',feature_no,np.shape(columns)\n",
    "    \n",
    "    labels=np.empty(rows)\n",
    "    labels[:]=np.NaN\n",
    "\n",
    "    for j in range(rows):\n",
    "        LP=A[1][j]\n",
    "        labels[j]=LP.label\n",
    "        for i in range(feature_no):\n",
    "            columns[i,j]=LP.features[i]\n",
    "    return {'index':A[0],\\\n",
    "            'labels':labels,\\\n",
    "            'weights':np.ones(len(labels)),\\\n",
    "            'feature_values':columns}\n",
    "\n",
    "def Add_weak_learner_matrix(A):\n",
    "    \"\"\" This procedure adds to each partition the matrix that will be \n",
    "        used to efficiently find the best weak classifier \"\"\"\n",
    "\n",
    "    try:\n",
    "        feature_no\n",
    "    except:\n",
    "        feature_no=global_feature_no.value\n",
    "\n",
    "    index=A['index']%feature_no\n",
    "    SP=Splits_Table.value[index]\n",
    "\n",
    "    Col=A['feature_values'][index,:]\n",
    "\n",
    "    ### The matrix M is organized as follows: \n",
    "    # * There are as many rows as there are thresholds in SP (last one is inf)\n",
    "    # * There are as many columns as there are examples in this partition.\n",
    "    # For threshold i, the i'th rw of M is +1 if Col is smaller than the trehold SP[i] and -1 otherwise\n",
    "\n",
    "    M=np.empty([len(SP),len(Col)])\n",
    "    M[:]=np.NaN\n",
    "\n",
    "    for i in range(len(SP)):\n",
    "        M[i,:]=2*(Col<SP[i])-1\n",
    "\n",
    "    A['M']=M # add M matrix to the data structure.\n",
    "    return A\n",
    "\n",
    "\n",
    "def Find_weak(A):\n",
    "    \"\"\"Find the best split for a single feature on a single partition\"\"\"\n",
    "\n",
    "    try:\n",
    "        feature_no\n",
    "    except:\n",
    "        feature_no=global_feature_no.value\n",
    "\n",
    "    index=A['index']%feature_no\n",
    "    SP=Splits_Table.value[index]\n",
    "\n",
    "    M=A['M']\n",
    "    weights=A['weights']\n",
    "    weighted_Labels=weights*A['labels']\n",
    "    SS=np.dot(M,weighted_Labels)/np.sum(weights)\n",
    "    i_max=np.argmax(np.abs(SS))\n",
    "    answer={'Feature_index':A['index']%feature_no,\\\n",
    "            'Threshold_index':i_max,\\\n",
    "            'Threshold':SP[i_max],\\\n",
    "            'Correlation':SS[i_max],\\\n",
    "            'SS':SS}\n",
    "    return answer\n",
    "\n",
    "# update weights. New splitter is shipped to partition as one of the referenced\n",
    "# Variables\n",
    "\n",
    "def update_weights(A):\n",
    "    \"\"\"Update the weights of the exammples belonging to this \n",
    "    partition according to the new splitter\"\"\"\n",
    "    best_splitter=global_best_splitter\n",
    "    F_index=best_splitter['Feature_index']\n",
    "    Thr=best_splitter['Threshold']\n",
    "    alpha=best_splitter['alpha']\n",
    "    y_hat=2*(A['feature_values'][F_index,:]<Thr)-1\n",
    "    y=A['labels']\n",
    "    weights=A['weights']*exp(-alpha*y_hat*y)\n",
    "    weights /= sum(weights)\n",
    "    A['weights']=weights\n",
    "    return A\n",
    "\n",
    "def calc_scores(Strong_Classifier,Columns,Lbl):\n",
    "    \n",
    "    Scores=np.zeros(len(Lbl))\n",
    "\n",
    "    for h in Strong_Classifier:\n",
    "        index=h['Feature_index']\n",
    "        Thr=h['Threshold']\n",
    "        alpha=h['alpha']\n",
    "        y_hat=2*(Columns[index,:]<Thr)-1\n",
    "        Scores += alpha*y_hat*Lbl\n",
    "    return Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Head-Node functions\n",
    "def init(sc,Data):\n",
    "    \"\"\" Given an RDD with labeled Points, create the RDD of data structures used for boosting\n",
    "    \"\"\"\n",
    "\n",
    "    global T,iteration,GR,proposals,Strong_Classifier, feature_no, partition_no, Splits_Table\n",
    "    global Strong_Classifier,global_best_splitter\n",
    "\n",
    "    T=Timer()\n",
    "    T.stamp('Started')\n",
    "\n",
    "    X=Data.first()\n",
    "    feature_no=len(X.features)\n",
    "#    print 'global_feature_no = sc.broadcast(feature_no)',feature_no\n",
    "    partition_no=Data.getNumPartitions()\n",
    "    if partition_no != feature_no:\n",
    "        Data=Data.repartition(feature_no).cache()\n",
    "    print 'number of features=',feature_no,'number of partitions=',Data.getNumPartitions()\n",
    "\n",
    "    # Split data into training and test\n",
    "    (trainingData,testData)=Data.randomSplit([0.7,0.3])\n",
    "    print 'Sizes: Data1=%d, trainingData=%d, testData=%d'%      (Data.count(),trainingData.cache().count(),testData.cache().count())\n",
    "    T.stamp('Split into train and test')\n",
    "    # Glom each partition into a local array\n",
    "    G=trainingData.glom()\n",
    "    GTest=testData.glom()  \n",
    "    T.stamp('glom')\n",
    "\n",
    "    # Add an index to each partition to identify it.\n",
    "    def f(splitIndex, iterator): yield splitIndex,iterator.next()\n",
    "    GI=G.mapPartitionsWithIndex(f)\n",
    "    GTI=GTest.mapPartitionsWithIndex(f)\n",
    "    T.stamp('add partition index')\n",
    "\n",
    "    return GI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features= 2 number of partitions= 2\n",
      "Sizes: Data1=100, trainingData=78, testData=22\n"
     ]
    }
   ],
   "source": [
    "GI=init(sc,dataRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [LabeledPoint(-1.0, [0.0,0.0]),\n",
       "   LabeledPoint(-1.0, [0.0,1.0]),\n",
       "   LabeledPoint(-1.0, [0.0,2.0]),\n",
       "   LabeledPoint(-1.0, [0.0,3.0]),\n",
       "   LabeledPoint(-1.0, [0.0,4.0]),\n",
       "   LabeledPoint(-1.0, [0.0,5.0]),\n",
       "   LabeledPoint(-1.0, [0.0,6.0]),\n",
       "   LabeledPoint(-1.0, [0.0,7.0]),\n",
       "   LabeledPoint(-1.0, [0.0,8.0]),\n",
       "   LabeledPoint(-1.0, [0.0,9.0]),\n",
       "   LabeledPoint(1.0, [1.0,0.0]),\n",
       "   LabeledPoint(-1.0, [1.0,1.0]),\n",
       "   LabeledPoint(-1.0, [1.0,3.0]),\n",
       "   LabeledPoint(-1.0, [1.0,5.0]),\n",
       "   LabeledPoint(-1.0, [1.0,6.0]),\n",
       "   LabeledPoint(-1.0, [1.0,7.0]),\n",
       "   LabeledPoint(-1.0, [1.0,8.0]),\n",
       "   LabeledPoint(-1.0, [1.0,9.0]),\n",
       "   LabeledPoint(-1.0, [2.0,0.0]),\n",
       "   LabeledPoint(-1.0, [2.0,1.0]),\n",
       "   LabeledPoint(-1.0, [2.0,2.0]),\n",
       "   LabeledPoint(-1.0, [2.0,3.0]),\n",
       "   LabeledPoint(1.0, [2.0,5.0]),\n",
       "   LabeledPoint(1.0, [2.0,6.0]),\n",
       "   LabeledPoint(1.0, [2.0,7.0]),\n",
       "   LabeledPoint(-1.0, [2.0,8.0]),\n",
       "   LabeledPoint(-1.0, [2.0,9.0]),\n",
       "   LabeledPoint(-1.0, [3.0,0.0]),\n",
       "   LabeledPoint(-1.0, [3.0,1.0]),\n",
       "   LabeledPoint(1.0, [3.0,5.0]),\n",
       "   LabeledPoint(1.0, [3.0,6.0]),\n",
       "   LabeledPoint(-1.0, [4.0,0.0]),\n",
       "   LabeledPoint(-1.0, [4.0,1.0]),\n",
       "   LabeledPoint(-1.0, [4.0,2.0]),\n",
       "   LabeledPoint(-1.0, [4.0,3.0]),\n",
       "   LabeledPoint(1.0, [4.0,5.0]),\n",
       "   LabeledPoint(1.0, [4.0,6.0]),\n",
       "   LabeledPoint(1.0, [4.0,7.0]),\n",
       "   LabeledPoint(-1.0, [4.0,9.0])]),\n",
       " (1,\n",
       "  [LabeledPoint(-1.0, [5.0,0.0]),\n",
       "   LabeledPoint(-1.0, [5.0,1.0]),\n",
       "   LabeledPoint(-1.0, [5.0,2.0]),\n",
       "   LabeledPoint(-1.0, [5.0,3.0]),\n",
       "   LabeledPoint(1.0, [5.0,5.0]),\n",
       "   LabeledPoint(1.0, [5.0,6.0]),\n",
       "   LabeledPoint(1.0, [5.0,7.0]),\n",
       "   LabeledPoint(1.0, [5.0,8.0]),\n",
       "   LabeledPoint(-1.0, [5.0,9.0]),\n",
       "   LabeledPoint(-1.0, [6.0,1.0]),\n",
       "   LabeledPoint(-1.0, [6.0,2.0]),\n",
       "   LabeledPoint(-1.0, [6.0,3.0]),\n",
       "   LabeledPoint(1.0, [6.0,4.0]),\n",
       "   LabeledPoint(1.0, [6.0,6.0]),\n",
       "   LabeledPoint(1.0, [6.0,7.0]),\n",
       "   LabeledPoint(1.0, [6.0,8.0]),\n",
       "   LabeledPoint(-1.0, [6.0,9.0]),\n",
       "   LabeledPoint(-1.0, [7.0,0.0]),\n",
       "   LabeledPoint(-1.0, [7.0,1.0]),\n",
       "   LabeledPoint(-1.0, [7.0,2.0]),\n",
       "   LabeledPoint(1.0, [7.0,5.0]),\n",
       "   LabeledPoint(-1.0, [7.0,6.0]),\n",
       "   LabeledPoint(-1.0, [7.0,9.0]),\n",
       "   LabeledPoint(-1.0, [8.0,0.0]),\n",
       "   LabeledPoint(-1.0, [8.0,1.0]),\n",
       "   LabeledPoint(-1.0, [8.0,2.0]),\n",
       "   LabeledPoint(-1.0, [8.0,3.0]),\n",
       "   LabeledPoint(-1.0, [8.0,4.0]),\n",
       "   LabeledPoint(-1.0, [8.0,7.0]),\n",
       "   LabeledPoint(-1.0, [8.0,8.0]),\n",
       "   LabeledPoint(-1.0, [8.0,9.0]),\n",
       "   LabeledPoint(-1.0, [9.0,1.0]),\n",
       "   LabeledPoint(-1.0, [9.0,2.0]),\n",
       "   LabeledPoint(-1.0, [9.0,3.0]),\n",
       "   LabeledPoint(-1.0, [9.0,4.0]),\n",
       "   LabeledPoint(1.0, [9.0,5.0]),\n",
       "   LabeledPoint(-1.0, [9.0,6.0]),\n",
       "   LabeledPoint(-1.0, [9.0,8.0]),\n",
       "   LabeledPoint(-1.0, [9.0,9.0])])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GI.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init2(GI):\n",
    "    # Prepare the data structure for each partition.\n",
    "    GR=GI.map(Prepare_partition_data_structure)\n",
    "    print 'number of elements in GR=', GR.cache().count()\n",
    "    T.stamp('Prepare_partition_data_structure')\n",
    "    return GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #compute the split points for each feature\n",
    "    Splits=find_splits(GR)\n",
    "    print 'Split points=',Splits\n",
    "    T.stamp('Compute Split points')\n",
    "\n",
    "    #broadcast split points\n",
    "    global Splits_Table\n",
    "    Splits_Table=sc.broadcast(Splits)\n",
    "    T.stamp('Broadcast split points')\n",
    "\n",
    "    # Create matrix for each partition to make finding the weak rules correlation a matter of taking a matrix product\n",
    "\n",
    "    iteration=0\n",
    "    global PS\n",
    "    PS[0]=GR.map(Add_weak_learner_matrix)\n",
    "    print 'number of partitions in PS=',PS[0].cache().count()\n",
    "    T.stamp('Add_weak_learner_matrix')\n",
    "\n",
    "    return PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements in GR= 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'feature_values': array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "           1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "           2.,  3.,  3.,  3.,  3.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.],\n",
       "         [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,  0.,  1.,  3.,\n",
       "           5.,  6.,  7.,  8.,  9.,  0.,  1.,  2.,  3.,  5.,  6.,  7.,  8.,\n",
       "           9.,  0.,  1.,  5.,  6.,  0.,  1.,  2.,  3.,  5.,  6.,  7.,  9.]]),\n",
       "  'index': 0,\n",
       "  'labels': array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1.,\n",
       "         -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.,  1., -1.]),\n",
       "  'weights': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])},\n",
       " {'feature_values': array([[ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  6.,  6.,  6.,  6.,\n",
       "           6.,  6.,  6.,  6.,  7.,  7.,  7.,  7.,  7.,  7.,  8.,  8.,  8.,\n",
       "           8.,  8.,  8.,  8.,  8.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.],\n",
       "         [ 0.,  1.,  2.,  3.,  5.,  6.,  7.,  8.,  9.,  1.,  2.,  3.,  4.,\n",
       "           6.,  7.,  8.,  9.,  0.,  1.,  2.,  5.,  6.,  9.,  0.,  1.,  2.,\n",
       "           3.,  4.,  7.,  8.,  9.,  1.,  2.,  3.,  4.,  5.,  6.,  8.,  9.]]),\n",
       "  'index': 1,\n",
       "  'labels': array([-1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.,\n",
       "          1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.]),\n",
       "  'weights': array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GR=init2(GI)\n",
    "GR.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boosting_iteration(k=1):\n",
    "    \"\"\" perform k boosting iterations \"\"\"\n",
    "    for i in range(iteration,iteration+k):\n",
    "        T.stamp('Start main loop %d'%i)\n",
    "\n",
    "        prop=PS[i].map(Find_weak).collect()\n",
    "        proposals.append(prop)\n",
    "        corrs=[p['Correlation'] for p in prop]\n",
    "        best_splitter_index=np.argmax(np.abs(corrs))\n",
    "        best_splitter = prop[best_splitter_index]\n",
    "        Strong_Classifier.append(best_splitter)\n",
    "        global global_Strong_Classifier\n",
    "        global_Strong_Classifier=sc.broadcast(Strong_Classifier)\n",
    "        T.stamp('found best splitter %d'%i)\n",
    "\n",
    "        corr=best_splitter['Correlation']\n",
    "        best_splitter['alpha']=0.5*np.log((1+corr)/(1-corr))\n",
    "        global global_best_splitter\n",
    "        global_best_splitter = sc.broadcast(best_splitter)\n",
    "        PS.append(PS[i].map(update_weights))\n",
    "        T.stamp('Updated Weights %d'%i)\n",
    "    iteration+=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_splits(GR,number_of_bins=10,debug=False):\n",
    "    \"\"\"Compute the split points for each feature to create number_of_bins bins\"\"\"\n",
    "    def find_split_points(A):\n",
    "\n",
    "        try:\n",
    "            feature_no\n",
    "        except:\n",
    "            feature_no=global_feature_no.value\n",
    "\n",
    "        j=A['index'] % feature_no\n",
    "        S=np.sort(A['feature_values'][j,:       ])\n",
    "        L=len(S) \n",
    "        step=L/number_of_bins+2*number_of_bins\n",
    "        return (j,S[range(0,L,step)])\n",
    "\n",
    "    global partition_no\n",
    "    Splits=GR.map(find_split_points).collect()\n",
    "    max_no=np.array([np.finfo(float).max])\n",
    "\n",
    "    # Average the split points across the partitions corresponding to the same feature.\n",
    "    Splits1=[]\n",
    "    for i in range(feature_no):\n",
    "        S=Splits[i][1]\n",
    "        if debug:\n",
    "            print 'no. ',i,' = ',Splits[i]\n",
    "        n=1  # number of copies (for averaging)\n",
    "        j=i+feature_no\n",
    "        while j<partition_no:\n",
    "            if debug:\n",
    "                print 'j=',j\n",
    "            S+=Splits[j][1]\n",
    "            if debug:\n",
    "                print 'no. ',j,' = ',Splits[j]\n",
    "            n+=1.0\n",
    "            j+=feature_no\n",
    "        Splits1.append(np.concatenate([S/n,max_no]))\n",
    "        if debug:\n",
    "            print n\n",
    "            print Splits1[i]\n",
    "            print '='*60\n",
    "\n",
    "    return Splits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
