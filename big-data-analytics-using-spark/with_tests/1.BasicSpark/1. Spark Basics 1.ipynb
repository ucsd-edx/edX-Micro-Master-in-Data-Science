{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Basics 1\n",
    "\n",
    "This notebook inroduces two fundamental objects in Spark:\n",
    "* The Spark Context\n",
    "\n",
    "* The Resilient Distributed DataSet or RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark Context\n",
    "* Spark is complex distributed doftware. \n",
    "* The python interface to spark is called **pyspark**\n",
    "* **SparkContext** is a python class, defined as part of **pyspark** which manages the communication between the user's program and spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We start by creating a **SparkContext** object named **sc**. In this case we create a spark context that uses 3 *executors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa4c846e7d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Only one sparkContext at a time!\n",
    "When you run spark in local mode, you can have only a single context at a time. Therefor, if you want to use spark in a second notebook, you should first stop the one you are using here. This is what the method `.stop()` is for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# sc.stop() #commented out so that you don't stop your context by mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>RDDs</h3>\n",
    "\n",
    "<p>RDD (or Resilient Distributed DataSet) is the main novel data structure in Spark. You can think of it as a list whose elements are stored on several computers.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"Figures/SparkContextAndRDD.jpg\" style=\"height:324px; width:900px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The elements of each `RDD` are distributed across the **worker nodes** which are the nodes that perform the actual computations. This notebook, however, is running on the **Driver node**. As the RDD is not stored on the driver-node you cannot access it directly. The variable name `RDD` is really just a pointer to a python object which holds the information regardnig the actual location of the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parallelize \n",
    "* Simplest way to create an RDD.\n",
    "* The method `A=sc.parallelize(L)`, creates an RDD named `A` from list `L`.\n",
    "* `A` is an RDD of type `ParallelCollectionRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=sc.parallelize(range(3))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Collect\n",
    "\n",
    "* RDD content is distributed among all executors.\n",
    "* `collect()` is the inverse of `parallelize()'\n",
    "* collects the elements of the RDD\n",
    "* Returns a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "L=A.collect()\n",
    "print type(L)\n",
    "print L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Using `.collect()` eliminates the benefits of parallelism\n",
    "It is often tempting to `.collect()` and RDD, make it into a list, and then process the list using standard python. However, note that this means that you are using only the head node to perform the computation which means that you are not getting any benefit from spark.\n",
    "\n",
    "Using RDD operations, as described below, **will** make use of all of the computers at your disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Map\n",
    "* applies a given operation to each element of an RDD\n",
    "* parameter is the function defining the operation.\n",
    "* returns a new RDD.\n",
    "* Operation performed in parallel on all executors.\n",
    "* Each executor operates on the data **local** to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Note:** Here we are using **lambda** functions, later we will see that regular functions can also be used.\n",
    "\n",
    "For more on lambda function see [here](http://www.secnetix.de/olli/Python/lambda_functions.hawk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(4) PythonRDD[16] at RDD at PythonRDD.scala:48 []\\n |  ParallelCollectionRDD[14] at parallelize at PythonRDD.scala:475 []'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.map(lambda x: x*x).toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'todebugstring'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e5a4be0cfb06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodebugstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'todebugstring'"
     ]
    }
   ],
   "source": [
    "A.map(np.cos).todebugstring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.54030231, -0.41614684])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.collect()\n",
    "np.cos(A.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Excercise1",
     "locked": true,
     "solution": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Excercise 1\n",
    "\n",
    "1. Write a function called `mapcos` that has a single paramater: an RDD of numbers, computes the `cos()` (cosine) and returns it as a list.\n",
    "\n",
    "```cos(A)``` should produce the output\n",
    "    \n",
    "```\n",
    "    [1.0, 0.54030..., -0.41614...]\n",
    "```\n",
    "\n",
    "**Approximate answers** The reason that the numbers above end with `...` is that, when computing with floating point numbers, there are always roundoff errors, and the roundoff errors may depend on the architecture of the computer you are using. To avoid this problem, we test the answers you produce within a tolerance of 0.1%, in other words if your answer is $a$ and the correct answer is $b$ then your answer is considered correct if\n",
    "$$ \\left| \\frac{a-b}{a} \\right| < 0.001$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fa49c645710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "testPath = '/'.join(os.getcwd().split('/')[:-1]) + \"/Tester\"\n",
    "sys.path.insert(0, testPath )\n",
    "\n",
    "import BasicSpark\n",
    "#import BasicSpark_Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teach Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapcos(A):\n",
    "    return A.map(np.cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "func_teacher = mapcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs= [sc.parallelize(range(3)),\n",
    "         sc.parallelize(range(4,12))   \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = testPath+ \"/SparkBasic1.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex = \"ex1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BasicSpark_Teacher.GenPickle(func_teacher, inputs, filename, ex )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapcos(A):\n",
    "    return A.map(lambda x: 0.00000000001+ np.cos(x) ) \n",
    "    #return A.map(np.cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0, 1, 2]\n",
      "Correct Output: [1.0, 0.54030230586813977, -0.41614683654714241]\n",
      "Great Job!\n",
      "\n",
      "Input: [4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Correct Output: [-0.65364362086361194, 0.28366218546322625, 0.96017028665036597, 0.7539022543433046, -0.14550003380861354, -0.91113026188467694, -0.83907152907645244, 0.0044256979880507854]\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BasicSpark.exercise1(testPath, mapcos,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.00000000e-05,  -1.00000000e-05,  -1.00000000e-05,\n",
       "        -1.00000000e-05,  -1.00000000e-05,  -1.00000000e-05,\n",
       "        -1.00000000e-05,  -1.00000000e-05])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([-0.65364362086361194, 0.28366218546322625, 0.96017028665036597, 0.7539022543433046, -0.14550003380861354, -0.91113026188467694, -0.83907152907645244, 0.0044256979880507854]) - \\\n",
    "np.array([-0.65363362086361199, 0.28367218546322626, 0.96018028665036592, 0.75391225434330456, -0.14549003380861353, -0.91112026188467699, -0.83906152907645248, 0.004435697988050785])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exercise1(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for the pickle \n",
    "correctRDD = mapcos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A=sc.parallelize(range(3))\n",
    "corAns = mapcos(A).collect()\n",
    "corType= type( mapcos(A) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toPickle = {\"ex1\": [[corAns,corType]] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('exercise1','w')\n",
    "pickle.dump(toPickle,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('exercise1')\n",
    "fromPickle = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex1': [[[1.0, 0.54030230586813977, -0.41614683654714241],\n",
       "   pyspark.rdd.PipelinedRDD]]}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fromPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mapcos_student(A):\n",
    "    return A.map(np.cos) #return (A.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/elliott/Desktop/SparkClass/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/with_tests'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(cwd.split('/')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named with_tests.Tester.BasicSpark",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f87dc3a8e55c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwith_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicSpark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named with_tests.Tester.BasicSpark"
     ]
    }
   ],
   "source": [
    "import with_tests.Tester.BasicSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0, 1, 2, 3]\n",
      "Correct Output: [1.0, 0.54030230586813977, -0.41614683654714241, -0.98999249660044542]\n",
      "Great Job! Problem complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise1(mapcos_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(4) ParallelCollectionRDD[28] at parallelize at PythonRDD.scala:475 []'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.toDebugString()[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(4) PythonRDD[46] at RDD at PythonRDD.scala:48 []\\n |  ParallelCollectionRDD[28] at parallelize at PythonRDD.scala:475 []'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapcos(A).toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data0 = sc.parallelize(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mapcos(data0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(4) PythonRDD[37] at RDD at PythonRDD.scala:48 []\\n |  ParallelCollectionRDD[35] at parallelize at PythonRDD.scala:475 []'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapcos(data0).toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Excercise 2\n",
    "Consider the following RDD: \n",
    "\n",
    "```\n",
    "stringRDD=sc.parallelize([\"Spring quarter\", \"Learning spark basics\", \"Big data analytics with Spark\"])\n",
    "```\n",
    "   Write a function called `mapwords` that has a single paramater: an RDD of strings, and returns a list of words for each string.  \n",
    "    ```mapwords(MapString)``` should produce the output:\n",
    "    \n",
    "``` \n",
    "[['Spring', 'quarter'], ['Learning', 'spark', 'basics'], ['Big', 'data', 'analytics', 'with', 'Spark']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduce\n",
    "\n",
    "* Takes RDD as input, returns a single value.\n",
    "* **Reduce operator** takes **two** elements as input returns **one** as output.\n",
    "* Repeatedly applies a **reduce operator**\n",
    "* Each executor reduces the data local to it.\n",
    "* The results from all executors are combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The simplest example of a 2-to-1 operation is the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an example of a reduce operation that finds the shortest string in an RDD of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=['this','is','the','best','mac','ever']\n",
    "wordRDD=sc.parallelize(words)\n",
    "wordRDD.reduce(lambda w,v: w if len(w)<len(v) else v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "1. Write a `reduce` command that outputs the maximum number from a list of numbers. Your command should produce the following output on the input ```RDD=sc.parallelize([0,2,1])```\n",
    "\n",
    "   Output: ``` 2 ```\n",
    "   \n",
    "\n",
    "2. Consider the stringRDD defined in Exercise. Write a `reduce` command to produce a single string which is the concatenation of all the strings in stringRDD(with a space between each string). You output should look like:\n",
    "\n",
    "    Output: ``` 'Spring quarter Learning spark basics Big data analytics with Spark' ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using regular functions instead of lambda functions\n",
    "\n",
    "* lambda function are short and sweet.\n",
    "* but sometimes it's hard to use just one line.\n",
    "* We can use full-fledged functions instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "suppose we want to find the \n",
    "* last word in a lexicographical order \n",
    "* among \n",
    "* the longest words in the list.\n",
    "\n",
    "We could achieve that as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def largerThan(x,y):\n",
    "    if len(x)>len(y): return x\n",
    "    elif len(y)>len(x): return y\n",
    "    else:  #lengths are equal, compare lexicographically\n",
    "        if x>y: \n",
    "            return x\n",
    "        else: \n",
    "            return y\n",
    "        \n",
    "wordRDD.reduce(largerThan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "1. Consider the following RDD:\n",
    "\n",
    "    ``` listRDD=sc.parallelize([[3,4],[2,1],[7,9]]) ```\n",
    " \n",
    "     Write a regular function with `reduce` command to output the maximum element from a set of lists. Your output should look like:\n",
    "     \n",
    "     Output: ```[9]```\n",
    "     \n",
    "     (Note: The output is a list containing a single number rather than just a single number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Reduce operations <strong>must not depend on the order</strong></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Order of operands should not matter</li>\n",
    "\t<li>Order of application of reduce operator should not matter</li>\n",
    "</ul>\n",
    "\n",
    "<p>Multiplication and summation are good:</p>\n",
    "\n",
    "<h1>&nbsp; &nbsp; &nbsp; &nbsp; 1 + 3 + 5 + 2 &nbsp; &nbsp; &nbsp;5 + 3 + 1 + 2 &nbsp;</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>Division and subtraction are bad:</p>\n",
    "\n",
    "<h1>&nbsp; &nbsp; &nbsp; &nbsp; 1 - 3 - 5 - 2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 - 3 - 5 - 2 &nbsp;</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Why must reordering not change the result?\n",
    "\n",
    "You can think about the reduce operation as a binary tree where the leaves are the elements of the list and the root is the final result. Each triplet of the form (parent, child1, child2) corresponds to a single application of the reduce function. \n",
    "\n",
    "The order in which the reduce operation is applied is **determined at run time** and depends on how the RDD is partitioned across the cluster.\n",
    "There are many different orders to apply the reduce operation. \n",
    "\n",
    "If we want the input RDD to uniquely determine the reduced value **all evaluation orders must must yield the same final result**. In addition, the order of the elements in the list must not change the result. In particular, reversing the order of the operands in a reduce function must not change the outcome. \n",
    "\n",
    "For example the arithmetic operations multiply `*` and add `+` can be used in a reduce, but the operations subtract `-` and divide `/` should not.\n",
    "\n",
    "Doing so will not raise an error, but the result is unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a example  \n",
    "Which of these the following orders was executed?\n",
    "* $$((1-3)-5)-2$$ or\n",
    "* $$(1-3)-(5-2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "B=sc.parallelize([1,3,5,2])\n",
    "B.reduce(lambda x,y: x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def very_close_lists(A,B,tol=0.001):\n",
    "    # Check that the two firs parameters are lists of equal length \n",
    "    #and then check\n",
    "    assert type(A)==list and type(B) ==list\n",
    "    assert len(A)==len(B)\n",
    "    for i in range(len(A)):\n",
    "        a=A[i]; b=B[i]\n",
    "        if a==0:\n",
    "            a+=1; b+=1\n",
    "        else:\n",
    "            r=(a-b)/a\n",
    "            assert abs(r)<tol\n",
    "            \n",
    "very_close_lists(mapcos(sc.parallelize(range(3))),[1.0, 0.5403, -0.4161])\n",
    "\n",
    "\n",
    "def old_stuff( mapcos_student ):\n",
    "    data0 = sc.parallelize(range(4))\n",
    "    initDebugStr = data0.toDebugString()\n",
    "    studentRDD = mapcos_student(data0)\n",
    "    \n",
    "    correctRDD = mapcos(data0)\n",
    "    \n",
    "    print \"Input: \" + str( sc.parallelize(range(4)).collect() )\n",
    "    print \"Correct Output: \" + str(  correctRDD.collect()  ) \n",
    "    \n",
    "    # OK: string from pickle:  type(correctRDD)\n",
    "    try: assert( type(correctRDD) == type(studentRDD) )\n",
    "    except AssertionError as e:\n",
    "        print \"\\n Incorrect return type. The return type of your function should be: \" + str(type(correctRDD))\n",
    "        return False\n",
    "    \n",
    "    newDebugStr  = studentRDD.toDebugString()\n",
    "    initDebugStr = ' '.join(initDebugStr.split(' ')[1:])\n",
    "\n",
    "    \n",
    "    # OK: don't change anything\n",
    "    try: assert( initDebugStr in newDebugStr )\n",
    "    except AssertionError as e:\n",
    "        print \"\\n U cheated!\"\n",
    "        return False\n",
    "\n",
    "    # OK: list from pickle:    correctRDD.collect()\n",
    "    try: assert( studentRDD.collect() == correctRDD.collect() )\n",
    "    except AssertionError as e:\n",
    "        print \"\\n function returned incorrect output\"\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    print \"Great Job!\"\n",
    "    return True\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
