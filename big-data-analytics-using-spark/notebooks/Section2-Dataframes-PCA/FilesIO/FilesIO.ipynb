{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Local files\n",
    "\n",
    "### Download\n",
    "\n",
    "Remote working directory can be downloaded with the `--download` parameter:\n",
    "\n",
    "```bash\n",
    "python spark-ec2-helper.py --download\n",
    "```\n",
    "\n",
    "This method will download all files, including the IPython Notebook and files that your program generated on the server (pickle files, etc.).\n",
    "Files will be downloaded to the `./remote_files` directory.\n",
    "\n",
    "### Upload\n",
    "\n",
    "You can upload a single file or all files in a directory with the `--upload` parameter:\n",
    "\n",
    "```bash\n",
    "python spark-ec2-helper.py --upload path/to/a/file\n",
    "python spark-ec2-helper.py --upload path/to/a/directory\n",
    "```\n",
    "\n",
    "If you want to read from a local text file, you can use this method to upload it to the server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## S3 files\n",
    "\n",
    "The object `s3helper` is created to help you access S3 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on instance of S3Helper in module __main__:\n",
      "\n",
      "class S3Helper\n",
      " |  A helper function to access S3 files\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |  \n",
      " |  get_file(self, key_name)\n",
      " |      Download the remote file `key_name` on S3 to local.\n",
      " |      \n",
      " |      Args:\n",
      " |          key_name\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  get_path(self, path='')\n",
      " |      Get paths of all files in `path` with s3 prefix,\n",
      " |      which can be passed to Spark.\n",
      " |      \n",
      " |       Args:\n",
      " |           path\n",
      " |       Returns:\n",
      " |           an array of file paths with s3 prefix\n",
      " |  \n",
      " |  load_path(self, path, tgt)\n",
      " |      Load all files in `path` to the directory `tgt` in HDFS.\n",
      " |      \n",
      " |      Args:\n",
      " |          path, tgt\n",
      " |      Returns:\n",
      " |          an array of file paths in HDFS\n",
      " |  \n",
      " |  ls(self, path='')\n",
      " |      List all files in `path`.\n",
      " |      \n",
      " |      Args:\n",
      " |          path\n",
      " |      Returns:\n",
      " |          an array of files in `path`\n",
      " |  \n",
      " |  open_bucket(self, bucket_name)\n",
      " |      Open a S3 bucket.\n",
      " |      \n",
      " |      Args:\n",
      " |          bucket_name\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  put_file(self, filename, tgt)\n",
      " |      Save a local file `filename` to the directory `tgt` on S3.\n",
      " |      \n",
      " |      Args:\n",
      " |          filename, tgt\n",
      " |      Returns:\n",
      " |          None\n",
      " |  \n",
      " |  set_credential(self, aws_access_key, aws_secret_access_key)\n",
      " |      Set AWS credential.\n",
      " |      \n",
      " |      Args:\n",
      " |          aws_access_key, aws_secret_access_key\n",
      " |      Returns:\n",
      " |          None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(s3helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To access s3 files, the first step is setting AWS credential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%run Credentials.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "s3helper.set_credential(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Moving data from S3 to your Spark Cluster\n",
    "Long term storage of data on AWS is done either on **S3** (about \\$30 per TB\\*Month) or \n",
    "**Glacier** (about \\$7 per TB\\*Month). \n",
    "\n",
    "You can, of course, keep the data on your personal server but moving data to and from AWS is slow and/or expensive.\n",
    "\n",
    "The cheapest way to upload large amounts of data to S3 is by [physically shipping disks](http://aws.amazon.com/importexport/)\n",
    "\n",
    "Once your files are on S3, it is quite fast to move them to your AWS instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Spark-Notebook package\n",
    "[Julaiti Arpapt](http://cseweb.ucsd.edu/~jalafate/) Has utility to simplify the task of creating and managing a spark cluster on AWS. The utility is available from GitHub [here](https://github.com/arapat/spark-notebook). (The utility is also described [here](http://mas-dse.github.io/DSE230/installation/))\n",
    "\n",
    "These scripts automate the creation of spark clusters on AWS, moving files between your computer, your AWS cluster and S3, and other useful features. I will not review the whole package here. I will just use it to demonstrate some useful actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Working with s3 buckets and files\n",
    "The first step to working with S3 is to open the **bucket** that has your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "s3helper.open_bucket('ucsd-twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now you can list your files in the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Constants.py', u'data', u'data-cse255', u'data2', u'jan_geodata', u'model-feb', u'otherdata', u'pairs-130-179', u'pairs-176-179', u'pairs-176-247', u'pairs-176-247-clean', u'sample-jan-57', u'xy-298-307', u'yelp', u'yx-298-307']\n",
      "[u'model-feb/users-partition-feb.txt']\n"
     ]
    }
   ],
   "source": [
    "print s3helper.ls()\n",
    "print s3helper.ls('model-feb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To read the files, you have two options. \n",
    "\n",
    "**Option 1** Get a list of s3 file paths and pass it to Spark. \n",
    "\n",
    "This is the better option if you have enough memory to\n",
    "keep all of the data, and redundancy / error recovery are not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u's3n://ucsd-twitter/model-feb/users-partition-feb.txt']\n"
     ]
    }
   ],
   "source": [
    "files = s3helper.get_path('/model-feb')\n",
    "print files\n",
    "rdd = sc.textFile(','.join(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Option 2** Load S3 files to HDFS and read them from HDFS\n",
    "\n",
    "This is the better option if the data is too large to fit in memory \n",
    "or if the data will be used over a long period of time so redundancy / error recovery \n",
    "are significant issues.\n",
    "\n",
    "Loading data into HDFS will be slower than loading it directly into memory.\n",
    "On the other hand, loading from HDFS to memory is much faster than loading from S3 to memory and\n",
    "HDFS provides redundancy and error recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'/feb/users-partition-feb.txt']\n"
     ]
    }
   ],
   "source": [
    "files = s3helper.load_path('/model-feb', '/feb')\n",
    "print files\n",
    "rdd = sc.textFile(','.join(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parquet Format\n",
    "Parquet is a file format developed specifically for large data applications. \n",
    "Using this file format a program can read a selected subset of the rows in a \n",
    "table using and SQL query. \n",
    "\n",
    "This is a much faster alternative than reading the whole file into memory and then filtering out\n",
    "the un-needed parts.\n",
    "\n",
    "Parquet files thus provide some of the functionality of an RDBMS. Specifically, \n",
    "an efficient way to read susets of large tables. However, to perform out-of-memory calculations other than selection, one needs to install a full-fledged RDBMS such as Hive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/parquet/_SUCCESS',\n",
       " u'/parquet/_common_metadata',\n",
       " u'/parquet/_metadata',\n",
       " u'/parquet/part-r-00000-0f4998c0-b27b-4f60-ad45-ed3212ddb46f.gz.parquet',\n",
       " u'/parquet/part-r-00001-0f4998c0-b27b-4f60-ad45-ed3212ddb46f.gz.parquet',\n",
       " u'/parquet/part-r-00002-0f4998c0-b27b-4f60-ad45-ed3212ddb46f.gz.parquet',\n",
       " u'/parquet/part-r-00003-0f4998c0-b27b-4f60-ad45-ed3212ddb46f.gz.parquet',\n",
       " u'/parquet/part-r-00004-0f4998c0-b27b-4f60-ad45-ed3212ddb46f.gz.parquet',\n",
       " u'/parquet/part-r-00005-0f4998c0-b27b-4f60-ad45-ed3212ddb46f.gz.parquet',\n",
       " u'/parquet/part-r-00006-0f4998c0-b27b-4f60-ad45-ed3212ddb46f.gz.parquet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3helper.open_bucket(\"mas-dse-public\")\n",
    "\n",
    "files = s3helper.load_path('/Weather/US_Weather.parquet', '/US_Weather.parquet')\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext(master=master_url)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(station=u'USC00415427', measurement=u'DAPR')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.sql(\"SELECT station, measurement FROM parquet.`/US_Weather.parquet`\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
