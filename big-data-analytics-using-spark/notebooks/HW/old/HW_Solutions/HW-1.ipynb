{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removePunctuation(text):\n",
    "    text=re.sub(\"[^0-9a-zA-Z ]\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textRDD = sc.newAPIHadoopFile('../../Data/Moby-Dick.txt',\n",
    "                              'org.apache.hadoop.mapreduce.lib.input.TextInputFormat',\n",
    "                              'org.apache.hadoop.io.LongWritable',\n",
    "                              'org.apache.hadoop.io.Text',\n",
    "                               conf={'textinputformat.record.delimiter': \"\\r\\n\\r\\n\"}) \\\n",
    "            .map(lambda p: p[1])\n",
    "\n",
    "sentences=textRDD.flatMap(lambda x: x.split(\". \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the project gutenberg ebook of moby dick  or the whale  by herman melville',\n",
       " u'this ebook is for the use of anyone anywhere at no cost and with  almost no restrictions whatsoever',\n",
       " u' you may copy it  give it away or  re use it under the terms of the project gutenberg license included  with this ebook or online at www gutenberg org']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=sentences.map(removePunctuation).map(lambda x:x.lower())\n",
    "sentences.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the input file is generated in Windows. In Windows, the newline character is \"\\r\\n\", while in Linux it is \"\\n\". So here we set the delimiter for paragraphs to be \"\\r\\n\\r\\n\" instead of \"\\n\\n\".\n",
    "\n",
    "Refererences:\n",
    "1. http://stackoverflow.com/questions/17692857/\n",
    "2. http://stackoverflow.com/questions/7013034/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'the', u'project'), 1),\n",
       " ((u'project', u'gutenberg'), 1),\n",
       " ((u'gutenberg', u'ebook'), 1),\n",
       " ((u'ebook', u'of'), 1),\n",
       " ((u'of', u'moby'), 1),\n",
       " ((u'moby', u'dick'), 1),\n",
       " ((u'dick', u'or'), 1),\n",
       " ((u'or', u'the'), 1),\n",
       " ((u'the', u'whale'), 1),\n",
       " ((u'whale', u'by'), 1),\n",
       " ((u'by', u'herman'), 1),\n",
       " ((u'herman', u'melville'), 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=2  # k is part of the environment variables of the function. Spark sends the value of the referenced variables \n",
    "     # to the slave nodes.\n",
    "def ngram(sentence):\n",
    "    set=[]\n",
    "    for i in range(len(sentence)-k+1):\n",
    "        set.append((tuple(sentence[i:i+k]),1))\n",
    "    return set\n",
    "\n",
    "ngram(sentences.map(lambda x:x.split()).first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printOutput(n,freq_ngramRDD):\n",
    "    top=freq_ngramRDD.take(5)\n",
    "    print '\\n============ %d most frequent %d-grams'%(5,n)\n",
    "    print '\\nindex\\tcount\\tngram'\n",
    "    for i in range(5):\n",
    "        print '%d.\\t%d: \\t\"%s\"'%(i+1,top[i][0],' '.join(top[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ 5 most frequent 1-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t14620: \t\"the\"\n",
      "2.\t6732: \t\"of\"\n",
      "3.\t6502: \t\"and\"\n",
      "4.\t4799: \t\"a\"\n",
      "5.\t4706: \t\"to\"\n",
      "\n",
      "============ 5 most frequent 2-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t1906: \t\"of the\"\n",
      "2.\t1193: \t\"in the\"\n",
      "3.\t746: \t\"to the\"\n",
      "4.\t444: \t\"from the\"\n",
      "5.\t413: \t\"the whale\"\n",
      "\n",
      "============ 5 most frequent 3-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t116: \t\"the sperm whale\"\n",
      "2.\t109: \t\"of the whale\"\n",
      "3.\t88: \t\"the white whale\"\n",
      "4.\t64: \t\"one of the\"\n",
      "5.\t60: \t\"of the sea\"\n",
      "\n",
      "============ 5 most frequent 4-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t43: \t\"of the sperm whale\"\n",
      "2.\t27: \t\"the sperm whale s\"\n",
      "3.\t20: \t\"at the same time\"\n",
      "4.\t18: \t\"project gutenberg tm electronic\"\n",
      "5.\t18: \t\"of the whale s\"\n",
      "\n",
      "============ 5 most frequent 5-grams\n",
      "\n",
      "index\tcount\tngram\n",
      "1.\t13: \t\"the project gutenberg literary archive\"\n",
      "2.\t13: \t\"project gutenberg literary archive foundation\"\n",
      "3.\t12: \t\"project gutenberg tm electronic works\"\n",
      "4.\t11: \t\"of the sperm whale s\"\n",
      "5.\t10: \t\"and at the same time\"\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,6):\n",
    "    ngrams = sentences.map(lambda x:x.split()).flatMap(ngram)\n",
    "    freq_ngrams = ngrams.reduceByKey(lambda x,y:x+y).map(lambda x:(x[1],x[0])).sortByKey(False)\n",
    "    m=5\n",
    "    printOutput(k,freq_ngrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
