{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources\n",
    "This notebook is based on information gleaned from the following documents:\n",
    "\n",
    "* [Spark official document: Tuning Spark](https://spark.apache.org/docs/latest/tuning.html#memory-tuning)\n",
    "* [Spark Memory Management](https://0x0fff.com/spark-memory-management/)\n",
    "* [Cloudera: How to tune you apache spark jobs (part 1)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/)\n",
    "* [Cloudera: How to tune you apache spark jobs (part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)\n",
    "* [DataBricks: tuning java garbage-collection for spark applications](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "In this notebook we change the configuration of Spark. the pyspark notebook creates a fully configured `SparkContext` before starting the notebook. In order to control the configuration you need to open this notebook as a plain iPython (Jupyter) notebook.\n",
    "\n",
    "In addition, you need to set the following environment variables, we recommend you add these lines to your `.bashrc` or `.bash_login` scripts. Then start a new terminal.\n",
    "\n",
    "```bash\n",
    "######################################################################\n",
    "# setting SPARK pointers\n",
    "######################################################################\n",
    "# set this to whereever you installed SPARK_HOME\n",
    "export SPARK_HOME='$HOME/spark-latest'\n",
    "\n",
    "# Where you specify options you would normally add after bin/pyspark\n",
    "export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.9-src.zip:$SPARK_HOME/python:$PYTHONPATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Memory Management\n",
    "\n",
    "COnfiguring memory management can have a large impact on the performance of a spark job.\n",
    "\n",
    "### Resources\n",
    "This notebook is based on information gleaned from the following documents:\n",
    "\n",
    "* [Spark official document: Tuning Spark](https://spark.apache.org/docs/latest/tuning.html#memory-tuning)\n",
    "* [Cloudera: How to tune you apache spark jobs (part 1)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/)\n",
    "* [Cloudera: How to tune you apache spark jobs (part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)\n",
    "* [DataBricks: tuning java garbage-collection for spark applications](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three considerations in tuning memory usage: \n",
    "\n",
    "Taken from [Spark official document](https://spark.apache.org/docs/latest/tuning.html#memory-tuning):\n",
    "\n",
    "1. the amount of memory used by your objects (you may want your entire dataset to fit in memory), \n",
    "1. the cost of accessing those objects, and \n",
    "1. the overhead of garbage collection (if you have high turnover in terms of objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Memory Consumption\n",
    "\n",
    "The \"Storage\" page in the Spark web UI will show the amount of memory consumption of a RDD after it is **put into cache**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only one Spark Contex can run in a notebook at one time\n",
    "\n",
    "# Kill previous SparkContext to apply new configurations,\n",
    "# will trigger exception if no SparkContext is created yet\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start a Spark Context using 3 out of the 4 cores on my laptop\n",
    "sc = SparkContext(master=\"local[3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open http://localhost:4040/.\n",
    "\n",
    "![](images/ui-application.png)\n",
    "\n",
    "![](images/empty-storage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Data Structures\n",
    "\n",
    "### Serialized RDD Storage\n",
    "\n",
    "Quote from [Spark official document](https://spark.apache.org/docs/latest/tuning.html#serialized-rdd-storage):\n",
    "\n",
    "    When your objects are too large to efficiently store despite this tuning, a much simpler way to reduce memory usage is to store them in serialized form, using the serialized StorageLevels in the RDD persistence API, such as MEMORY_ONLY_SER. Spark will then store each RDD partition as one large byte array. The only downside of storing data in serialized form is slower access times, due to having to deserialize each object on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [\"hello world\"] * (10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(data) \\\n",
    "         .persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize(data) \\\n",
    "         .persist(StorageLevel.MEMORY_ONLY)\n",
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/serialization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage Collection\n",
    "Garbage collection is one of the most complex part of the Java Virtual Machine and it has a large impact on performance.\n",
    "\n",
    "The problem that Garbage Collection intends to solve is to free the memory space occupied by objects that are no longer needed.\n",
    "\n",
    "In some object-oriented languages such as **C++** the programmer has direct control of memory allocation. This means that each object type has to have a **constructor** and a **destructor**. The *constructor allocates* space for a new object (usually on the heap), and the *destructor de-allocates* the space when the object is no longer needed.\n",
    "\n",
    "This type of memory management allows the programmer to write highly optimized code. On the other hand, it puts on her a significant burden as keeping track of which objects are needed and which are not can be very complex. Failing to deallocate memory in a timely manner can lead to *memory leaks* which are hard to debug.\n",
    "\n",
    "**Java** (which is the native language of Spark) takes a different approach to memory management than **C++**. It does not require that the programmer write destructors. The only requirement on the programmer is that, when an object is no longer needed, there will be no way to reach that object from one of the variables in the program. For example, if a function assigns an object to a local variable, that object will have no pointer leading to it when the function returns.\n",
    "\n",
    "More precisely, the program has, at any time, a set of pointers that point to objects on the **Heap**. Thi set of pointers is referred to as the *root set of references* in the figure below. When the heap gets too full, the *garbage collector* is invoked. The garbage collector identifies objects in the heap that cannot be reached from the root set of references. Those objects are the added to the \"free\" part of the heap and are made available for new objects.\n",
    "\n",
    "![](images/Garbage-Collection.gif)\n",
    "\n",
    "For more details read [here](http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html)\n",
    "\n",
    "#### Measuring the Impact of GC\n",
    "\n",
    "The impact of GC can be viewed using either the Spark UI or the terminal log messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ui-gc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Garbage Collection\n",
    "*  [Spark configuration](http://spark.apache.org/docs/latest/configuration.html#viewing-spark-properties)\n",
    "*  [Memory management overview](http://spark.apache.org/docs/latest/tuning.html#memory-management-overview)\n",
    "\n",
    "#### From Memory Management Overview\n",
    "\n",
    "Memory usage in Spark largely falls under one of two categories: **execution** and **storage**. Execution memory refers to that used for computation in shuffles, joins, sorts and aggregations, while storage memory refers to that used for caching and propagating internal data across the cluster. In Spark, execution and storage share a unified region (M). When no execution memory is used, storage can acquire all the available memory and vice versa. Execution may evict storage if necessary, but only until total storage memory usage falls under a certain threshold (R). In other words, R describes a subregion within M where cached blocks are never evicted. Storage may not evict execution due to complexities in implementation.\n",
    "\n",
    "<img src=\"images/Spark-Memory-Management-1.6.0.png\" width=\"400\">\n",
    "\n",
    "Although there are two relevant configurations, the typical user should not need to adjust them as the default values are applicable to most workloads:\n",
    "\n",
    "* **spark.memory.fraction** expresses the size of M as a fraction of the (JVM heap space - 300MB) (default 0.75). The rest of the space (25%) is reserved for user data structures, internal metadata in Spark, and safeguarding against OOM errors in the case of sparse and unusually large records.\n",
    "* **spark.memory.storageFraction** expresses the size of R as a fraction of M (default 0.5). R is the storage space within M where cached blocks immune to being evicted by execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, try with very restricted memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[2]\") \\\n",
    "                  .set(\"spark.memory.fraction\", 0.1**10)  # Limit the memory accessible to Spark\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'\\ufeffMOBY', u'\\ufeffMOBY')\n",
      "2581129\n",
      "It takes 11.22 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "## trigger_gc generates all pairs of words in a given line s\n",
    "## Named trigger_gc because it produces a large amount of output, that would trigger GC.\n",
    "def trigger_gc(s):\n",
    "    ws = s.split()\n",
    "    for a in ws:\n",
    "        for b in ws:\n",
    "            yield (a, b)\n",
    "\n",
    "st = time.time()\n",
    "rdd = sc.textFile('./Moby-Dick-Edited.txt', minPartitions=1000) \\\n",
    "         .flatMap(trigger_gc)\n",
    "print rdd.first()\n",
    "print rdd.count()\n",
    "print \"It takes %.2f seconds.\" % (time.time() - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/many-gc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[2]\") \\\n",
    "                  .set(\"spark.memory.fraction\", 1.0)  # No memory limit\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 9.23 seconds.\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "rdd = sc.textFile('./Moby-Dick-Edited.txt', minPartitions=1000) \\\n",
    "         .flatMap(triger_gc)\n",
    "rdd.count()\n",
    "print \"It takes %.2f seconds.\" % (time.time() - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/few-gc.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
