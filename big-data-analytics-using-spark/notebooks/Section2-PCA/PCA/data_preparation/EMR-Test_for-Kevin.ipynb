{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Test of python compatibility\n",
    "\n",
    "We want this notebook to run using python3 both in the driver and in the workers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data and creating dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 16 ms, total: 180 ms\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pwd\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "libfilename='/mnt/workspace/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/notebooks/Section2-PCA/PCA/data_preparation/lib/numpy_pack.py'\n",
    "#libfilename='./lib/numpy_pack.py'\n",
    "sc = SparkContext(pyFiles=[libfilename])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56392\n",
      "drwxrwxr-x 2 hadoop hadoop    36864 Mar  6 20:44 decon_SSSSBBBB_SNWD.parquet\n",
      "-rw-rw-r-- 1 hadoop hadoop 25674261 Mar  6 20:44 STAT_SSSSBBBB.pickle\n",
      "-rw-rw-r-- 1 hadoop hadoop 12456904 Mar  6 20:44 US_Weather_BBSSBBSS.csv\n",
      "-rw-rw-r-- 1 hadoop hadoop  3430874 Mar  6 20:44 US_Weather_BBSSBBSS.csv.gz\n",
      "drwxrwxr-x 2 hadoop hadoop     4096 Mar  6 20:44 US_Weather_BBSSBBSS.parquet\n",
      "-rw-rw-r-- 1 hadoop hadoop 12880638 Mar  6 20:44 US_Weather_SSSSBBBB.csv\n",
      "-rw-rw-r-- 1 hadoop hadoop  3245918 Mar  6 20:44 US_Weather_SSSSBBBB.csv.gz\n",
      "curl https://mas-dse-open.s3.amazonaws.com/Weather/small/US_Weather_SSSSBBBB.csv.gz > ../../../Data/Weather/US_Weather_SSSSBBBB.csv.gz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 3169k  100 3169k    0     0  1584k      0  0:00:02  0:00:02 --:--:-- 1403k\n",
      "-rw-rw-r-- 1 hadoop hadoop 3.1M Mar  7 03:46 ../../../Data/Weather/US_Weather_SSSSBBBB.csv.gz\n",
      "CPU times: user 28 ms, sys: 36 ms, total: 64 ms\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_index='SSSSBBBB'\n",
    "data_dir='../../../Data/Weather'\n",
    "\n",
    "!ls -l $data_dir\n",
    "filebase='US_Weather_%s'%file_index\n",
    "!rm -rf $data_dir/$filebase*\n",
    "\n",
    "c_filename=filebase+'.csv.gz'\n",
    "u_filename=filebase+'.csv'\n",
    "\n",
    "command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/small/%s > %s/%s\"%(c_filename,data_dir,c_filename)\n",
    "print(command)\n",
    "!$command\n",
    "!ls -lh $data_dir/$c_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56392\r\n",
      "drwxrwxr-x 2 hadoop hadoop    36864 Mar  6 20:44 decon_SSSSBBBB_SNWD.parquet\r\n",
      "-rw-rw-r-- 1 hadoop hadoop 25674261 Mar  6 20:44 STAT_SSSSBBBB.pickle\r\n",
      "-rw-rw-r-- 1 hadoop hadoop 12456904 Mar  6 20:44 US_Weather_BBSSBBSS.csv\r\n",
      "-rw-rw-r-- 1 hadoop hadoop  3430874 Mar  6 20:44 US_Weather_BBSSBBSS.csv.gz\r\n",
      "drwxrwxr-x 2 hadoop hadoop     4096 Mar  6 20:44 US_Weather_BBSSBBSS.parquet\r\n",
      "-rw-rw-r-- 1 hadoop hadoop 12880638 Mar  7 03:47 US_Weather_SSSSBBBB.csv\r\n",
      "-rw-rw-r-- 1 hadoop hadoop  3245918 Mar  7 03:46 US_Weather_SSSSBBBB.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "#unzip\n",
    "!gunzip -c $data_dir/$c_filename > $data_dir/$u_filename\n",
    "!ls -l $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "List=pickle.load(open(data_dir+'/'+u_filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12740\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|elevation|latitude|longitude|measurement|    station|undefs|              vector|  year|   label|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "|   2449.1| 36.0333|-105.8167|       TMAX|USC00299113|     6|[40 56 30 54 40 5...|1910.0|SSSSBBBB|\n",
      "|   2449.1| 36.0333|-105.8167|       TMAX|USC00299113|    21|[80 C9 30 D4 00 4...|1911.0|SSSSBBBB|\n",
      "+---------+--------+---------+-----------+-----------+------+--------------------+------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Weather_df=sqlContext.createDataFrame(List)\n",
    "print(Weather_df.count())\n",
    "Weather_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.,  67.,  50., -11., -83., -89., -39.,   0.,  22.,  67.,  50.,\n",
       "        83.,  17.,  44.,  72.,  28.,  22.,  -6.,  44.,  61.,  78., 100.,\n",
       "       100.,  nan,  78.,  44.,  39.,  44.,  50.,  61.,  39.,  78.,  17.,\n",
       "        17.,  nan,  nan,  33.,  44.,  28., -28.,  17.,  17.,  28.,  78.,\n",
       "       111.,  nan,  44., -72.,   0., -11., -17.,  39.,  78.,  50., 100.,\n",
       "        72.,  50.,  61.,  56., 122., 150., 150., 156., 189., 183., 178.,\n",
       "       133., 150.,  94., 128., 133., 156., 100.,  94., 122., 144., 167.,\n",
       "       172., 167., 172., 183., 161., 167., 150., 122., 150., 133.,  17.,\n",
       "        44., 106., 128., 150., 200., 117., 144., 100., 122., 189.,  94.,\n",
       "       139., 150., 156., 106., 111.,  39.,  78.,  39., 167., 189., 211.,\n",
       "       183., 183., 183., 189., 206., 217., 228., 239., 217., 228., 150.,\n",
       "        nan, 150., 167., 167., 178., 183., 211., 239., 250., 244., 228.,\n",
       "       244., 183., 200., 172., 128., 133., 194., 178., 133., 117., 172.,\n",
       "       228., 183., 217., 239., 283., 278., 267., 261., 272., 261., 250.,\n",
       "       261., 267., 272., 283., 267., 250., 228., 239., 272., 239., 233.,\n",
       "       239., 261., 267., 272., 289., 272., 267., 261., 244., 239., 206.,\n",
       "       222., 222., 233., 239., 244., 267., 306., 267., 272., 272., 267.,\n",
       "       294., 289., 283., 272., 283., 294., 239., 272., 278., 289., 283.,\n",
       "       267., 294., 283., 294., 289., 294., 300., 311., 311., 306., 256.,\n",
       "       250., 261., 256., 250., 256., 261., 222., 200., 239., 261., 261.,\n",
       "       239., 233., 239., 211., 233., 239., 256., 239., 244., 228., 244.,\n",
       "       267., 283., 278., 272., 267., 200., 244., 267., 267., 261., 172.,\n",
       "       211., 206., 233., 222., 222., 239., 239., 239., 267., 206., 261.,\n",
       "       250., 261., 239., 178., 211., 222., 233., 239., 239., 239., 239.,\n",
       "       233., 228., 233., 233., 222., 228., 161., 222., 239., 233., 206.,\n",
       "       183., 183., 211., 233., 239., 267., 250., 222., 211., 222., 183.,\n",
       "       183., 183., 206., 156., 150.,  44.,  72.,  61., 117., 133., 156.,\n",
       "       183., 172., 117., 111., 156., 161., 167., 161., 156., 161., 128.,\n",
       "        61., 122., 139., 156., 144., 161., 178., 150., 156.,  72.,  78.,\n",
       "        61.,  89.,  94.,  89.,  83.,  78.,  83., 128., 156., 150., 122.,\n",
       "        72.,  72., 111., 128., 150., 139., 139., 144.,  50.,  67.,  83.,\n",
       "        89.,  94., 128., 122.,  94.,  83.,  11.,  50.,  67.,  83.,  89.,\n",
       "        44.,  39.,  28.,  11.,  11.,  44.,  28.,  22.,  17.,  11., -11.,\n",
       "        -6.,  nan], dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row,=Weather_df.take(1)\n",
    "V=row.vector\n",
    "V\n",
    "\n",
    "import sys\n",
    "sys.path.append('./lib')\n",
    "from numpy_pack import unpackArray\n",
    "\n",
    "import numpy as np\n",
    "unpackArray(V,data_type=np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing all rows in a DataFrame\n",
    "using UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-----------+-----------+------+------+--------+--------------------+\n",
      "|elevation|latitude|longitude|measurement|    station|undefs|  year|   label|               Array|\n",
      "+---------+--------+---------+-----------+-----------+------+------+--------+--------------------+\n",
      "|   2449.1| 36.0333|-105.8167|       TMAX|USC00299113|     6|1910.0|SSSSBBBB|[100.0, 67.0, 50....|\n",
      "|   2449.1| 36.0333|-105.8167|       TMAX|USC00299113|    21|1911.0|SSSSBBBB|[-11.0, -67.0, 28...|\n",
      "+---------+--------+---------+-----------+-----------+------+------+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType,FloatType, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def Unpack(V):\n",
    "    A=[float(x) for x in unpackArray(V,data_type=np.float16)]\n",
    "    return A\n",
    "Unpack_udf = udf(Unpack,ArrayType(FloatType(),True))\n",
    "sqlContext.registerFunction(\"UnpackArray\", Unpack,ArrayType(FloatType()))\n",
    "sqlContext.registerDataFrameAsTable(Weather_df,'weather')\n",
    "\n",
    "Weather_df2=Weather_df.withColumn(\"Array\", Unpack_udf(Weather_df.vector)).drop('vector')\n",
    "Weather_df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "PCA_using_numpy for HW3",
  "notebookId": 85286,
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
