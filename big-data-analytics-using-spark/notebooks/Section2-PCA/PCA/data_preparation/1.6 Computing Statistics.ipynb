{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 24 ms, total: 188 ms\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pwd\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(pyFiles=['/mnt/workspace/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/notebooks/Section2-PCA/PCA/data_preparation/lib/numpy_pack.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "STAT_Descriptions=[\n",
    "('SortedVals', 'Sample of values', 'vector whose length varies between measurements'),\n",
    " ('UnDef', 'sample of number of undefs per row', 'vector whose length varies between measurements'),\n",
    " ('mean', 'mean value', ()),\n",
    " ('std', 'std', ()),\n",
    " ('low100', 'bottom 1%', ()),\n",
    " ('high100', 'top 1%', ()),\n",
    " ('low1000', 'bottom 0.1%', ()),\n",
    " ('high1000', 'top 0.1%', ()),\n",
    " ('E', 'Sum of values per day', (365,)),\n",
    " ('NE', 'count of values per day', (365,)),\n",
    " ('Mean', 'E/NE', (365,)),\n",
    " ('O', 'Sum of outer products', (365, 365)),\n",
    " ('NO', 'counts for outer products', (365, 365)),\n",
    " ('Cov', 'O/NO', (365, 365)),\n",
    " ('Var', 'The variance per day = diagonal of Cov', (365,)),\n",
    " ('eigval', 'PCA eigen-values', (365,)),\n",
    " ('eigvec', 'PCA eigen-vectors', (365, 365))\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM parquet.`/weather.parquet`\n",
      "\tWHERE measurement='TMAX'\n",
      "\tor measurement='TMIN'\n",
      "\tor measurement='TOBS'\n",
      "\tor measurement='SNOW'\n",
      "\tor measurement='SNWD'\n",
      "\tor measurement='PRCP'\n"
     ]
    }
   ],
   "source": [
    "US_Weather_parquet='/weather.parquet'\n",
    "measurements=['TMAX','TMIN','TOBS','SNOW','SNWD','PRCP']\n",
    "Query=\"SELECT * FROM parquet.`%s`\\n\\tWHERE \"%US_Weather_parquet+\"\\n\\tor \".join([\"measurement='%s'\"%m for m in measurements])\n",
    "print Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6b7f082c7060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'took'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'seconds'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "t=time()\n",
    "df = sqlContext.sql(Query).cache()\n",
    "print(df.count()\n",
    "print 'took',time()-t,'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "t=time()\n",
    "\n",
    "N=sc.defaultParallelism\n",
    "print 'Number of executors=',N\n",
    "rdd0=df.rdd.map(lambda row:(str(row['station']),((str(row['measurement'])\\\n",
    "                        ,row['year']),np.array([np.float64(row[str(i)]) for i in range(1,366)])))).cache()#.repartition(N)\n",
    "print 'took',time()-t,'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def F(row):\n",
    "    return (str(row['station']),((str(row['measurement'])\\\n",
    "                        ,row['year']),np.array([np.float64(row[str(i)]) for i in range(1,366)])))\n",
    "row,=df.take(1)\n",
    "#print F(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=time()\n",
    "print rdd0.count()\n",
    "print 'took',time()-t,'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=time()\n",
    "print rdd0.count()\n",
    "print 'took',time()-t,'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=time()\n",
    "print rdd0.repartition(N).count()\n",
    "print 'took',time()-t,'seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the overall distribution of values and the distribution of the number of nan per year\n",
    "def find_percentiles(SortedVals,percentile):\n",
    "  L=len(SortedVals)/percentile\n",
    "  return SortedVals[L],SortedVals[-L]\n",
    "  \n",
    "def computeOverAllDist(rdd0):\n",
    "  UnDef=np.array(rdd0.map(lambda row:sum(np.isnan(row))).sample(False,0.01).collect())\n",
    "  flat=rdd0.flatMap(lambda v:list(v)).filter(lambda x: not np.isnan(x)).cache()\n",
    "  count,S1,S2=flat.map(lambda x: np.float64([1,x,x**2]))\\\n",
    "                  .reduce(lambda x,y: x+y)\n",
    "  mean=S1/count\n",
    "  std=np.sqrt(S2/count-mean**2)\n",
    "  Vals=flat.sample(False,0.0001).collect()\n",
    "  SortedVals=np.array(sorted(Vals))\n",
    "  low100,high100=find_percentiles(SortedVals,100)\n",
    "  low1000,high1000=find_percentiles(SortedVals,1000)\n",
    "  return {'UnDef':UnDef,\\\n",
    "          'mean':mean,\\\n",
    "          'std':std,\\\n",
    "          'SortedVals':SortedVals,\\\n",
    "          'low100':low100,\\\n",
    "          'high100':high100,\\\n",
    "          'low1000':low100,\\\n",
    "          'high1000':high1000\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "STAT={}  # dictionary storing the statistics for each measurement\n",
    "Clean_Tables={}\n",
    "\n",
    "for meas in measurements:\n",
    "  t=time()\n",
    "  Query=\"SELECT * FROM parquet.`%s`\\n\\tWHERE measurement = '%s'\"%(US_Weather_parquet,meas)\n",
    "  print Query\n",
    "  df = sqlContext.sql(Query)\n",
    "  rdd0=df.rdd.map(lambda row:(row['station'],((row['measurement'],row['year']),np.array([np.float64(row[str(i)]) for i in range(1,366)])))).cache()\n",
    "\n",
    "  rdd1=rdd0.sample(False,1)\\\n",
    "           .map(lambda (key,val): val[1])\\\n",
    "           .cache()\\\n",
    "           .repartition(N)\n",
    "  print rdd1.count()\n",
    "\n",
    "  #get basic statistics\n",
    "  STAT[meas]=computeOverAllDist(rdd1)   # Compute the statistics \n",
    "  low1000 = STAT[meas]['low1000']  # unpack the extreme values statistics\n",
    "  high1000 = STAT[meas]['high1000']\n",
    "\n",
    "  #clean up table from extreme values and from rows with too many undefinde entries.\n",
    "  rdd2=rdd1.map(lambda V: np.array([x if (x>low1000-1) and (x<high1000+1) else np.nan for x in V]))\n",
    "  rdd3=rdd2.filter(lambda row:sum(np.isnan(row))<50)\n",
    "  Clean_Tables[meas]=rdd3.cache().repartition(N)\n",
    "  C=Clean_Tables[meas].count()\n",
    "  print 'for measurement %s, we get %d clean rows'%(meas,C)\n",
    "\n",
    "  # compute covariance matrix\n",
    "  OUT=computeCov(Clean_Tables[meas])\n",
    "\n",
    "  #find PCA decomposition\n",
    "  eigval,eigvec=LA.eig(OUT['Cov'])\n",
    "\n",
    "  # collect all of the statistics in STAT[meas]\n",
    "  STAT[meas]['eigval']=eigval\n",
    "  STAT[meas]['eigvec']=eigvec\n",
    "  STAT[meas].update(OUT)\n",
    "\n",
    "  # print summary of statistics\n",
    "  print 'the statistics for %s consists of:'%meas\n",
    "  for key in STAT[meas].keys():\n",
    "    e=STAT[meas][key]\n",
    "    if type(e)==list:\n",
    "      print key,'list',len(e)\n",
    "    elif type(e)==np.ndarray:\n",
    "      print key,'ndarray',e.shape\n",
    "    elif type(e)==np.float64:\n",
    "      print key,'scalar'\n",
    "    else:\n",
    "      print key,'Error type=',type(e)\n",
    "  print 'time for',meas,'is',time()-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "def dumpS3(object,S3dir,filename):\n",
    "    dump(object,open(filename,'wb'))\n",
    "    !ls -l $filename\n",
    "    s3helper.local_to_s3(filename, S3dir+filename)\n",
    "dumpS3((STAT,STAT_Descriptions),'/Weather/','STAT1.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STAT.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample Stations\n",
    "Generate a sample of stations, for each one store all available year X measurement pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd0.take(10) # test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "groups=rdd0.groupByKey().cache()\n",
    "print 'number of stations=',groups.count()\n",
    "\n",
    "groups1=groups.sample(False,0.01).collect()\n",
    "groups2=[(g[0],[e for e in g[1]]) for g in groups1]\n",
    "\n",
    "dumpS3(groups2,'/Weather/','SampleStations.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "group_Sample=rdd0.sample(False,0.001).groupByKey().mapValues(list).cache()\n",
    "group_Sample.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Means=rdd0.aggregateByKey((np.zeros(365),1),\\\n",
    "                          lambda S,D: sumWithNan(S,(D[1],1)),\\\n",
    "                          lambda S1,S2: sumWithNanithNan(S1,S2))\\\n",
    ".cache()#.repartition(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Means.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "groups=rdd0.groupByKey().cache()\n",
    "print 'number of stations=',groups.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "PCA_using_numpy for HW3",
  "notebookId": 85286
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
