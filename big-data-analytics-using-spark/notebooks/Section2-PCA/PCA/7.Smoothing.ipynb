{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Measurement sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "CPU times: user 1.59 s, sys: 64 ms, total: 1.65 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math\n",
    "%pylab inline\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.numpy_pack import packArray,unpackArray\n",
    "from lib.spark_PCA import computeCov\n",
    "from lib.computeStatistics import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data through open bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state='NY'\n",
    "EMR=True\n",
    "if not EMR:\n",
    "    data_dir='../../Data/Weather'\n",
    "\n",
    "    tarname=state+'.tgz'\n",
    "    parquet=state+'.parquet'\n",
    "\n",
    "    !rm -rf $data_dir/$tarname\n",
    "\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s > %s/%s\"%(tarname,data_dir,tarname)\n",
    "    print(command)\n",
    "    !$command\n",
    "    !ls -lh $data_dir/$tarname\n",
    "    cur_dir,=!pwd\n",
    "    %cd $data_dir\n",
    "    !tar -xzf $tarname\n",
    "    !du ./$parquet\n",
    "    %cd $cur_dir\n",
    "\n",
    "    #read statistics\n",
    "    filename='STAT_%s.pickle'%state\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s.gz > %s/%s.gz\"%(filename,data_dir,filename)\n",
    "    print(command)\n",
    "    !$command\n",
    "    \n",
    "    filename='US_stations.tsv.gz'\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/Info/%s > %s/%s\"%(filename,data_dir,filename)\n",
    "    print(command)\n",
    "    !$command\n",
    "    filename_no_gz = filename[:-3]\n",
    "    !gunzip -f $data_dir/$filename\n",
    "    !ls -lh $data_dir/US_stations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data when on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - hadoop hadoop          0 2018-04-22 15:31 /weather/US_Weather_with_smoothed.parquet\n",
      "drwxr-xr-x   - hadoop hadoop          0 2018-04-22 15:20 /weather/US_stations.parquet\n",
      "drwxr-xr-x   - hadoop hadoop          0 2018-04-22 15:20 /weather/US_weather.parquet\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "|    Station|dist_coast|latitude|longitude|elevation|state|            name|\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "|USC00341900|   739.956|    36.3| -96.4667|    242.3|   OK|       CLEVELAND|\n",
      "|USC00428114|    908.22|    40.1|-111.6667|   1409.1|   UT|SPANISH FORK 1 S|\n",
      "|USC00165926|   23.8801| 29.7853| -90.1158|      0.9|   LA|   MARRERO 9 SSW|\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-----------+----+--------------------+\n",
      "|    Station|Measurement|Year|              Values|\n",
      "+-----------+-----------+----+--------------------+\n",
      "|CA001126150|       PRCP|1941|[00 7E 00 7E 00 7...|\n",
      "|CA001126150|       PRCP|1942|[00 00 80 4A 00 0...|\n",
      "+-----------+-----------+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 64 ms, sys: 8 ms, total: 72 ms\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if EMR:\n",
    "    !hdfs dfs -ls /weather/\n",
    "\n",
    "    stations_df=sqlContext.read.parquet('/weather/US_stations.parquet')\n",
    "    stations_df.show(3)\n",
    "\n",
    "    weather_df=sqlContext.read.parquet('/weather/US_weather.parquet')\n",
    "    weather_df.show(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3259494"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3259494\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|dist_coast|latitude|longitude|elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "|CA001126150|       PRCP|1941|[00 7E 00 7E 00 7...|   226.659|  49.467|   -119.6|    344.0|  NaN|PENTICTON AIRPORT|\n",
      "|CA001126150|       PRCP|1942|[00 00 80 4A 00 0...|   226.659|  49.467|   -119.6|    344.0|  NaN|PENTICTON AIRPORT|\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "jdf=weather_df.join(stations_df,on='Station',how='left')\n",
    "print(jdf.count())\n",
    "jdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Smoothing by convolving with gaussian window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load lib/numpy_pack.py\n",
    "import numpy as np\n",
    "\"\"\"Code for packing and unpacking a numpy array into a byte array.\n",
    "   the array is flattened if it is not 1D.\n",
    "   This is intended to be used as the interface for storing \n",
    "   \n",
    "   This code is intended to be used to store numpy array as fields in a dataframe and then store the \n",
    "   dataframes in a parquet file.\n",
    "\"\"\"\n",
    "\n",
    "def packArray(a):\n",
    "    \"\"\"\n",
    "    pack a numpy array into a bytearray that can be stored as a single \n",
    "    field in a spark DataFrame\n",
    "\n",
    "    :param a: a numpy ndarray \n",
    "    :returns: a bytearray\n",
    "    :rtype:\n",
    "\n",
    "    \"\"\"\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "\n",
    "\n",
    "def unpackArray(x,data_type=np.int16):\n",
    "    \"\"\"\n",
    "    unpack a bytearray into a numpy.ndarray\n",
    "\n",
    "    :param x: a bytearray\n",
    "    :param data_type: The dtype of the array. This is important because if determines how many bytes go into each entry in the array.\n",
    "    :returns: a numpy array\n",
    "    :rtype: a numpy ndarray of dtype data_type.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.frombuffer(x,dtype=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.convolution import convolve\n",
    "from scipy import signal\n",
    "from copy import deepcopy\n",
    "#using astrophy.convolution.convolve and not scipy.signal.convolve because the first can handle nans.\n",
    "\n",
    "order=101\n",
    "std=20\n",
    "window = signal.gaussian(order, std=std)\n",
    "window/=sum(window)\n",
    "\n",
    "def Smoother(item):\n",
    "    key,List = item\n",
    "    \n",
    "    sorted_List=sorted(List,key=lambda row:row['Year'])\n",
    "    L=[(Row['Year'],unpackArray(Row['Values'],np.float16)) for Row in sorted_List]\n",
    "    \n",
    "    orig=np.stack([V[1] for V in L])\n",
    "    print('orig.shape=',orig.shape)\n",
    "    orig_shape=orig.shape\n",
    "    orig=orig.flatten()\n",
    "    smoothed = convolve(orig, window)\n",
    "    smoothed=np.reshape(smoothed,orig_shape)\n",
    "    smoothed=np.array(smoothed,dtype=np.float16)\n",
    "    print('smoothed.shape=',smoothed.shape)\n",
    "\n",
    "    #create a list of Rows with the smoothed \n",
    "    new_L = []\n",
    "    new_name = List[0]['Measurement']+'_s%d'%std\n",
    "    for i in range(len(List)):\n",
    "        row=List[i]\n",
    "        new_row=Row(Station = row['Station'],\\\n",
    "                    Measurement=new_name,\\\n",
    "                    Year = int(row['Year']),\\\n",
    "                    Values = packArray(smoothed[i,:]),\\\n",
    "                    dist_coast = float(row['dist_coast']),\\\n",
    "                    latitude = float(row['latitude']),\\\n",
    "                    longitude = float(row['longitude']),\\\n",
    "                    elevation = float(row['elevation']),\\\n",
    "                    state = row['state'],\\\n",
    "                    name = row['name'])\n",
    "\n",
    "        new_L.append(new_row)\n",
    "    return new_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig.shape= (11, 365)\n",
      "smoothed.shape= (11, 365)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), Year=1908, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\r-\\xb7-t.D/\\x150\\x930\\x1c1\\xb31V2\\x073\\xc73K4\\xba415\\xb0576\\xc66^7\\xfd7S8\\xaa8\\x069e9\\xc79,:\\x94:\\xfe:k;\\xd9;$<\\\\<\\x93<\\xcb<\\x02=8=l=\\xa0=\\xd1=\\x00>->W>~>\\xa2>\\xc2>\\xde>\\xf7>\\x0b?\\x1b?&?-?/?-?&?\\x1b?\\x0b?\\xf7>\\xde>\\xc2>\\xa2>\\xb5>\\x96>t>P>'), Year=1909, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b\"*>\\x04>\\xdc=\\xb4=\\x8d=e=>=\\x18=\\xf3<\\xd0<\\xaf<\\x90<s<Y<B<-<\\x1c<\\x0e<\\x02<\\xf4;\\xea;\\xe5;\\xe7;\\xed;\\xf9;\\x05<\\x0f<\\x1c<*<9<J<[<m<\\x7f<\\x91<\\xa3<\\xb4<\\xc4<\\x8c<\\xa2<\\xb5<\\xc6<\\xd4<\\xde<\\xe6<\\xeb<\\xed<\\xeb<\\xe6<\\xde<\\xd4<\\xc6<\\xb5<\\xa2<\\x8c<s<X<<<\\x1d<\\xf9;\\xb5;o;\\';\\xdd:\\x92:F:\\xf99\\xad9a9\\x159\\xcb8\\x828;8\\xeb7d7\\xe26e6\\xed5z5\\r5\\xa54B4\\xcc3\\x1e3{2\\xe21U1\\xd10X0\\xcf/\\x01/E.\\x98-\\xfb,l,\\xd6+\\xec*\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"), Year=1910, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), Year=1911, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), Year=1912, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'), Year=1913, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xe7!\\xae\"\\x8a#?$\\xc5$X%\\xf9%+*\\xea*\\xbb+P,\\xcd,U-\\xe7-\\x86.1/\\xe8/V0\\xbf0.1\\xa41!2\\xa42.3\\xbf3+4y4\\xca4\\x1d5s5\\xcb5$6\\x7f6\\xdb6'), Year=1914, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b'77\\x947\\xf07&8S8\\x7f8\\xaa8\\xd48\\xfc8\"9F9h9\\x879\\xa39\\xbc9\\xd29\\xe59~:\\x9c:\\xb8:\\xd3:\\xec:\\x03;\\x1a;/;D;X;l;\\x7f;\\x93;\\xa8;\\xbd;\\xd3;\\xeb;\\x02<\\x0f<\\x1e<-<><O<b<v<\\x8b<\\xa2<\\xb9<\\xd2<\\xeb<\\x05=\\x1f=:=U=p=\\x8b=\\xa6=\\xbf=\\xd8=\\xef=\\x05>\\x19>+>:>H>R>Z>_>a>`>P>J>?>2> >\\x0c>\\xf4=\\xc5=\\xaa=\\x8b=j=F= =\\xf7<\\xcd<\\xa1<t<F<\\x18<\\xd0;q;\\x12;\\xb3:U:\\xf99\\x9e9E9\\xef8\\x9b8J8\\xf77b7\\xd36J6\\xc95O5\\xdb4o4\\t4U3\\xa42\\x012i1\\xde0]0\\xcf/\\xf8.4.\\x83-\\xe2,P,\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xad%l&@\\'\\x15(\\x96(#)\\xbe)g*\\x1f+\\xe6+_,\\xd3,P-\\xd5-d.\\xfc.\\x9d/#0}0\\xdb0>1\\xa41\\x0f2~2\\xef2d3\\xdc3+4h4\\xa74\\xe54$5b5\\xa05\\xdd5\\x186Q6\\x896\\xbe6\\xf06 7K7t7\\x987\\xb87\\xd37\\xea7\\xfb7\\x048\\x088\\t8\\x088\\x048\\xfb7'), Year=1915, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b\"\\xea7\\xd37\\xb87\\x987t7K7 7\\xf06\\xbe6\\x896Q6\\x186\\xdd5\\xa05b5$5\\xe54\\xa74h4+4\\xdc3d3\\xef2~2\\x0f2\\xa41>1\\xdb0}0#0\\x9d/\\xfc.d.\\xd5-P-\\xd3,_,\\xe6+\\x1f+g*\\xbe)#)\\x96(\\x15(@\\'l&\\xad%\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"), Year=1916, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xca)\\x8d*e+\\xfa0\\x9d1Q2\\x183\\xf13p4\\xf14\\xd95~617\\xf17`8\\xcf8F9\\xc59L:\\xda:q;\\x08<[<\\xb1<\\x0c=i=\\xca=.>\\x94>\\xfd>g?\\xd3? @V@\\x8d@\\xc3@\\xf9@-AaA\\x93A'), Year=1917, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX'),\n",
       " Row(Measurement='SNOW_s20', Station='USC00413644', Values=bytearray(b\"\\xc3A\\xf1A\\x1dBFBlB\\x8fB\\xaeB\\xcaB\\xe2B\\xf5B\\x05C\\x10C\\x17C\\x1aC\\x18C\\x12C\\x07C\\xf8B\\xe5B\\xceB\\xb3B\\x94BrBLB$B\\xf8A\\xcbA\\x9bAiA6A\\x02A\\xcd@\\x97@`@*@\\xe7?|?\\x12?\\xa9>C>\\xdf=~= =\\xc5<n<\\x1a<\\x96;\\xfe:n:\\xe69f9\\xee8}8\\x148e7\\xb06\\x086m5\\xdf4]4\\xcc3\\xac1\\x0e1\\x7f0\\xbe)#)\\x96(\\x15(@\\'l&\\xad%\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"), Year=1918, dist_coast=303.4989929199219, elevation=159.6999969482422, latitude=31.649999618530273, longitude=-97.2166976928711, name='GORHAM', state='TX')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Smoother(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station = row['Station'],\\\n",
      "Measurement = row['Measurement'],\\\n",
      "Year = row['Year'],\\\n",
      "Values = row['Values'],\\\n",
      "dist_coast = row['dist_coast'],\\\n",
      "latitude = row['latitude'],\\\n",
      "longitude = row['longitude'],\\\n",
      "elevation = row['elevation'],\\\n",
      "state = row['state'],\\\n",
      "name = row['name'],\\\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(jdf.schema.fields)):\n",
    "    s=jdf.schema.fields[i]\n",
    "    print(\"%s = row['%s'],\\\\\"%(s.name,s.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyVal=jdf.rdd.map(lambda row:((row['Station'],row['Measurement']),[row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Reduced=keyVal.reduceByKey(lambda x,y:x+y)\n",
    "item = Reduced.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig.shape= (11, 365)\n",
      "smoothed.shape= (11, 365)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_L = Smoother(item)\n",
    "len(new_L[0]['Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_L),len(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Smoothed=Reduced.flatMap(Smoother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X=Smoothed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 1.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3259494"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Smoothed.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=Smoothed.first()\n",
    "Y=jdf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.int16, int)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X['Year']),type(Y['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 7.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3259494"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elevation', float),\n",
       " ('Values', bytearray),\n",
       " ('Year', int),\n",
       " ('name', str),\n",
       " ('dist_coast', float),\n",
       " ('longitude', float),\n",
       " ('Measurement', str),\n",
       " ('latitude', float),\n",
       " ('Station', str),\n",
       " ('state', str)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row,=Smoothed.take(1)\n",
    "D=row.asDict()\n",
    "[(name,type(D[name])) for name in D.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USC00413644'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['Station']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing order of dataframe columns\n",
    "\n",
    "It turns out that to take the union of two dataframes, the order of the columns needs to be the same, it is not enough if the columns match by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jdf.schema.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{pyspark.sql.types.StringType: str,\n",
       " pyspark.sql.types.IntegerType: int,\n",
       " pyspark.sql.types.FloatType: float,\n",
       " pyspark.sql.types.BinaryType: bytearray}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, FloatType, BinaryType, IntegerType\n",
    "typeMap={StringType:str,\n",
    "         FloatType:float,\n",
    "         IntegerType:int,\n",
    "         BinaryType:bytearray}\n",
    "typeMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station StringType\n",
      "Measurement StringType\n",
      "Year IntegerType\n",
      "Values BinaryType\n",
      "dist_coast FloatType\n",
      "latitude FloatType\n",
      "longitude FloatType\n",
      "elevation FloatType\n",
      "state StringType\n",
      "name StringType\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(jdf.schema.fields)):\n",
    "    s=jdf.schema.fields[i]\n",
    "    print(s.name,s.dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 50.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#Using the schema from JDF causes an error, it works without specifying a schema explicitly (the rows have the type info)\n",
    "smoothed_jdf=sqlContext.createDataFrame(Smoothed,verifySchema=True)\n",
    "smoothed_jdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+----+------------------+------------------+--------+-------------------+--------------+-----+\n",
      "|Measurement|    Station|              Values|Year|        dist_coast|         elevation|latitude|          longitude|          name|state|\n",
      "+-----------+-----------+--------------------+----+------------------+------------------+--------+-------------------+--------------+-----+\n",
      "|   PRCP_s20|USC00051745|[FE 40 86 41 38 4...|1968|1005.4199829101562|1969.9000244140625|   39.25|-107.93329620361328|COLLBRAN 3 ENE|   CO|\n",
      "+-----------+-----------+--------------------+----+------------------+------------------+--------+-------------------+--------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoothed_jdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|dist_coast|latitude|longitude|elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "|CA001126150|       PRCP|1941|[00 7E 00 7E 00 7...|   226.659|  49.467|   -119.6|    344.0|  NaN|PENTICTON AIRPORT|\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldNames=jdf.schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Station',\n",
       " 'Measurement',\n",
       " 'Year',\n",
       " 'Values',\n",
       " 'dist_coast',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'elevation',\n",
       " 'state',\n",
       " 'name']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+------------------+--------+-------------------+------------------+-----+--------------+\n",
      "|    Station|Measurement|Year|              Values|        dist_coast|latitude|          longitude|         elevation|state|          name|\n",
      "+-----------+-----------+----+--------------------+------------------+--------+-------------------+------------------+-----+--------------+\n",
      "|USC00051745|   PRCP_s20|1968|[FE 40 86 41 38 4...|1005.4199829101562|   39.25|-107.93329620361328|1969.9000244140625|   CO|COLLBRAN 3 ENE|\n",
      "+-----------+-----------+----+--------------------+------------------+--------+-------------------+------------------+-----+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoothed_jdf=smoothed_jdf.select(fieldNames)\n",
    "smoothed_jdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- elevation: float (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoothed_jdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuck here\n",
    "Managed to get the two dataframes to have the same coumn order. However, there are differences in the types: float vs double and integer vs. long, that I dont know how to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine original and smoothed rows into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o189.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 27.0 failed 4 times, most recent failure: Lost task 25.3 in stage 27.0 (TID 159, ip-10-129-250-3.ec2.internal, executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 671, in prepare\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1421, in verify\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1402, in verify_struct\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1421, in verify\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1347, in verify_integer\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1310, in verify_acceptable_types\nTypeError: field Year: IntegerType can not accept object bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\xdc\\\\\\xbc\\\\d\\\\a\\\\m\\\\{\\\\\\x86\\\\\\x83\\\\\\x80\\\\s\\\\p\\\\p\\\\p\\\\o\\\\l\\\\h\\\\f\\\\f\\\\f\\\\e\\\\c\\\\c\\\\b\\\\\\\\\\\\Y\\\\X\\\\W\\\\U\\\\S\\\\Q\\\\O\\\\M\\\\K\\\\G\\\\E\\\\B\\\\>\\\\;\\\\8\\\\6\\\\3\\\\/\\\\+\\\\(\\\\$\\\\ \\\\\\x1d\\\\\\x19\\\\\\x14\\\\\\x10\\\\\\x0c\\\\\\x07\\\\\\x02\\\\\\xfa[\\xf0[\\xe4[\\xda[\\xcf[\\xc4[\\xb9[\\xad[\\xa1[\\x95[\\x88[{[m[_[P[B[3[$[\\x15[\\x05[\\xf5Z\\xe4Z\\xd3Z\\xc2Z\\xb0Z\\x9fZ\\x8dZ|ZjZYZGZ5Z#Z\\x11Z\\xfeY\\xecY\\xdaY\\xc8Y\\xb7Y\\xa5Y\\x94Y\\x83YqY`YOY>Y.Y\\x1dY\\x0cY\\xfbX\\xeaX\\xdaX\\xcbX\\xbbX\\xacX\\x9dX\\x8eX\\x80XqXbXSXEX7X)X\\x1cX\\x0eX\\x01X\\xe8W\\xcfW\\xb6W\\x9eW\\x88WrWZWBW-W\\x18W\\x04W\\xf1V\\xdfV\\xceV\\xbfV\\xb0V\\xa2V\\x95V\\x8bV\\x80VvVmVfV`V]VZVWVUVTVVVXVZV_VfVnVxV') in type <class 'bytearray'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1750)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1738)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1737)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1737)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1920)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1909)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 671, in prepare\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1421, in verify\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1402, in verify_struct\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1421, in verify\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1347, in verify_integer\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1310, in verify_acceptable_types\nTypeError: field Year: IntegerType can not accept object bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\xdc\\\\\\xbc\\\\d\\\\a\\\\m\\\\{\\\\\\x86\\\\\\x83\\\\\\x80\\\\s\\\\p\\\\p\\\\p\\\\o\\\\l\\\\h\\\\f\\\\f\\\\f\\\\e\\\\c\\\\c\\\\b\\\\\\\\\\\\Y\\\\X\\\\W\\\\U\\\\S\\\\Q\\\\O\\\\M\\\\K\\\\G\\\\E\\\\B\\\\>\\\\;\\\\8\\\\6\\\\3\\\\/\\\\+\\\\(\\\\$\\\\ \\\\\\x1d\\\\\\x19\\\\\\x14\\\\\\x10\\\\\\x0c\\\\\\x07\\\\\\x02\\\\\\xfa[\\xf0[\\xe4[\\xda[\\xcf[\\xc4[\\xb9[\\xad[\\xa1[\\x95[\\x88[{[m[_[P[B[3[$[\\x15[\\x05[\\xf5Z\\xe4Z\\xd3Z\\xc2Z\\xb0Z\\x9fZ\\x8dZ|ZjZYZGZ5Z#Z\\x11Z\\xfeY\\xecY\\xdaY\\xc8Y\\xb7Y\\xa5Y\\x94Y\\x83YqY`YOY>Y.Y\\x1dY\\x0cY\\xfbX\\xeaX\\xdaX\\xcbX\\xbbX\\xacX\\x9dX\\x8eX\\x80XqXbXSXEX7X)X\\x1cX\\x0eX\\x01X\\xe8W\\xcfW\\xb6W\\x9eW\\x88WrWZWBW-W\\x18W\\x04W\\xf1V\\xdfV\\xceV\\xbfV\\xb0V\\xa2V\\x95V\\x8bV\\x80VvVmVfV`V]VZVWVUVTVVVXVZV_VfVnVxV') in type <class 'bytearray'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d47c9dda9d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Combined_jdf=jdf.union(smoothed_jdf)\\nCombined_jdf.count()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/anaconda/envs/ipykernel_py34/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda/envs/ipykernel_py34/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda/envs/ipykernel_py34/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o189.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 27.0 failed 4 times, most recent failure: Lost task 25.3 in stage 27.0 (TID 159, ip-10-129-250-3.ec2.internal, executor 10): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 671, in prepare\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1421, in verify\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1402, in verify_struct\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1421, in verify\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1347, in verify_integer\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1310, in verify_acceptable_types\nTypeError: field Year: IntegerType can not accept object bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\xdc\\\\\\xbc\\\\d\\\\a\\\\m\\\\{\\\\\\x86\\\\\\x83\\\\\\x80\\\\s\\\\p\\\\p\\\\p\\\\o\\\\l\\\\h\\\\f\\\\f\\\\f\\\\e\\\\c\\\\c\\\\b\\\\\\\\\\\\Y\\\\X\\\\W\\\\U\\\\S\\\\Q\\\\O\\\\M\\\\K\\\\G\\\\E\\\\B\\\\>\\\\;\\\\8\\\\6\\\\3\\\\/\\\\+\\\\(\\\\$\\\\ \\\\\\x1d\\\\\\x19\\\\\\x14\\\\\\x10\\\\\\x0c\\\\\\x07\\\\\\x02\\\\\\xfa[\\xf0[\\xe4[\\xda[\\xcf[\\xc4[\\xb9[\\xad[\\xa1[\\x95[\\x88[{[m[_[P[B[3[$[\\x15[\\x05[\\xf5Z\\xe4Z\\xd3Z\\xc2Z\\xb0Z\\x9fZ\\x8dZ|ZjZYZGZ5Z#Z\\x11Z\\xfeY\\xecY\\xdaY\\xc8Y\\xb7Y\\xa5Y\\x94Y\\x83YqY`YOY>Y.Y\\x1dY\\x0cY\\xfbX\\xeaX\\xdaX\\xcbX\\xbbX\\xacX\\x9dX\\x8eX\\x80XqXbXSXEX7X)X\\x1cX\\x0eX\\x01X\\xe8W\\xcfW\\xb6W\\x9eW\\x88WrWZWBW-W\\x18W\\x04W\\xf1V\\xdfV\\xceV\\xbfV\\xb0V\\xa2V\\x95V\\x8bV\\x80VvVmVfV`V]VZVWVUVTVVVXVZV_VfVnVxV') in type <class 'bytearray'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1750)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1738)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1737)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1737)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1920)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1909)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1524409064425_0004/container_1524409064425_0004_01_000012/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 671, in prepare\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1421, in verify\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1402, in verify_struct\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1421, in verify\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1347, in verify_integer\n  File \"/usr/lib/spark/python/pyspark/sql/types.py\", line 1310, in verify_acceptable_types\nTypeError: field Year: IntegerType can not accept object bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\xdc\\\\\\xbc\\\\d\\\\a\\\\m\\\\{\\\\\\x86\\\\\\x83\\\\\\x80\\\\s\\\\p\\\\p\\\\p\\\\o\\\\l\\\\h\\\\f\\\\f\\\\f\\\\e\\\\c\\\\c\\\\b\\\\\\\\\\\\Y\\\\X\\\\W\\\\U\\\\S\\\\Q\\\\O\\\\M\\\\K\\\\G\\\\E\\\\B\\\\>\\\\;\\\\8\\\\6\\\\3\\\\/\\\\+\\\\(\\\\$\\\\ \\\\\\x1d\\\\\\x19\\\\\\x14\\\\\\x10\\\\\\x0c\\\\\\x07\\\\\\x02\\\\\\xfa[\\xf0[\\xe4[\\xda[\\xcf[\\xc4[\\xb9[\\xad[\\xa1[\\x95[\\x88[{[m[_[P[B[3[$[\\x15[\\x05[\\xf5Z\\xe4Z\\xd3Z\\xc2Z\\xb0Z\\x9fZ\\x8dZ|ZjZYZGZ5Z#Z\\x11Z\\xfeY\\xecY\\xdaY\\xc8Y\\xb7Y\\xa5Y\\x94Y\\x83YqY`YOY>Y.Y\\x1dY\\x0cY\\xfbX\\xeaX\\xdaX\\xcbX\\xbbX\\xacX\\x9dX\\x8eX\\x80XqXbXSXEX7X)X\\x1cX\\x0eX\\x01X\\xe8W\\xcfW\\xb6W\\x9eW\\x88WrWZWBW-W\\x18W\\x04W\\xf1V\\xdfV\\xceV\\xbfV\\xb0V\\xa2V\\x95V\\x8bV\\x80VvVmVfV`V]VZVWVUVTVVVXVZV_VfVnVxV') in type <class 'bytearray'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithoutKey$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Combined_jdf=jdf.union(smoothed_jdf)\n",
    "Combined_jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6518988"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Combined_jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filename='US_Weather_with_smoothed.parquet'\n",
    "outfilename='/weather/'+filename\n",
    "s3filename='s3://dse-weather/'+filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /weather/US_Weather_with_smoothed.parquet\n",
      "CPU times: user 60 ms, sys: 16 ms, total: 76 ms\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r $outfilename   #remove old copy\n",
    "Combined_jdf.write.save(outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827.9 K  /weather/US_stations.parquet\r\n",
      "1.9 G    /weather/US_weather.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h /weather/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/04/22 15:31:26 INFO s3distcp.S3DistCp: Running with args: -libjars /usr/share/aws/emr/s3-dist-cp/lib/commons-httpclient-3.1.jar,/usr/share/aws/emr/s3-dist-cp/lib/commons-logging-1.0.4.jar,/usr/share/aws/emr/s3-dist-cp/lib/guava-18.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp-2.10.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp.jar --src s3://dse-weather/US_Weather_with_smoothed.parquet --dest /weather/US_Weather_with_smoothed.parquet \n",
      "18/04/22 15:31:27 INFO s3distcp.S3DistCp: S3DistCp args: --src s3://dse-weather/US_Weather_with_smoothed.parquet --dest /weather/US_Weather_with_smoothed.parquet \n",
      "18/04/22 15:31:27 INFO s3distcp.S3DistCp: Using output path 'hdfs:/tmp/6862ae49-728f-4267-afb4-4d12e6ee7eb3/output'\n",
      "18/04/22 15:31:27 INFO s3distcp.S3DistCp: GET http://169.254.169.254/latest/meta-data/placement/availability-zone result: us-east-1a\n",
      "18/04/22 15:31:30 WARN cred.CredentialsLegacyConfigLocationProvider: Found the legacy config profiles file at [/home/hadoop/.aws/config]. Please move it to the latest default location [~/.aws/credentials].\n",
      "18/04/22 15:31:30 INFO s3distcp.S3DistCp: DefaultAWSCredentialsProviderChain is used to create AmazonS3Client. KeyId: ASIAIJRUQXLN4EF6JRDA\n",
      "18/04/22 15:31:31 INFO s3distcp.FileInfoListing: Opening new file: hdfs:/tmp/6862ae49-728f-4267-afb4-4d12e6ee7eb3/files/1\n",
      "18/04/22 15:31:31 INFO s3distcp.S3DistCp: Created 1 files to copy 31 files \n",
      "18/04/22 15:31:32 INFO s3distcp.S3DistCp: Reducer number: 39\n",
      "18/04/22 15:31:32 INFO client.RMProxy: Connecting to ResourceManager at ip-10-129-255-17.ec2.internal/10.129.255.17:8032\n",
      "18/04/22 15:31:32 INFO input.FileInputFormat: Total input files to process : 1\n",
      "18/04/22 15:31:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "18/04/22 15:31:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1524409064425_0001\n",
      "18/04/22 15:31:33 INFO impl.YarnClientImpl: Submitted application application_1524409064425_0001\n",
      "18/04/22 15:31:33 INFO mapreduce.Job: The url to track the job: http://ip-10-129-255-17.ec2.internal:20888/proxy/application_1524409064425_0001/\n",
      "18/04/22 15:31:33 INFO mapreduce.Job: Running job: job_1524409064425_0001\n",
      "18/04/22 15:31:40 INFO mapreduce.Job: Job job_1524409064425_0001 running in uber mode : false\n",
      "18/04/22 15:31:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/04/22 15:31:45 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/04/22 15:31:50 INFO mapreduce.Job:  map 100% reduce 3%\n",
      "18/04/22 15:31:52 INFO mapreduce.Job:  map 100% reduce 26%\n",
      "18/04/22 15:31:53 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "18/04/22 15:31:54 INFO mapreduce.Job:  map 100% reduce 49%\n",
      "18/04/22 15:31:55 INFO mapreduce.Job:  map 100% reduce 56%\n",
      "18/04/22 15:31:56 INFO mapreduce.Job:  map 100% reduce 62%\n",
      "18/04/22 15:31:57 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "18/04/22 15:31:58 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "18/04/22 15:31:59 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "18/04/22 15:32:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/04/22 15:32:01 INFO mapreduce.Job: Job job_1524409064425_0001 completed successfully\n",
      "18/04/22 15:32:01 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3764\n",
      "\t\tFILE: Number of bytes written=6928768\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8146\n",
      "\t\tHDFS: Number of bytes written=2834230731\n",
      "\t\tHDFS: Number of read operations=183\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=109\n",
      "\t\tS3: Number of bytes read=2834230731\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=39\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=673524\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=102302720\n",
      "\t\tTotal time spent by all map tasks (ms)=3177\n",
      "\t\tTotal time spent by all reduce tasks (ms)=241280\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3177\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=241280\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21552768\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3273687040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31\n",
      "\t\tMap output records=31\n",
      "\t\tMap output bytes=10373\n",
      "\t\tMap output materialized bytes=3608\n",
      "\t\tInput split bytes=155\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=31\n",
      "\t\tReduce shuffle bytes=3608\n",
      "\t\tReduce input records=31\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=62\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=6854\n",
      "\t\tCPU time spent (ms)=264010\n",
      "\t\tPhysical memory (bytes) snapshot=18532954112\n",
      "\t\tVirtual memory (bytes) snapshot=545750437888\n",
      "\t\tTotal committed heap usage (bytes)=37451464704\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7991\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "18/04/22 15:32:01 INFO s3distcp.S3DistCp: Try to recursively delete hdfs:/tmp/6862ae49-728f-4267-afb4-4d12e6ee7eb3\n",
      "CPU times: user 768 ms, sys: 144 ms, total: 912 ms\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#!s3-dist-cp --src $outfilename --dest $s3filename\n",
    "!s3-dist-cp --src $s3filename --dest $outfilename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-19 00:06:40          0 _SUCCESS\r\n",
      "2018-04-19 00:06:42   79711937 part-00000-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42   80341446 part-00001-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:47   77394575 part-00002-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:39   74556551 part-00003-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44   77102526 part-00004-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:46   74678869 part-00005-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:40   72640052 part-00006-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:40   70800754 part-00007-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:43   68902426 part-00008-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:45   71010476 part-00009-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:45   68134967 part-00010-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42   66162885 part-00011-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42   66791067 part-00012-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:47   63174234 part-00013-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:39    2815550 part-00014-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44  129034413 part-00015-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:46  132186125 part-00016-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44  128698950 part-00017-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:41  129090789 part-00018-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:41  131989112 part-00019-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44  128243726 part-00020-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:45  127521853 part-00021-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42  130376089 part-00022-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42  131589366 part-00023-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:47  133074811 part-00024-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:39  128489903 part-00025-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:40  130647446 part-00026-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42  127245806 part-00027-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44  131824027 part-00028-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dse-weather/US_Weather_with_smoothed.parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "toc": {
   "nav_menu": {
    "height": "190px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "514px",
    "left": "0px",
    "right": "925px",
    "top": "107px",
    "width": "323px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
