{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Measurement sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "CPU times: user 772 ms, sys: 88 ms, total: 860 ms\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math\n",
    "%pylab inline\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.numpy_pack import packArray,unpackArray\n",
    "from lib.spark_PCA import computeCov\n",
    "from lib.computeStatistics import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data through open bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state='NY'\n",
    "EMR=True\n",
    "if not EMR:\n",
    "    data_dir='../../Data/Weather'\n",
    "\n",
    "    tarname=state+'.tgz'\n",
    "    parquet=state+'.parquet'\n",
    "\n",
    "    !rm -rf $data_dir/$tarname\n",
    "\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s > %s/%s\"%(tarname,data_dir,tarname)\n",
    "    print(command)\n",
    "    !$command\n",
    "    !ls -lh $data_dir/$tarname\n",
    "    cur_dir,=!pwd\n",
    "    %cd $data_dir\n",
    "    !tar -xzf $tarname\n",
    "    !du ./$parquet\n",
    "    %cd $cur_dir\n",
    "\n",
    "    #read statistics\n",
    "    filename='STAT_%s.pickle'%state\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s.gz > %s/%s.gz\"%(filename,data_dir,filename)\n",
    "    print(command)\n",
    "    !$command\n",
    "    \n",
    "    filename='US_stations.tsv.gz'\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/Info/%s > %s/%s\"%(filename,data_dir,filename)\n",
    "    print(command)\n",
    "    !$command\n",
    "    filename_no_gz = filename[:-3]\n",
    "    !gunzip -f $data_dir/$filename\n",
    "    !ls -lh $data_dir/US_stations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data when on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - hadoop hadoop          0 2018-04-18 18:36 /weather/US_stations.parquet\n",
      "drwxr-xr-x   - hadoop hadoop          0 2018-04-18 18:36 /weather/US_weather.parquet\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "|    Station|dist_coast|latitude|longitude|elevation|state|            name|\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "|USC00341900|   739.956|    36.3| -96.4667|    242.3|   OK|       CLEVELAND|\n",
      "|USC00428114|    908.22|    40.1|-111.6667|   1409.1|   UT|SPANISH FORK 1 S|\n",
      "|USC00165926|   23.8801| 29.7853| -90.1158|      0.9|   LA|   MARRERO 9 SSW|\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-----------+----+--------------------+\n",
      "|    Station|Measurement|Year|              Values|\n",
      "+-----------+-----------+----+--------------------+\n",
      "|CA001126150|       PRCP|1941|[00 7E 00 7E 00 7...|\n",
      "|CA001126150|       PRCP|1942|[00 00 80 4A 00 0...|\n",
      "+-----------+-----------+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 52 ms, sys: 16 ms, total: 68 ms\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if EMR:\n",
    "    !hdfs dfs -ls /weather/\n",
    "\n",
    "    stations_df=sqlContext.read.parquet('/weather/US_stations.parquet')\n",
    "    stations_df.show(3)\n",
    "\n",
    "    weather_df=sqlContext.read.parquet('/weather/US_weather.parquet')\n",
    "    weather_df.show(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3259494"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3259494\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|dist_coast|latitude|longitude|elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "|CA001126150|       PRCP|1941|[00 7E 00 7E 00 7...|   226.659|  49.467|   -119.6|    344.0|  NaN|PENTICTON AIRPORT|\n",
      "|CA001126150|       PRCP|1942|[00 00 80 4A 00 0...|   226.659|  49.467|   -119.6|    344.0|  NaN|PENTICTON AIRPORT|\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "jdf=weather_df.join(stations_df,on='Station',how='left')\n",
    "print(jdf.count())\n",
    "jdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Smoothing by convolving with gaussian window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load lib/numpy_pack.py\n",
    "import numpy as np\n",
    "\"\"\"Code for packing and unpacking a numpy array into a byte array.\n",
    "   the array is flattened if it is not 1D.\n",
    "   This is intended to be used as the interface for storing \n",
    "   \n",
    "   This code is intended to be used to store numpy array as fields in a dataframe and then store the \n",
    "   dataframes in a parquet file.\n",
    "\"\"\"\n",
    "\n",
    "def packArray(a):\n",
    "    \"\"\"\n",
    "    pack a numpy array into a bytearray that can be stored as a single \n",
    "    field in a spark DataFrame\n",
    "\n",
    "    :param a: a numpy ndarray \n",
    "    :returns: a bytearray\n",
    "    :rtype:\n",
    "\n",
    "    \"\"\"\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "\n",
    "\n",
    "def unpackArray(x,data_type=np.int16):\n",
    "    \"\"\"\n",
    "    unpack a bytearray into a numpy.ndarray\n",
    "\n",
    "    :param x: a bytearray\n",
    "    :param data_type: The dtype of the array. This is important because if determines how many bytes go into each entry in the array.\n",
    "    :returns: a numpy array\n",
    "    :rtype: a numpy ndarray of dtype data_type.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.frombuffer(x,dtype=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from astropy.convolution import convolve\n",
    "from scipy import signal\n",
    "from copy import deepcopy\n",
    "#using astrophy.convolution.convolve and not scipy.signal.convolve because the first can handle nans.\n",
    "\n",
    "order=101\n",
    "std=20\n",
    "window = signal.gaussian(order, std=std)\n",
    "window/=sum(window)\n",
    "\n",
    "def Smoother(item):\n",
    "    key,List = item\n",
    "    \n",
    "    sorted_List=sorted(List,key=lambda row:row['Year'])\n",
    "    L=[(Row['Year'],unpackArray(Row['Values'],np.float16)) for Row in sorted_List]\n",
    "    \n",
    "    orig=np.stack([V[1] for V in L])\n",
    "    print('orig.shape=',orig.shape)\n",
    "    orig_shape=orig.shape\n",
    "    orig=orig.flatten()\n",
    "    smoothed = convolve(orig, window)\n",
    "    smoothed=np.reshape(smoothed,orig_shape)\n",
    "    print('smoothed.shape=',smoothed.shape)\n",
    "\n",
    "    #create a list of Rows with the smoothed \n",
    "    new_L = []\n",
    "    new_name = List[0]['Measurement']+'_s%d'%std\n",
    "    for i in range(len(List)):\n",
    "        new_row = deepcopy(List[i].asDict())\n",
    "        new_row['Measurement']=new_name\n",
    "        new_row['Values']=packArray(smoothed[i,:])\n",
    "        new_L.append(Row(**new_row))\n",
    "\n",
    "    return new_L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 39.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyVal=jdf.rdd.map(lambda row:((row['Station'],row['Measurement']),[row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Reduced=keyVal.reduceByKey(lambda x,y:x+y)\n",
    "item = Reduced.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig.shape= (39, 365)\n",
      "smoothed.shape= (39, 365)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_L = Smoother(item)\n",
    "len(new_L[0]['Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 39)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_L),len(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Smoothed=Reduced.flatMap(Smoother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X=Smoothed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 1min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3259494"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Smoothed.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3259494"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "jdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing order of dataframe columns\n",
    "\n",
    "It turns out that to take the union of two dataframes, the order of the columns needs to be the same, it is not enough if the columns match by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Station',\n",
       " 'Measurement',\n",
       " 'Year',\n",
       " 'Values',\n",
       " 'dist_coast',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'elevation',\n",
       " 'state',\n",
       " 'name']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldNames=[X.name for X in jdf.schema.fields]\n",
    "fieldNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "#Using the schema from JDF causes an error, it works without specifying a schema explicitly (the rows have the type info)\n",
    "smoothed_jdf=sqlContext.createDataFrame(Smoothed,verifySchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- elevation: float (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoothed_jdf=smoothed_jdf.select(fieldNames)\n",
    "\n",
    "jdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoothed_jdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine original and smoothed rows into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Combined_jdf=jdf.union(smoothed_jdf)\n",
    "Combined_jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6518988"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Combined_jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /weather/US_Weather_with_smoothed.parquet\n",
      "CPU times: user 60 ms, sys: 16 ms, total: 76 ms\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filename='US_Weather_with_smoothed.parquet'\n",
    "outfilename='/weather/'+filename\n",
    "s3filename='s3://dse-weather/'+filename\n",
    "!hdfs dfs -rm -r $outfilename   #remove old copy\n",
    "Combined_jdf.write.save(outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6 G    /weather/US_Weather_with_smoothed.parquet\r\n",
      "827.9 K  /weather/US_stations.parquet\r\n",
      "1.9 G    /weather/US_weather.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h /weather/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/04/19 00:06:17 INFO s3distcp.S3DistCp: Running with args: -libjars /usr/share/aws/emr/s3-dist-cp/lib/commons-httpclient-3.1.jar,/usr/share/aws/emr/s3-dist-cp/lib/commons-logging-1.0.4.jar,/usr/share/aws/emr/s3-dist-cp/lib/guava-18.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp-2.10.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp.jar --src /weather/US_Weather_with_smoothed.parquet --dest s3://dse-weather/US_Weather_with_smoothed.parquet \n",
      "18/04/19 00:06:18 INFO s3distcp.S3DistCp: S3DistCp args: --src /weather/US_Weather_with_smoothed.parquet --dest s3://dse-weather/US_Weather_with_smoothed.parquet \n",
      "18/04/19 00:06:18 INFO s3distcp.S3DistCp: Using output path 'hdfs:/tmp/b0136120-fe18-4f2d-9cc4-d15f60fd3009/output'\n",
      "18/04/19 00:06:18 INFO s3distcp.S3DistCp: GET http://169.254.169.254/latest/meta-data/placement/availability-zone result: us-east-1a\n",
      "18/04/19 00:06:19 INFO s3distcp.FileInfoListing: Opening new file: hdfs:/tmp/b0136120-fe18-4f2d-9cc4-d15f60fd3009/files/1\n",
      "18/04/19 00:06:19 INFO s3distcp.S3DistCp: Created 1 files to copy 30 files \n",
      "18/04/19 00:06:21 INFO s3distcp.S3DistCp: Reducer number: 39\n",
      "18/04/19 00:06:21 INFO client.RMProxy: Connecting to ResourceManager at ip-10-129-253-53.ec2.internal/10.129.253.53:8032\n",
      "18/04/19 00:06:21 INFO input.FileInputFormat: Total input files to process : 1\n",
      "18/04/19 00:06:21 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "18/04/19 00:06:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1524067195749_0003\n",
      "18/04/19 00:06:22 INFO impl.YarnClientImpl: Submitted application application_1524067195749_0003\n",
      "18/04/19 00:06:22 INFO mapreduce.Job: The url to track the job: http://ip-10-129-253-53.ec2.internal:20888/proxy/application_1524067195749_0003/\n",
      "18/04/19 00:06:22 INFO mapreduce.Job: Running job: job_1524067195749_0003\n",
      "18/04/19 00:06:28 INFO mapreduce.Job: Job job_1524067195749_0003 running in uber mode : false\n",
      "18/04/19 00:06:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/04/19 00:06:32 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/04/19 00:06:38 INFO mapreduce.Job:  map 100% reduce 13%\n",
      "18/04/19 00:06:39 INFO mapreduce.Job:  map 100% reduce 28%\n",
      "18/04/19 00:06:40 INFO mapreduce.Job:  map 100% reduce 36%\n",
      "18/04/19 00:06:41 INFO mapreduce.Job:  map 100% reduce 46%\n",
      "18/04/19 00:06:42 INFO mapreduce.Job:  map 100% reduce 54%\n",
      "18/04/19 00:06:43 INFO mapreduce.Job:  map 100% reduce 62%\n",
      "18/04/19 00:06:44 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "18/04/19 00:06:45 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "18/04/19 00:06:46 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "18/04/19 00:06:47 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "18/04/19 00:06:48 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "18/04/19 00:06:49 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "18/04/19 00:06:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/04/19 00:06:51 INFO mapreduce.Job: Job job_1524067195749_0003 completed successfully\n",
      "18/04/19 00:06:51 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4536\n",
      "\t\tFILE: Number of bytes written=6933592\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2834240013\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=151\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=78\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=2834230731\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=39\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=552260\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=97870224\n",
      "\t\tTotal time spent by all map tasks (ms)=2605\n",
      "\t\tTotal time spent by all reduce tasks (ms)=230826\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2605\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=230826\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=17672320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3131847168\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30\n",
      "\t\tMap output records=30\n",
      "\t\tMap output bytes=11702\n",
      "\t\tMap output materialized bytes=4380\n",
      "\t\tInput split bytes=155\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=30\n",
      "\t\tReduce shuffle bytes=4380\n",
      "\t\tReduce input records=30\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=60\n",
      "\t\tShuffled Maps =39\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=39\n",
      "\t\tGC time elapsed (ms)=5365\n",
      "\t\tCPU time spent (ms)=240760\n",
      "\t\tPhysical memory (bytes) snapshot=17759227904\n",
      "\t\tVirtual memory (bytes) snapshot=545690959872\n",
      "\t\tTotal committed heap usage (bytes)=36485201920\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9127\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "18/04/19 00:06:51 INFO s3distcp.S3DistCp: Try to recursively delete hdfs:/tmp/b0136120-fe18-4f2d-9cc4-d15f60fd3009\n",
      "CPU times: user 660 ms, sys: 200 ms, total: 860 ms\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!s3-dist-cp --src $outfilename --dest $s3filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-19 00:06:40          0 _SUCCESS\r\n",
      "2018-04-19 00:06:42   79711937 part-00000-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42   80341446 part-00001-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:47   77394575 part-00002-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:39   74556551 part-00003-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44   77102526 part-00004-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:46   74678869 part-00005-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:40   72640052 part-00006-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:40   70800754 part-00007-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:43   68902426 part-00008-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:45   71010476 part-00009-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:45   68134967 part-00010-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42   66162885 part-00011-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42   66791067 part-00012-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:47   63174234 part-00013-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:39    2815550 part-00014-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44  129034413 part-00015-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:46  132186125 part-00016-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44  128698950 part-00017-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:41  129090789 part-00018-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:41  131989112 part-00019-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44  128243726 part-00020-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:45  127521853 part-00021-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42  130376089 part-00022-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42  131589366 part-00023-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:47  133074811 part-00024-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:39  128489903 part-00025-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:40  130647446 part-00026-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:42  127245806 part-00027-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n",
      "2018-04-19 00:06:44  131824027 part-00028-e97f4502-af58-4cb8-a949-14e7c6d9e6b3-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dse-weather/US_Weather_with_smoothed.parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://dse-weather/US_Weather_with_smoothed.parquet'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "toc": {
   "nav_menu": {
    "height": "190px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "514px",
    "left": "0px",
    "right": "925px",
    "top": "107px",
    "width": "323px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
