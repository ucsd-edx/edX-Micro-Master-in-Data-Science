{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to work with Pandas_UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "CPU times: user 1.54 s, sys: 116 ms, total: 1.66 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import urllib\n",
    "import math\n",
    "%pylab inline\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "sc = SparkContext(pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py'])\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.numpy_pack import packArray,unpackArray\n",
    "from lib.spark_PCA import computeCov\n",
    "from lib.computeStatistics import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data through open bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state='NY'\n",
    "EMR=True\n",
    "if not EMR:\n",
    "    data_dir='../../Data/Weather'\n",
    "\n",
    "    tarname=state+'.tgz'\n",
    "    parquet=state+'.parquet'\n",
    "\n",
    "    !rm -rf $data_dir/$tarname\n",
    "\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s > %s/%s\"%(tarname,data_dir,tarname)\n",
    "    print(command)\n",
    "    !$command\n",
    "    !ls -lh $data_dir/$tarname\n",
    "    cur_dir,=!pwd\n",
    "    %cd $data_dir\n",
    "    !tar -xzf $tarname\n",
    "    !du ./$parquet\n",
    "    %cd $cur_dir\n",
    "\n",
    "    #read statistics\n",
    "    filename='STAT_%s.pickle'%state\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s.gz > %s/%s.gz\"%(filename,data_dir,filename)\n",
    "    print(command)\n",
    "    !$command\n",
    "    \n",
    "    filename='US_stations.tsv.gz'\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/Info/%s > %s/%s\"%(filename,data_dir,filename)\n",
    "    print(command)\n",
    "    !$command\n",
    "    filename_no_gz = filename[:-3]\n",
    "    !gunzip -f $data_dir/$filename\n",
    "    !ls -lh $data_dir/US_stations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data when on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - hadoop hadoop          0 2018-04-18 18:36 /weather/US_stations.parquet\n",
      "drwxr-xr-x   - hadoop hadoop          0 2018-04-18 18:36 /weather/US_weather.parquet\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "|    Station|dist_coast|latitude|longitude|elevation|state|            name|\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "|USC00341900|   739.956|    36.3| -96.4667|    242.3|   OK|       CLEVELAND|\n",
      "|USC00428114|    908.22|    40.1|-111.6667|   1409.1|   UT|SPANISH FORK 1 S|\n",
      "|USC00165926|   23.8801| 29.7853| -90.1158|      0.9|   LA|   MARRERO 9 SSW|\n",
      "+-----------+----------+--------+---------+---------+-----+----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-----------+----+--------------------+\n",
      "|    Station|Measurement|Year|              Values|\n",
      "+-----------+-----------+----+--------------------+\n",
      "|CA001126150|       PRCP|1941|[00 7E 00 7E 00 7...|\n",
      "|CA001126150|       PRCP|1942|[00 00 80 4A 00 0...|\n",
      "+-----------+-----------+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 60 ms, sys: 12 ms, total: 72 ms\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if EMR:\n",
    "    !hdfs dfs -ls /weather/\n",
    "\n",
    "    stations_df=sqlContext.read.parquet('/weather/US_stations.parquet')\n",
    "    stations_df.show(3)\n",
    "\n",
    "    weather_df=sqlContext.read.parquet('/weather/US_weather.parquet')\n",
    "    weather_df.show(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3259494\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|dist_coast|latitude|longitude|elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "|CA001126150|       PRCP|1941|[00 7E 00 7E 00 7...|   226.659|  49.467|   -119.6|    344.0|  NaN|PENTICTON AIRPORT|\n",
      "|CA001126150|       PRCP|1942|[00 00 80 4A 00 0...|   226.659|  49.467|   -119.6|    344.0|  NaN|PENTICTON AIRPORT|\n",
      "+-----------+-----------+----+--------------------+----------+--------+---------+---------+-----+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "jdf=weather_df.join(stations_df,on='Station',how='left')\n",
    "print(jdf.count())\n",
    "jdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Smoothing by convolving with gaussian window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load lib/numpy_pack.py\n",
    "import numpy as np\n",
    "\"\"\"Code for packing and unpacking a numpy array into a byte array.\n",
    "   the array is flattened if it is not 1D.\n",
    "   This is intended to be used as the interface for storing \n",
    "   \n",
    "   This code is intended to be used to store numpy array as fields in a dataframe and then store the \n",
    "   dataframes in a parquet file.\n",
    "\"\"\"\n",
    "\n",
    "def packArray(a):\n",
    "    \"\"\"\n",
    "    pack a numpy array into a bytearray that can be stored as a single \n",
    "    field in a spark DataFrame\n",
    "\n",
    "    :param a: a numpy ndarray \n",
    "    :returns: a bytearray\n",
    "    :rtype:\n",
    "\n",
    "    \"\"\"\n",
    "    if type(a)!=np.ndarray:\n",
    "        raise Exception(\"input to packArray should be numpy.ndarray. It is instead \"+str(type(a)))\n",
    "    return bytearray(a.tobytes())\n",
    "\n",
    "\n",
    "def unpackArray(x,data_type=np.int16):\n",
    "    \"\"\"\n",
    "    unpack a bytearray into a numpy.ndarray\n",
    "\n",
    "    :param x: a bytearray\n",
    "    :param data_type: The dtype of the array. This is important because if determines how many bytes go into each entry in the array.\n",
    "    :returns: a numpy array\n",
    "    :rtype: a numpy ndarray of dtype data_type.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.frombuffer(x,dtype=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from astropy.convolution import convolve\n",
    "from scipy import signal\n",
    "from copy import deepcopy\n",
    "#using astrophy.convolution.convolve and not scipy.signal.convolve because the first can handle nans.\n",
    "\n",
    "order=101\n",
    "std=20\n",
    "window = signal.gaussian(order, std=std)\n",
    "window/=sum(window)\n",
    "\n",
    "def Smoother(item):\n",
    "    key,List = item\n",
    "    \n",
    "    sorted_List=sorted(List,key=lambda row:row['Year'])\n",
    "    L=[(Row['Year'],unpackArray(Row['Values'],np.float16)) for Row in sorted_List]\n",
    "    \n",
    "    orig=np.stack([V[1] for V in L])\n",
    "    print('orig.shape=',orig.shape)\n",
    "    orig_shape=orig.shape\n",
    "    orig=orig.flatten()\n",
    "    smoothed = convolve(orig, window)\n",
    "    smoothed=np.reshape(smoothed,orig_shape)\n",
    "\n",
    "    #create a list of Rows with the smoothed \n",
    "    new_L = []\n",
    "    new_name = List[0]['Measurement']+'_s%d'%std\n",
    "    for i in range(len(List)):\n",
    "        new_row = List[i].asDict()\n",
    "        new_row['Measurement']=new_name\n",
    "        new_row['Values']=packArray(smoothed[i,:])\n",
    "        new_L.append(Row(**new_row))\n",
    "\n",
    "    return new_L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 7.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keyVal=jdf.rdd.map(lambda row:((row['Station'],row['Measurement']),[row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Reduced=keyVal.reduceByKey(lambda x,y:x+y)\n",
    "item = Reduced.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig.shape= (39, 365)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Measurement='TMAX_s20', Station='CA007016902', Values=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x00~\\x8fT\\x88T\\x82T|TvTqTkTcT]TUTOTITBT<T4T-T&T T\\x19T\\x12T\\nT\\x02T\\xf4S\\xe5S\\xd4S\\xc2S\\xb2S\\xa0S\\x8fS}SkSYSFS3S\\x1eS\\nS\\xf7R\\xe4R\\xcfR\\xb9R\\xa2R\\x8cRtR[R?R#R\\x06R\\xe9Q\\xcaQ\\xaaQ\\x89QfQBQ\\x1eQ\\xf6P\\xcfP\\xa7P~PUP*P\\xfeO\\xa5OHO\\xe8N\\x8bN+N\\xc7MeM\\x02M\\x9fL;L\\xafK\\xe8J!J[I\\x96H\\xa7G$F\\xa6DZB\\xe5>%5\\xb0\\xbc>\\xc1\\xed\\xc3T\\xc5\\xa4\\xc6\\xf9\\xc7\\x9e\\xc8B\\xc9\\xe4\\xc9\\x85\\xca\"\\xcb\\xbd\\xcb/\\xcc\\x7f\\xcc\\xcb\\xcc\\x14\\xcd[\\xcd\\xa4\\xcd\\xef\\xcd@\\xce\\x89\\xce\\xd8\\xce\\'\\xcft\\xcf\\xbf\\xcf\\x03\\xd0&\\xd0I\\xd0k\\xd0'), Year=1974, dist_coast=194.63999938964844, elevation=198.0, latitude=46.20000076293945, longitude=-73.5999984741211, name='STE BEATRIX', state='NaN')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_L = Smoother(item)\n",
    "new_L[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 39)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_L),len(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Smoothed=Reduced.flatMap(Smoother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X=Smoothed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Smoothed.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "jdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfilename='../../Data/Weather/Weather_with_smoothed.parquet'\n",
    "jdf.write.save(outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!du -sh $outfilename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BinaryType not supported  by pandas_udf\n",
    "Running the following code: \n",
    "```python\n",
    "import pyspark.sql.functions as sqlf\n",
    "import pyspark\n",
    "import pyarrow\n",
    "pyspark.__version__  (2.3.0)\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "def Smoother(orig_pdf):\n",
    "    return orig_pdf\n",
    "\n",
    "### Offending command\n",
    "smoother_udf=pandas_udf(Smoother,df.select(['Station','Year','Values']).schema, PandasUDFType.GROUPED_MAP) \n",
    "\n",
    "X=df.groupby(\"Station\").apply(smoother_udf)\n",
    "X.show()\n",
    "```\n",
    "Generates the following error message\n",
    "```\n",
    "NotImplementedError: Invalid returnType with grouped map Pandas UDFs: StructType(List(StructField(Station,StringType,true),StructField(Year,IntegerType,true),StructField(Values,BinaryType,true))) is not supported\n",
    "```\n",
    "\n",
    "Works find if only ('Station','Year') are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lib.YearPlotter import YearPlotter\n",
    "fig, ax = plt.subplots(figsize=(10,7));\n",
    "YP=YearPlotter()\n",
    "YP.plot(smoothed[110:120,:].transpose(),fig,ax,title='smoothed %s for %s'%(measurement,stat));\n",
    "plt.savefig('percipitation.png')\n",
    "#title('A sample of graphs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7));\n",
    "YP=YearPlotter()\n",
    "i=85\n",
    "factor=5\n",
    "pair=np.stack([orig[i,:],smoothed[i,:]*factor])\n",
    "pair.shape\n",
    "\n",
    "YP.plot(pair.transpose(),fig,ax,title='smoothed %s for %s'%(measurement,stat));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from astropy.convolution import convolve\n",
    "window = signal.gaussian(81, std=20)\n",
    "\n",
    "window/=sum(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P=T[3,:]\n",
    "P[10:30]=np.nan\n",
    "f=filtered = convolve(P, window)\n",
    "print(len(f))\n",
    "plot(f)\n",
    "plot(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distribution of missing observations\n",
    "The distribution of missing observations is not uniform throughout the year. We visualize it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from MultiPlot import *                \n",
    "def plot_valid(m,fig,axis):\n",
    "    valid_m=STAT[m]['NE']\n",
    "    YP.plot(valid_m,fig,axis,title='valid-counts '+m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TMIN','TMAX'],plot_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TOBS','PRCP'],plot_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['SNOW', 'SNWD'],plot_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plots of mean and std of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mean_std(m,fig,axis):\n",
    "    scale=1.\n",
    "    temps=['TMIN','TMAX','TOBS']\n",
    "    percipitation=['PRCP','SNOW','SNWD']\n",
    "    _labels=['mean+std','mean','mean-std']\n",
    "    if (m in temps or m=='PRCP'):\n",
    "        scale=10.\n",
    "    mean=STAT[m]['Mean']/scale\n",
    "    std=np.sqrt(STAT[m]['Var'])/scale\n",
    "    graphs=np.vstack([mean+std,mean,mean-std]).transpose()\n",
    "    YP.plot(graphs,fig,axis,labels=_labels,title='Mean+-std   '+m)\n",
    "    if (m in temps):\n",
    "        axis.set_ylabel('Degrees Celsius')\n",
    "    if (m in percipitation):\n",
    "        axis.set_ylabel('millimeter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TMIN','TMAX'],plot_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TOBS','PRCP'],plot_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_single('TOBS',plot_mean_std,'r_figures/TOBS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['SNOW', 'SNWD'],plot_mean_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_single('SNOW',plot_mean_std,'r_figures/SNOW.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_single('SNWD',plot_mean_std,'r_figures/SNWD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### plotting top 3 eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_eigen(m,fig,axis):\n",
    "    EV=STAT[m]['eigvec']\n",
    "    YP.plot(EV[:,:3],fig,axis,title='Top Eigenvectors '+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TMIN','TMAX'],plot_eigen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['TOBS','PRCP'],plot_eigen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pair(['SNOW', 'SNWD'],plot_eigen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Script for plotting percentage of variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def pltVarExplained(j):\n",
    "    subplot(1,3,j)\n",
    "    EV=STAT[m]['eigval']\n",
    "    k=5\n",
    "    L=([0,]+list(cumsum(EV[:k])))/sum(EV)\n",
    "    #print m,L\n",
    "    plot(L)\n",
    "    title('Percentage of Variance Explained for '+ m)\n",
    "    ylabel('Percentage of Variance')\n",
    "    xlabel('# Eigenvector')\n",
    "    grid()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(15,4))\n",
    "j=1\n",
    "for m in ['TMIN', 'TOBS', 'TMAX']: #,\n",
    "    pltVarExplained(j)\n",
    "    j+=1\n",
    "f.savefig('r_figures/VarExplained1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(15,4))\n",
    "j=1\n",
    "for m in ['SNOW', 'SNWD', 'PRCP']:\n",
    "    pltVarExplained(j)\n",
    "    j+=1 \n",
    "f.savefig('r_figures/VarExplained2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "toc": {
   "nav_menu": {
    "height": "190px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "514px",
    "left": "0px",
    "right": "925px",
    "top": "107px",
    "width": "323px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
