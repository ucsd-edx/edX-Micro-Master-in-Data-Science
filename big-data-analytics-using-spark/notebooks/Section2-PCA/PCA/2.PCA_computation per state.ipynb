{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMR=False # Set to True if running notebook on EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing PCA using RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  PCA\n",
    "\n",
    "The vectors that we want to analyze have length, or dimension, of 365, corresponding to the number of \n",
    "days in a year.\n",
    "\n",
    "We will perform [Principle component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "on these vectors. There are two steps to this process:\n",
    "\n",
    "1. Computing the covariance matrix: this is a  simple computation. However, it takes a long time to compute and it benefits from using an RDD because it involves all of the input vectors.\n",
    "2. Computing the eigenvector decomposition. this is a more complex computation, but it takes a fraction of a second because the size to the covariance matrix is $365 \\times 365$, which is quite small. We do it on the head node usin `linalg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computing the covariance matrix\n",
    "Suppose that the data vectors are the column vectors denoted $x$ then the covariance matrix is defined to be\n",
    "$$\n",
    "E(x x^T)-E(x)E(x)^T\n",
    "$$\n",
    "\n",
    "Where $x x^T$ is the **outer product** of $x$ with itself.\n",
    "\n",
    "If the data that we have is $x_1,x_2,x_n$ then the estimates we use are:\n",
    "$$\n",
    "\\hat{E}(x x^T) = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T,\\;\\;\\;\\;\\;\n",
    "\\hat{E}(x) = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The effect of  `nan`s in arithmetic operations\n",
    "* We use an RDD of numpy arrays, instead of Dataframes.\n",
    "* Why? Because numpy treats `nan` entries correctly:\n",
    "  * In numpy `5+nan=5` while in dataframes `5+nan=nan`\n",
    "\n",
    "### Performing Cov matrix on vectors with NaNs\n",
    "As it happens, we often get vectors $x$ in which some, but not all, of the entries are `nan`. \n",
    "Suppose that we want to compute the mean of the elements of $x$. If we use `np.mean` we will get the result `nan`. A useful alternative is to use `np.nanmean` which removes the `nan` elements and takes the mean of the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### When should you not use `np.nanmean` ?\n",
    "Using `n.nanmean` is equivalent to assuming that choice of which elements to remove is independent of the values of the elements. \n",
    "* Example of bad case: suppose the larger elements have a higher probability of being `nan`. In that case `np.nanmean` will under-estimate the mean\n",
    "\n",
    "#### Computing the covariance  when there are `nan`s\n",
    "The covariance is a mean of outer products.\n",
    "\n",
    "If the data that we have is $x_1,x_2,x_n$ then the estimates we use are:\n",
    "$$\n",
    "\\hat{E}(x x^T) = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T,\\;\\;\\;\\;\\;\n",
    "\\hat{E}(x) = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('spark.app.name', 'Weather_PCA'), ('spark.executor.memory', '2g'), ('spark.executor.cores', '4'), ('spark.cores.max', '40'), ('spark.default.parallelism', '10'), ('spark.logConf', 'True')])\n"
     ]
    }
   ],
   "source": [
    "if not EMR:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "from pyspark import SparkContext,SparkConf\n",
    "\n",
    "def create_sc(pyFiles):\n",
    "    sc_conf = SparkConf()\n",
    "    sc_conf.setAppName(\"Weather_PCA\")\n",
    "    sc_conf.set('spark.executor.memory', '2g')\n",
    "    sc_conf.set('spark.executor.cores', '4')\n",
    "    sc_conf.set('spark.cores.max', '40')\n",
    "    sc_conf.set('spark.default.parallelism','10')\n",
    "    sc_conf.set('spark.logConf', True)\n",
    "    print(sc_conf.getAll())\n",
    "\n",
    "    sc = SparkContext(conf=sc_conf,pyFiles=pyFiles)\n",
    "\n",
    "    return sc \n",
    "\n",
    "#sc.stop()\n",
    "#sc = SparkContext(master=\"local[3]\",pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py'])\n",
    "sc = create_sc(pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "import numpy as np\n",
    "from lib.computeStatistics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Climate data\n",
    "\n",
    "The data we will use here comes from [NOAA](https://www.ncdc.noaa.gov/). Specifically, it was downloaded from This [FTP site](ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/).\n",
    "\n",
    "There is a large variety of measurements from all over the world, from 1870 will 2012.\n",
    "in the directory `../../Data/Weather` you will find the following useful files:\n",
    "\n",
    "* data-source.txt: the source of the data\n",
    "* ghcnd-readme.txt: A description of the content and format of the data\n",
    "* ghcnd-stations.txt: A table describing the Meteorological stations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data cleaning\n",
    "\n",
    "* Most measurements exists only for a tiny fraction of the stations and years. We therefor restrict our use to the following measurements:\n",
    "```python\n",
    "['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']\n",
    "```\n",
    "\n",
    "* 8 We consider only measurement-years that have at most 50 `NaN` entries\n",
    "\n",
    "* We consider only measurements in the continential USA\n",
    "\n",
    "* We partition the stations into the states of the continental USA (plus a few stations from states in canada and mexico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mNY.parquet\u001b[m\u001b[m        NY.tgz            \u001b[34mOld\u001b[m\u001b[m               STAT_NY.pickle.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/yoavfreund/projects/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/notebooks/Data/Weather/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state='NY'\n",
    "if not EMR:\n",
    "    data_dir='../../Data/Weather'\n",
    "    tarname=state+'.tgz'\n",
    "    parquet=state+'.parquet'\n",
    "\n",
    "    !rm -rf $data_dir/$tarname\n",
    "\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/by_state/%s > %s/%s\"%(tarname,data_dir,tarname)\n",
    "    print(command)\n",
    "    !$command\n",
    "    !ls -lh $data_dir/$tarname\n",
    "\n",
    "    cur_dir,=!pwd\n",
    "    %cd $data_dir\n",
    "    !tar -xzf $tarname\n",
    "    !du ./$parquet\n",
    "    %cd $cur_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if EMR:  # not debugged, should use complete parquet and extract just the state of interest.\n",
    "    data_dir='/mnt/workspace/Data'\n",
    "    !hdfs dfs -mkdir /weather/\n",
    "    !hdfs dfs -CopyFromLocal $data_dir/$parquet /weather/$parquet\n",
    "\n",
    "    # When running on cluster\n",
    "    #!mv ../../Data/Weather/NY.parquet /mnt/workspace/Data/NY.parquet\n",
    "\n",
    "    !aws s3 cp --recursive --quiet /mnt/workspace/Data/NY.parquet s3://dse-weather/NY.parquet\n",
    "\n",
    "    !aws s3 ls s3://dse-weather/\n",
    "\n",
    "    local_path=data_dir+'/'+parquet\n",
    "    hdfs_path='/weather/'+parquet\n",
    "    local_path,hdfs_path\n",
    "\n",
    "    !hdfs dfs -copyFromLocal $local_path $hdfs_path\n",
    "\n",
    "    !hdfs dfs -du /weather/\n",
    "    parquet_path=hdfs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28M\t../../Data/Weather/NY.parquet\r\n"
     ]
    }
   ],
   "source": [
    "parquet_path = data_dir+'/'+parquet\n",
    "!du -sh $parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84199\n",
      "+-----------+-----------+----+--------------------+-----+\n",
      "|    Station|Measurement|Year|              Values|State|\n",
      "+-----------+-----------+----+--------------------+-----+\n",
      "|USC00303452|       PRCP|1903|[00 7E 00 7E 00 7...|   NY|\n",
      "|USC00303452|       PRCP|1904|[00 00 28 5B 00 0...|   NY|\n",
      "|USC00303452|       PRCP|1905|[00 00 60 56 60 5...|   NY|\n",
      "|USC00303452|       PRCP|1906|[00 00 00 00 00 0...|   NY|\n",
      "|USC00303452|       PRCP|1907|[00 00 00 00 60 5...|   NY|\n",
      "+-----------+-----------+----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 3.43 ms, sys: 3.2 ms, total: 6.63 ms\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df=sqlContext.read.parquet(parquet_path)\n",
    "print(df.count())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of executors= 10\n",
      "took 0.0011188983917236328 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t=time()\n",
    "\n",
    "N=sc.defaultParallelism\n",
    "print('Number of executors=',N)\n",
    "print('took',time()-t,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "STAT=computeStatistics(sqlContext,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['UnDef', 'mean', 'std', 'SortedVals', 'low100', 'high100', 'low1000', 'high1000', 'eigval', 'eigvec', 'E', 'NE', 'O', 'NO', 'Cov', 'Mean', 'Var'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STAT['TMAX'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 97400\r\n",
      "drwxr-xr-x  17 yoavfreund  staff       544 Apr  1 14:06 \u001b[34mNY.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 yoavfreund  staff  23182008 Apr  1 14:06 NY.tgz\r\n",
      "drwxr-xr-x  20 yoavfreund  staff       640 Apr  1 13:57 \u001b[34mOld\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 yoavfreund  staff  25684524 Apr  1 14:31 STAT_NY.pickle\r\n"
     ]
    }
   ],
   "source": [
    "filename=data_dir+'/STAT_%s.pickle'%state\n",
    "dump((STAT,STAT_Descriptions),open(filename,'wb'))\n",
    "!ls -l $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMAX 0.0\n",
      "SNOW 852107.7058667188\n",
      "SNWD 4464167.852118857\n",
      "TMIN 319734.53150046256\n",
      "PRCP 1184305.1228400553\n",
      "TOBS 277719.0089381143\n"
     ]
    }
   ],
   "source": [
    "X=STAT['TMAX']['Var']\n",
    "for key in STAT.keys():\n",
    "    Y=STAT[key]['Var']\n",
    "    print(key,sum(abs(X-Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 yoavfreund  staff  25684524 Apr  1 14:31 ../../Data/Weather/STAT_NY.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../../Data/Weather/STAT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 yoavfreund  staff  14948011 Apr  1 14:31 ../../Data/Weather/STAT_NY.pickle.gz\r\n"
     ]
    }
   ],
   "source": [
    "!gzip -f ../../Data/Weather/STAT*.pickle\n",
    "!ls -l ../../Data/Weather/STAT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3  cp ../../Data/Weather/STAT_NY.pickle.gz s3://mas-dse-open/Weather/by_state/STAT_NY.pickle.gz\n",
      "upload: ../../Data/Weather/STAT_NY.pickle.gz to s3://mas-dse-open/Weather/by_state/STAT_NY.pickle.gz\n"
     ]
    }
   ],
   "source": [
    "for state in ['NY']:\n",
    "    command=\"aws s3  cp ../../Data/Weather/STAT_%s.pickle.gz s3://mas-dse-open/Weather/by_state/STAT_%s.pickle.gz\"%(state,state)\n",
    "    print(command)\n",
    "    !$command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-01 14:32:56   14948011 STAT_NY.pickle.gz\r\n",
      "2018-03-18 20:33:54   11717259 STAT_RI.pickle.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3  ls s3://mas-dse-open/Weather/by_state/ | grep STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "PCA_using_numpy for HW3",
  "notebookId": 85286,
  "toc": {
   "nav_menu": {
    "height": "116px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
