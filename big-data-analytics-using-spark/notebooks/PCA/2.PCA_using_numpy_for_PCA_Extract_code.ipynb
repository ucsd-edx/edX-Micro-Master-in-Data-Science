{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Performing PCA on vectors with NaNs\n",
    "This notebook demonstrates the use of numpy arrays as the content of RDDs.\n",
    "\n",
    "The code was used on a spark cluster to produce the pickle file that you will examine locally on your laptop.\n",
    "\n",
    "\n",
    "The reason that we use numpy arrays instead of dataframes is that numpy is better in handling `nan` etries.\n",
    "\n",
    "In numpy `5+nan=5` while in dataframes `5+nan=nan`\n",
    "\n",
    "Suppose that the data vectors are the column vectors denoted $x$ then the covariance matrix is defined to be\n",
    "$$\n",
    "E(x x^T)-E(x)E(x)^T\n",
    "$$\n",
    "\n",
    "Where $x x^T$ is the **outer product** of $x$ with itself.\n",
    "\n",
    "If the data that we have is $x_1,x_2,x_n$ then the estimates we use are:\n",
    "$$\n",
    "\\hat{E}(x x^T) = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T,\\;\\;\\;\\;\\;\n",
    "\\hat{E}(x) = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$\n",
    "\n",
    "As it happens, we often get vectors $x$ in which some, but not all, of the entries are `nan`. In such cases we sum the elements that are defined and keep a seperate counter for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing spark_PCA.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_PCA.py\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def outerProduct(X):\n",
    "    \"\"\"Computer outer product and indicate which locations in matrix are undefined\"\"\"\n",
    "    O=np.outer(X,X)\n",
    "    N=1-np.isnan(O)\n",
    "    return (O,N)\n",
    "\n",
    "def sumWithNan(M1,M2):\n",
    "    \"\"\"Add two pairs of (matrix,count)\"\"\"\n",
    "    (X1,N1)=M1\n",
    "    (X2,N2)=M2\n",
    "    N=N1+N2\n",
    "    X=np.nansum(np.dstack((X1,X2)),axis=2)\n",
    "    return (X,N)\n",
    "\n",
    "def computeCov(RDDin):\n",
    "    \"\"\"computeCov recieves as input an RDD of np arrays, all of the same length, \n",
    "    and computes the covariance matrix for that set of vectors\"\"\"\n",
    "    RDD=RDDin.map(lambda v:np.insert(v,0,1)) # insert a 1 at the beginning of each vector so that the same \n",
    "                                           #calculation also yields the mean vector\n",
    "    OuterRDD=RDD.map(outerProduct)   # separating the map and the reduce does not matter because of Spark uses lazy execution.\n",
    "    (S,N)=OuterRDD.reduce(sumWithNan)\n",
    "    # Unpack result and compute the covariance matrix\n",
    "    # print 'RDD=',RDD.collect()\n",
    "    # print 'shape of S=',S.shape,'shape of N=',N.shape\n",
    "    # print 'S=',S\n",
    "    # print 'N=',N\n",
    "    E=S[0,1:]\n",
    "    NE=np.float64(N[0,1:])\n",
    "    print 'shape of E=',E.shape,'shape of NE=',NE.shape\n",
    "    Mean=E/NE\n",
    "    O=S[1:,1:]\n",
    "    NO=np.float64(N[1:,1:])\n",
    "    Cov=O/NO - np.outer(Mean,Mean)\n",
    "    # Output also the diagnal which is the variance for each day\n",
    "    Var=np.array([Cov[i,i] for i in range(Cov.shape[0])])\n",
    "    return {'E':E,'NE':NE,'O':O,'NO':NO,'Cov':Cov,'Mean':Mean,'Var':Var}\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # create synthetic data matrix with j rows and rank k\n",
    "    \n",
    "    V=2*(np.random.random([2,10])-0.5)\n",
    "    data_list=[]\n",
    "    for i in range(1000):\n",
    "        f=2*(np.random.random(2)-0.5)\n",
    "        data_list.append(np.dot(f,V))\n",
    "    # compute covariance matrix\n",
    "    RDD=sc.parallelize(data_list)\n",
    "    OUT=computeCov(RDD)\n",
    "\n",
    "    #find PCA decomposition\n",
    "    eigval,eigvec=LA.eig(OUT['Cov'])\n",
    "    print 'eigval=',eigval\n",
    "    print 'eigvec=',eigvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.663874398391\n",
      "-0.669779782646\n",
      "-0.348153549432\n",
      "0.23661400356\n",
      "0.419256709861\n",
      "0.541662313304\n",
      "0.0778082555974\n",
      "-0.973508681384\n",
      "0.141734073067\n",
      "0.0878864067124\n"
     ]
    }
   ],
   "source": [
    "k=2;\n",
    "v1=2*(np.random.random(10)-0.5)\n",
    "v2=2*(np.random.random(10)-0.5)\n",
    "for i in range(10):\n",
    "    print 2*(np.random.random(2)-0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.52494753  0.71343136]\n",
      "[[-0.37686643  0.81022259  0.3631601  -0.69728499  0.39768481 -0.38220484\n",
      "  -0.38578483 -0.66212614 -0.08301629 -0.28822163]\n",
      " [-0.3519055  -0.23730313  0.76488067 -0.69442587 -0.06461917  0.67716477\n",
      "  -0.50260746 -0.18149416  0.59949313  0.50434405]]\n",
      "[-0.44889552  0.25602485  0.73632986 -0.86146323  0.16266232  0.2824731\n",
      " -0.56109272 -0.47706511  0.38411801  0.20851363]\n"
     ]
    }
   ],
   "source": [
    "D=2*(np.random.random([2,10])-0.5)\n",
    "f=2*(np.random.random(2)-0.5)\n",
    "print f\n",
    "print D\n",
    "print np.dot(f,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97934435, -0.00407431]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the overall distribution of values and the distribution of the number of nan per year\n",
    "def find_percentiles(SortedVals,percentile):\n",
    "  L=len(SortedVals)/percentile\n",
    "  return SortedVals[L],SortedVals[-L]\n",
    "  \n",
    "def computeOverAllDist(rdd0):\n",
    "  UnDef=np.array(rdd0.map(lambda row:sum(np.isnan(row))).sample(False,0.01).collect())\n",
    "  flat=rdd0.flatMap(lambda v:list(v)).filter(lambda x: not np.isnan(x)).cache()\n",
    "  count,S1,S2=flat.map(lambda x: np.float64([1,x,x**2]))\\\n",
    "                  .reduce(lambda x,y: x+y)\n",
    "  mean=S1/count\n",
    "  std=np.sqrt(S2/count-mean**2)\n",
    "  Vals=flat.sample(False,0.0001).collect()\n",
    "  SortedVals=np.array(sorted(Vals))\n",
    "  low100,high100=find_percentiles(SortedVals,100)\n",
    "  low1000,high1000=find_percentiles(SortedVals,1000)\n",
    "  return {'UnDef':UnDef,\\\n",
    "          'mean':mean,\\\n",
    "          'std':std,\\\n",
    "          'SortedVals':SortedVals,\\\n",
    "          'low100':low100,\\\n",
    "          'high100':high100,\\\n",
    "          'low1000':low100,\\\n",
    "          'high1000':high1000\n",
    "          }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/yfreund@ucsd.edu/Vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "AWS_BUCKET_NAME = \"mas-dse-public\" \n",
    "MOUNT_NAME = \"NCDC-weather\"\n",
    "OPEN_BUCKET_NAME = \"mas-dse-open\"\n",
    "OPEN_MOUNT_NAME = \"OPEN-weather\"\n",
    "dbutils.fs.unmount(\"/mnt/%s\" % MOUNT_NAME)\n",
    "dbutils.fs.unmount(\"/mnt/%s\" % OPEN_MOUNT_NAME)\n",
    "output_code=dbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n",
    "print 'Mount public status=',output_code\n",
    "output_code=dbutils.fs.mount(\"s3n://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, OPEN_BUCKET_NAME), \"/mnt/%s\" % OPEN_MOUNT_NAME)\n",
    "print 'Mount open status=',output_code\n",
    "\n",
    "file_list=dbutils.fs.ls('/mnt/%s/Weather'%MOUNT_NAME)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "file_list=dbutils.fs.ls('/mnt/%s/Weather'%OPEN_MOUNT_NAME)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "N=sc.defaultParallelism\n",
    "print 'Number of executors=',N\n",
    "\n",
    "STAT={}  # dictionary storing the statistics for each measurement\n",
    "for meas in measurements:\n",
    "\n",
    "  Query=\"SELECT * FROM parquet.`%s`\\n\\tWHERE measurement = '%s'\"%(US_Weather_parquet,meas)\n",
    "  print Query\n",
    "  df = sqlContext.sql(Query)\n",
    "  rdd0=df.map(lambda row:(row['station'],((row['measurement'],row['year']),np.array([np.float64(row[str(i)]) for i in range(1,366)])))).cache()\n",
    "\n",
    "  rdd1=rdd0.sample(False,1)\\\n",
    "           .map(lambda (key,val): val[1])\\\n",
    "           .cache()\\\n",
    "           .repartition(N)\n",
    "  print rdd1.count()\n",
    "\n",
    "  #get basic statistics\n",
    "  STAT[meas]=computeOverAllDist(rdd1)   # Compute the statistics \n",
    "  low1000 = STAT[meas]['low1000']  # unpack the extreme values statistics\n",
    "  high1000 = STAT[meas]['high1000']\n",
    "\n",
    "  #clean up table from extreme values and from rows with too many undefinde entries.\n",
    "  rdd2=rdd1.map(lambda V: np.array([x if (x>low1000-1) and (x<high1000+1) else np.nan for x in V]))\n",
    "  rdd3=rdd2.filter(lambda row:sum(np.isnan(row))<50)\n",
    "  Clean_Tables[meas]=rdd3.cache().repartition(N)\n",
    "  C=Clean_Tables[meas].count()\n",
    "  print 'for measurement %s, we get %d clean rows'%(meas,C)\n",
    "\n",
    "  # compute covariance matrix\n",
    "  OUT=computeCov(Clean_Tables[meas])\n",
    "\n",
    "  #find PCA decomposition\n",
    "  eigval,eigvec=LA.eig(OUT['Cov'])\n",
    "\n",
    "  # collect all of the statistics in STAT[meas]\n",
    "  STAT[meas]['eigval']=eigval\n",
    "  STAT[meas]['eigvec']=eigvec\n",
    "  STAT[meas].update(OUT)\n",
    "\n",
    "  # print summary of statistics\n",
    "  print 'the statistics for %s consists of:'%meas\n",
    "  for key in STAT[meas].keys():\n",
    "    e=STAT[meas][key]\n",
    "    if type(e)==list:\n",
    "      print key,'list',len(e)\n",
    "    elif type(e)==np.ndarray:\n",
    "      print key,'ndarray',e.shape\n",
    "    elif type(e)==np.float64:\n",
    "      print key,'scalar'\n",
    "    else:\n",
    "      print key,'Error type=',type(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "STAT_Descriptions=[\n",
    "('SortedVals', 'Sample of values', 'vector whose length varies between measurements'),\n",
    " ('UnDef', 'sample of number of undefs per row', 'vector whose length varies between measurements'),\n",
    " ('mean', 'mean value', ()),\n",
    " ('std', 'std', ()),\n",
    " ('low100', 'bottom 1%', ()),\n",
    " ('high100', 'top 1%', ()),\n",
    " ('low1000', 'bottom 0.1%', ()),\n",
    " ('high1000', 'top 0.1%', ()),\n",
    " ('E', 'Sum of values per day', (365,)),\n",
    " ('NE', 'count of values per day', (365,)),\n",
    " ('Mean', 'E/NE', (365,)),\n",
    " ('O', 'Sum of outer products', (365, 365)),\n",
    " ('NO', 'counts for outer products', (365, 365)),\n",
    " ('Cov', 'O/NO', (365, 365)),\n",
    " ('Var', 'The variance per day = diagonal of Cov', (365,)),\n",
    " ('eigval', 'PCA eigen-values', (365,)),\n",
    " ('eigvec', 'PCA eigen-vectors', (365, 365))\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import dumps\n",
    "dbutils.fs.put(\"/mnt/OPEN-weather/Weather/STAT.pickle\",dumps((STAT,STAT_Descriptions)),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample Stations\n",
    "Generate a sample of stations, for each one store all available year X measurement pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "US_Weather_parquet='/mnt/NCDC-weather/Weather/US_Weather.parquet/'\n",
    "measurements=['TMAX','TMIN','TOBS','SNOW','SNWD','PRCP']\n",
    "Query=\"SELECT * FROM parquet.`%s`\\n\\tWHERE \"%US_Weather_parquet+\"\\n\\tor \".join([\"measurement='%s'\"%m for m in measurements])\n",
    "print Query\n",
    "df = sqlContext.sql(Query)\n",
    "\n",
    "rdd0=df.map(lambda row:(str(row['station']),((str(row['measurement'])\\\n",
    "                        ,row['year']),np.array([np.float64(row[str(i)]) for i in range(1,366)]))))\\\n",
    ".cache().repartition(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd0.take(10) # test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "groups=rdd0.groupByKey().cache()\n",
    "print 'number of stations=',groups.count()\n",
    "\n",
    "groups1=groups.sample(False,0.01).collect()\n",
    "groups2=[(g[0],[e for e in g[1]]) for g in groups1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pickle import dumps\n",
    "dbutils.fs.put(\"/mnt/OPEN-weather/Weather/SampleStations_copy.pickle\",dumps(groups2),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "US_Weather_parquet='/mnt/NCDC-weather/Weather/US_Weather.parquet/'\n",
    "measurements=['TMAX','TMIN','TOBS','SNOW','SNWD','PRCP']\n",
    "Query=\"SELECT * FROM parquet.`%s`\\n\\tWHERE \"%US_Weather_parquet+\"\\n\\tor \".join([\"measurement='%s'\"%m for m in measurements])\n",
    "print Query\n",
    "df = sqlContext.sql(Query)\n",
    "\n",
    "rdd0=df.map(lambda row:((str(row['station']),str(row['measurement'])),\\\n",
    "                        ((row['year']),np.array([np.float64(row[str(i)]) for i in range(1,366)]))))\\\n",
    ".cache().repartition(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "group_Sample=rdd0.sample(False,0.001).groupByKey().mapValues(list).cache()\n",
    "group_Sample.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Means=rdd0.aggregateByKey((np.zeros(365),1),\\\n",
    "                          lambda S,D: sumWithNan(S,(D[1],1)),\\\n",
    "                          lambda S1,S2: sumWithNanithNan(S1,S2))\\\n",
    ".cache().repartition(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "Means.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "groups=rdd0.groupByKey().cache()\n",
    "print 'number of stations=',groups.count()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "PCA_using_numpy for HW3",
  "notebookId": 85286
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
