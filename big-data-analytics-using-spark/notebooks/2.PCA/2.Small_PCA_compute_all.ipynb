{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of executors= 8\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import pickle\n",
    "from numpy import linalg as LA\n",
    "sc.stop()\n",
    "sc = SparkContext(pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStats.py'])\n",
    "\n",
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from time import time\n",
    "N=sc.defaultParallelism\n",
    "print 'Number of executors=',N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./lib')\n",
    "\n",
    "import numpy as np\n",
    "from numpy_pack import packArray,unpackArray\n",
    "from spark_PCA import computeCov\n",
    "from computeStats import computeOverAllDist, STAT_Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SBBSBBSS'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def index_to_letters(n):\n",
    "    Letters=['B','S']\n",
    "    out=''\n",
    "    for i in range(8):\n",
    "        out+=Letters[n%2]\n",
    "        n=n/2\n",
    "    return out\n",
    "index_to_letters(201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl https://mas-dse-open.s3.amazonaws.com/Weather/small/US_Weather_BBBBBBBB.csv.gz > ../../Data/Weather/US_Weather_BBBBBBBB.csv.gz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4769k  100 4769k    0     0  2771k      0  0:00:01  0:00:01 --:--:-- 2772k\n",
      "-rw-r--r--  1 yoavfreund  staff   4.7M Apr 30 09:45 ../../Data/Weather/US_Weather_BBBBBBBB.csv.gz\n",
      "12854\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'TMAX'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for TMAX is 16.2257618904\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'SNOW'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for SNOW is 6.27778887749\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'SNWD'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for SNWD is 4.80610918999\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'TMIN'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for TMIN is 16.6750259399\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'PRCP'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for PRCP is 21.9909250736\n",
      "SELECT * FROM weather\n",
      "\tWHERE measurement = 'TOBS'\n",
      "shape of E= (365,) shape of NE= (365,)\n",
      "time for TOBS is 2.37084817886\n"
     ]
    }
   ],
   "source": [
    "#file_index='BBBSBBBB'\n",
    "data_dir='../../Data/Weather'\n",
    "\n",
    "for index in range(256):\n",
    "    \n",
    "    file_index=index_to_letters(index)\n",
    "\n",
    "    filebase='US_Weather_%s'%file_index\n",
    "    !rm -rf $data_dir/$filebase*\n",
    "\n",
    "    c_filename=filebase+'.csv.gz'\n",
    "    u_filename=filebase+'.csv'\n",
    "\n",
    "    command=\"curl https://mas-dse-open.s3.amazonaws.com/Weather/small/%s > %s/%s\"%(c_filename,data_dir,c_filename)\n",
    "    print command\n",
    "    !$command\n",
    "    !ls -lh $data_dir/$c_filename\n",
    "\n",
    "    #unzip\n",
    "    !gunzip -c $data_dir/$c_filename > $data_dir/$u_filename\n",
    "    List=pickle.load(open(data_dir+'/'+u_filename,'rb'))\n",
    "    len(List)\n",
    "\n",
    "    df=sqlContext.createDataFrame(List)\n",
    "    print df.count()\n",
    "\n",
    "    measurements=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']\n",
    "\n",
    "    sqlContext.registerDataFrameAsTable(df,'weather') #using older sqlContext instead of newer (V2.0) sparkSession\n",
    "\n",
    "    from numpy import linalg as LA\n",
    "    STAT={}  # dictionary storing the statistics for each measurement\n",
    "    Clean_Tables={}\n",
    "\n",
    "    for meas in measurements:\n",
    "        t=time()\n",
    "        Query=\"SELECT * FROM weather\\n\\tWHERE measurement = '%s'\"%(meas)\n",
    "        print Query\n",
    "        df = sqlContext.sql(Query)\n",
    "        data=df.rdd.map(lambda row: unpackArray(row['vector'],np.float16))\n",
    "        #get very basic statistics\n",
    "        STAT[meas]=computeOverAllDist(data)   # Compute the statistics \n",
    "\n",
    "        # compute covariance matrix\n",
    "        OUT=computeCov(data)\n",
    "\n",
    "        #find PCA decomposition\n",
    "        eigval,eigvec=LA.eig(OUT['Cov'])\n",
    "\n",
    "        # collect all of the statistics in STAT[meas]\n",
    "        STAT[meas]['eigval']=eigval\n",
    "        STAT[meas]['eigvec']=eigvec\n",
    "        STAT[meas].update(OUT)\n",
    "\n",
    "        print 'time for',meas,'is',time()-t\n",
    "\n",
    "    filename=data_dir+'/STAT_%s.pickle'%file_index\n",
    "    pickle.dump((STAT,STAT_Descriptions),open(filename,'wb'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1454016\r\n",
      "-rw-r--r--    1 yoavfreund  staff  417856543 Mar  6 13:38 SampleStations.pickle\r\n",
      "-rw-r--r--    1 yoavfreund  staff    2816937 Mar  6 13:52 stations_projections.pickle\r\n",
      "-rw-r--r--    1 yoavfreund  staff   13888914 Mar  6 13:52 stations.pkl\r\n",
      "-rw-r--r--    1 yoavfreund  staff     210335 Mar  6 13:52 stable-clusters.pickle\r\n",
      "-rw-r--r--    1 yoavfreund  staff        270 Mar  6 13:52 ghcnd-version.txt\r\n",
      "-rw-r--r--    1 yoavfreund  staff    7760844 Mar  6 13:52 ghcnd-stations_buffered.txt\r\n",
      "-rw-r--r--    1 yoavfreund  staff    8357832 Mar  6 13:52 ghcnd-stations.txt\r\n",
      "-rw-r--r--    1 yoavfreund  staff      22422 Mar  6 13:52 ghcnd-readme.txt\r\n",
      "-rw-r--r--    1 yoavfreund  staff        218 Mar  6 13:52 data-source.txt\r\n",
      "-rw-r--r--    1 yoavfreund  staff        243 Mar  9 20:56 STAT1.pickle\r\n",
      "-rw-r--r--    1 yoavfreund  staff   81681132 Mar  9 21:05 STAT.pickle\r\n",
      "drwxr-xr-x   10 yoavfreund  staff        340 Apr 21 12:10 \u001b[34mUS_Weather_BBBSBBBB.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--    1 yoavfreund  staff   82276503 Apr 21 12:20 STAT_BBBSBBBB.pickle\r\n",
      "drwxr-xr-x  404 yoavfreund  staff      13736 Apr 23 14:20 \u001b[34mdecon_BBBSBBBB.parquet\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  404 yoavfreund  staff      13736 Apr 23 16:14 \u001b[34mdecon_BBBSBBBB_TOBS.parquet\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  404 yoavfreund  staff      13736 Apr 23 16:20 \u001b[34mdecon_BBBSBBBB_PRCP.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--    1 yoavfreund  staff   13222287 Apr 25 12:54 US_Weather_BBBSBBBB.csv\r\n",
      "drwxr-xr-x  404 yoavfreund  staff      13736 Apr 29 14:07 \u001b[34mdecon_BBBSBBBB_SNWD.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--    1 yoavfreund  staff    3245918 Apr 30 09:10 US_Weather_SSSSBBBB.csv.gz\r\n",
      "-rw-r--r--    1 yoavfreund  staff   12880638 Apr 30 09:10 US_Weather_SSSSBBBB.csv\r\n",
      "drwxr-xr-x   10 yoavfreund  staff        340 Apr 30 09:10 \u001b[34mUS_Weather_SSSSBBBB.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--    1 yoavfreund  staff    4883744 Apr 30 09:45 US_Weather_BBBBBBBB.csv.gz\r\n",
      "-rw-r--r--    1 yoavfreund  staff   13840604 Apr 30 09:45 US_Weather_BBBBBBBB.csv\r\n",
      "-rw-r--r--    1 yoavfreund  staff   81469151 Apr 30 09:47 STAT_BBBBBBBB.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lrt ../../Data/Weather/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "PCA_using_numpy for HW3",
  "notebookId": 85286,
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "116px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
