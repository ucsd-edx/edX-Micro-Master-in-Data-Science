{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Count\n",
    "Counting the number of occurances of words in a text is one of the most popular first eercises when learning Map-Reduce Programming. It is the equivalent to `Hello World!` in regular programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#start the SparkContext\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Read text into an RDD\n",
    "\n",
    "### Download data file from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 yoavfreund  staff  1257260 Feb  6 19:20 ../../Data/Moby-Dick.txt\n",
      "CPU times: user 37.2 ms, sys: 26 ms, total: 63.2 ms\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import urllib\n",
    "data_dir='../../Data'\n",
    "filename='Moby-Dick.txt'\n",
    "f = urllib.request.urlretrieve (\"https://mas-dse-open.s3.amazonaws.com/\"+filename, data_dir+'/'+filename)\n",
    "\n",
    "# First, check that the text file is where we expect it to be\n",
    "!ls -l $data_dir/$filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define an RDD that will read the file\n",
    "Note that, as execution is Lazy, this does not necessarily mean that actual reading of the file content has occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.63 ms, sys: 3.79 ms, total: 7.42 ms\n",
      "Wall time: 786 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(data_dir+'/'+filename)\n",
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Counting the words\n",
    "* split line by spaces.\n",
    "* map `word` to `(word,1)`\n",
    "* count the number of occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 ms, sys: 3.89 ms, total: 15.2 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .filter(lambda x: x!='')\\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "type(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Have a look a the execution plan\n",
    "Note that the earliest node in the dependency graph is the file `../../Data/Moby-Dick.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'(2) PythonRDD[6] at RDD at PythonRDD.scala:48 []',\n",
       " b' |  MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []',\n",
       " b' |  ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 []',\n",
       " b' +-(2) PairwiseRDD[3] at reduceByKey at <timed exec>:1 []',\n",
       " b'    |  PythonRDD[2] at reduceByKey at <timed exec>:1 []',\n",
       " b'    |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []',\n",
       " b'    |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.toDebugString().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Count!\n",
    "Finally we count the number of times each word has occured.\n",
    "Now, finally, the Lazy execution model finally performs some actual work, which takes a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 10.3 ms, sys: 4.27 ms, total: 14.6 ms\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the most common words\n",
    "* `counts`: RDD with 33301 pairs of the form `(word,count)`. \n",
    "* Find the 2 most frequent words. \n",
    "* **Method1:** `collect` and sort on head node.\n",
    "* **Method2:** Pure Spark, `collect` only at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Method1: `collect` and sort on head node \n",
    "#### Collect the RDD into the driver node\n",
    "* Collect can take significant time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 5.58 ms, total: 17.6 ms\n",
      "Wall time: 78.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C=counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sort \n",
    "* RDD collected into list in driver node.\n",
    "* No longer using spark parallelism.\n",
    "* Sort in python\n",
    "* will not scale to very large documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words\n",
      " to:\t4510\n",
      "a:\t4533\n",
      "and:\t5951\n",
      "of:\t6587\n",
      "the:\t13766\n",
      "\n",
      "Least common words\n",
      " EBook:\t1\n",
      "Author::\t1\n",
      "Last:\t1\n",
      "January:\t1\n",
      "Posting:\t1\n"
     ]
    }
   ],
   "source": [
    "C.sort(key=lambda x:x[1])\n",
    "print('most common words\\n','\\n'.join(['%s:\\t%d'%c for c in C[-5:]]))\n",
    "print('\\nLeast common words\\n','\\n'.join(['%s:\\t%d'%c for c in C[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Compute the mean number of occurances per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count2=33781.000000, sum2=215133.000000, mean2=6.368462\n"
     ]
    }
   ],
   "source": [
    "Count2=len(C)\n",
    "Sum2=sum([i for w,i in C])\n",
    "print('count2=%f, sum2=%f, mean2=%f'%(Count2,Sum2,float(Sum2)/Count2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Method2:** Pure Spark, `collect` only at the end.\n",
    "* Collect into the head node only the more frquent words.\n",
    "* Requires multiple **stages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 1 split, clean and map to `(word,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 µs, sys: 0 ns, total: 28 µs\n",
      "Wall time: 33.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD=text_file.flatMap(lambda x: x.split(' '))\\\n",
    "    .filter(lambda x: x!='')\\\n",
    "    .map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### **Step 2** Count occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.97 ms, sys: 2.71 ms, total: 8.68 ms\n",
      "Wall time: 20.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD1=RDD.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### **Step 3** Reverse `(word,count)` to `(count,word)` and sort by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 ms, sys: 4.86 ms, total: 19.7 ms\n",
      "Wall time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD2=RDD1.map(lambda x:(x[1],x[0]))   # reverse order of word and count\n",
    "RDD3=RDD2.sortByKey(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Full execution plan\n",
    "\n",
    "We now have a complete plan to compute the most common words in the text. Nothing has been executed yet! Not even a single byte has been read from the file `Moby-Dick.txt` !\n",
    "\n",
    "For more on execution plans and lineage see [jace Klaskowski's blog](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-lineage.html#toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD3:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'(2) PythonRDD[19] at RDD at PythonRDD.scala:48 []',\n",
       " b' |  MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:436 []',\n",
       " b' |  ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:0 []',\n",
       " b' +-(2) PairwiseRDD[16] at sortByKey at <timed exec>:2 []',\n",
       " b'    |  PythonRDD[15] at sortByKey at <timed exec>:2 []',\n",
       " b'    |  MapPartitionsRDD[12] at mapPartitions at PythonRDD.scala:436 []',\n",
       " b'    |  ShuffledRDD[11] at partitionBy at NativeMethodAccessorImpl.java:0 []',\n",
       " b'    +-(2) PairwiseRDD[10] at reduceByKey at <timed exec>:1 []',\n",
       " b'       |  PythonRDD[9] at reduceByKey at <timed exec>:1 []',\n",
       " b'       |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []',\n",
       " b'       |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('RDD3:')\n",
    "RDD3.toDebugString().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### **Step 4** Take the top 5 words. **only now the computer executes the plan!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words\n",
      " 13766:\tthe\n",
      "6587:\tof\n",
      "5951:\tand\n",
      "4533:\ta\n",
      "4510:\tto\n",
      "CPU times: user 9.05 ms, sys: 3.19 ms, total: 12.2 ms\n",
      "Wall time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C=RDD3.take(5)\n",
    "print('most common words\\n','\\n'.join(['%d:\\t%s'%c for c in C]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "This was our first pyspark program, hurray!\n",
    "\n",
    "What did we learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) The core data structure of Spark is an RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) An RDD is a distributed array. However, it appears to the programmer as an abstract object on which the programmer can perform `map` and `reduce` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) `map` operations transform an RDD into another RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4) `reduce` and `collect` operations transform an RDD back into a python array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5) Once an RDD is reduce, your program is running on a single computer - the head node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Next\n",
    "Complete the HW notebook, submit it for evaluation, and continue to the next class, in which we will learn more about spark."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "349px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
