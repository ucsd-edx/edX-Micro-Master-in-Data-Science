{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Count\n",
    "\n",
    "Counting the number of occurances of words in a text is a popular first exercise using map-reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Task\n",
    "**Input:** A text file consisisting of words separated by spaces.  \n",
    "**Output:** A list of words and their counts, sorted from the most to the least common.\n",
    "\n",
    "We will use the book \"Moby Dick\" as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#start the SparkContext\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local[4]\")\n",
    "\n",
    "# set import path\n",
    "import sys\n",
    "sys.path.append('../../Utils/')\n",
    "from plan2table import plan2table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Download data file from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 yoavfreund  staff  1257260 Feb 24 13:09 ../../Data/Moby-Dick.txt\n",
      "CPU times: user 146 ms, sys: 48.9 ms, total: 195 ms\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "data_dir='../../Data'\n",
    "filename='Moby-Dick.txt'\n",
    "url = \"https://mas-dse-open.s3.amazonaws.com/\"+filename\n",
    "local_path = data_dir+'/'+filename\n",
    "\n",
    "# Copy URL content to local_path\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open(local_path, 'wb').write(r.content)\n",
    "\n",
    "# check that the text file is where we expect it to be\n",
    "!ls -l $local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define an RDD that will read the file\n",
    "* As execution is **lazy**, this does not necessarily mean that actual reading of the file content has occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.43 ms, sys: 2.61 ms, total: 5.05 ms\n",
      "Wall time: 667 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(data_dir+'/'+filename)\n",
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Steps for counting the words\n",
    "\n",
    "* split line by spaces.\n",
    "* map `word` to `(word,1)`\n",
    "* count the number of occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 ms, sys: 3.81 ms, total: 14 ms\n",
      "Wall time: 83.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "not_empty = words.filter(lambda x: x!='')\n",
    "key_values=  not_empty.map(lambda word: (word, 1)) \n",
    "counts=     key_values.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We have a plan!\n",
    "In this cell we defined the execution plan, but we have not started to execute it.\n",
    "\n",
    "Note that preparing the plan took ~100ms, which is a non-trivial amount of time, but still much less than the time it will take to execute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lets have a look a the execution plan\n",
    "Note that the earliest node in the dependency graph is the file `../../Data/Moby-Dick.txt`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Print the execution plan for each of the RDDs\n",
    "Note that the execution plan for `words`, `not_empty` and `key_values` are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Native`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodA`| rdd |\n"
     ]
    }
   ],
   "source": [
    "plan2table(text_file.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_PythonRDD[6] at RDD at PythonRDD.scala:48 []`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Native`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodA`| rdd |\n"
     ]
    }
   ],
   "source": [
    "plan2table(words.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_PythonRDD[7] at RDD at PythonRDD.scala:48 []`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Native`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodA`| rdd |\n"
     ]
    }
   ],
   "source": [
    "plan2table(not_empty.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_PythonRDD[8] at RDD at PythonRDD.scala:48 []`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Native`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodA`| rdd |\n"
     ]
    }
   ],
   "source": [
    "plan2table(key_values.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_PythonRDD[9] at RDD at PythonRDD.scala:48 []`| rdd |\n",
      "|`_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []`| rdd |\n",
      "|`_/__ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 [`| rdd |\n",
      "|`_+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 []`| rdd |\n",
      "|`____/__PythonRDD[2] at reduceByKey at <timed exec>:4 []`| rdd |\n",
      "|`____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat`| rdd |\n",
      "|`____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth`| rdd |\n"
     ]
    }
   ],
   "source": [
    "plan2table(counts.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Execution plan   | RDD |\n",
    "| :---------------------------------------------------------------- | :------------: |\n",
    "|`(2)_PythonRDD[6] at RDD at PythonRDD.scala:48 []`| **counts** |\n",
    "|`_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []`| **---\"---** |\n",
    "|`_/__ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 [`| **---\"---** |\n",
    "|`_+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 []`| **---\"---** |\n",
    "|`____/__PythonRDD[2] at reduceByKey at <timed exec>:4 []`| **words, not_empty, key_values** |\n",
    "|`____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat`| **text_file** |\n",
    "|`____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth`| **---\"---** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Count!\n",
    "Finally we count the number of times each word has occured.\n",
    "Now, finally, the Lazy execution model finally performs some actual work, which takes a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 12 ms, sys: 5.69 ms, total: 17.6 ms\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run #1\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 10.8 ms, sys: 4.46 ms, total: 15.2 ms\n",
      "Wall time: 133 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run #2\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 8.47 ms, sys: 4.21 ms, total: 12.7 ms\n",
      "Wall time: 152 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run #3, cache\n",
    "Count=counts.cache().count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 9.82 ms, sys: 4.37 ms, total: 14.2 ms\n",
      "Wall time: 83.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run #4\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 12 ms, sys: 5.11 ms, total: 17.1 ms\n",
      "Wall time: 90.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run #5\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the most common words\n",
    "* `counts`: RDD with 33301 pairs of the form `(word,count)`. \n",
    "* Find the 2 most frequent words. \n",
    "* **Method1:** `collect` and sort on head node.\n",
    "* **Method2:** Pure Spark, `collect` only at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Method1: `collect` and sort on head node \n",
    "#### Collect the RDD into the driver node\n",
    "* Collect can take significant time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.2 ms, sys: 12.6 ms, total: 67.9 ms\n",
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C=counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sort \n",
    "* RDD collected into list in driver node.\n",
    "* No longer using spark parallelism.\n",
    "* Sort in python\n",
    "* will not scale to very large documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words\n",
      " to:\t4510\n",
      "a:\t4533\n",
      "and:\t5951\n",
      "of:\t6587\n",
      "the:\t13766\n",
      "\n",
      "Least common words\n",
      " EBook:\t1\n",
      "Author::\t1\n",
      "Last:\t1\n",
      "January:\t1\n",
      "Posting:\t1\n"
     ]
    }
   ],
   "source": [
    "C.sort(key=lambda x:x[1])\n",
    "print('most common words\\n','\\n'.join(['%s:\\t%d'%c for c in C[-5:]]))\n",
    "print('\\nLeast common words\\n','\\n'.join(['%s:\\t%d'%c for c in C[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Compute the mean number of occurances per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count2=33781.000000, sum2=215133.000000, mean2=6.368462\n"
     ]
    }
   ],
   "source": [
    "Count2=len(C)\n",
    "Sum2=sum([i for w,i in C])\n",
    "print('count2=%f, sum2=%f, mean2=%f'%(Count2,Sum2,float(Sum2)/Count2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Method2:** Pure Spark, `collect` only at the end.\n",
    "* Collect into the head node only the more frquent words.\n",
    "* Requires multiple **stages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Step 1 split, clean and map to `(word,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43 µs, sys: 1 µs, total: 44 µs\n",
      "Wall time: 71.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_pairs=text_file.flatMap(lambda x: x.split(' '))\\\n",
    "    .filter(lambda x: x!='')\\\n",
    "    .map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### **Step 2** Count occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.53 ms, sys: 2.59 ms, total: 11.1 ms\n",
      "Wall time: 51.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts=word_pairs.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### **Step 3** Reverse `(word,count)` to `(count,word)` and sort by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 ms, sys: 7.31 ms, total: 28.4 ms\n",
      "Wall time: 490 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reverse_counts=counts.map(lambda x:(x[1],x[0]))   # reverse order of word and count\n",
    "sorted_counts=reverse_counts.sortByKey(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Full execution plan\n",
    "\n",
    "We now have a complete plan to compute the most common words in the text. Nothing has been executed yet! Not even a single byte has been read from the file `Moby-Dick.txt` !\n",
    "\n",
    "For more on execution plans and lineage see [jace Klaskowski's blog](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-lineage.html#toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_pairs:\n",
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_PythonRDD[30] at RDD at PythonRDD.scala:48 []`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Native`| rdd |\n",
      "|`_/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodA`| rdd |\n",
      "\n",
      "counts:\n",
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_PythonRDD[31] at RDD at PythonRDD.scala:48 []`| rdd |\n",
      "|`_/__MapPartitionsRDD[23] at mapPartitions at PythonRDD.scala:436 []`| rdd |\n",
      "|`_/__ShuffledRDD[22] at partitionBy at NativeMethodAccessorImpl.java:0 `| rdd |\n",
      "|`_+-(2)_PairwiseRDD[21] at reduceByKey at <timed exec>:1 []`| rdd |\n",
      "|`____/__PythonRDD[20] at reduceByKey at <timed exec>:1 []`| rdd |\n",
      "|`____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat`| rdd |\n",
      "|`____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth`| rdd |\n",
      "\n",
      "reverse_counts:\n",
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_PythonRDD[32] at RDD at PythonRDD.scala:48 []`| rdd |\n",
      "|`_/__MapPartitionsRDD[23] at mapPartitions at PythonRDD.scala:436 []`| rdd |\n",
      "|`_/__ShuffledRDD[22] at partitionBy at NativeMethodAccessorImpl.java:0 `| rdd |\n",
      "|`_+-(2)_PairwiseRDD[21] at reduceByKey at <timed exec>:1 []`| rdd |\n",
      "|`____/__PythonRDD[20] at reduceByKey at <timed exec>:1 []`| rdd |\n",
      "|`____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat`| rdd |\n",
      "|`____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth`| rdd |\n",
      "\n",
      "sorted_counts:\n",
      "\n",
      "| Execution plan   | RDD |\n",
      "| :---------------------------------------------------------------- | :------------: |\n",
      "|`(2)_PythonRDD[33] at RDD at PythonRDD.scala:48 []`| rdd |\n",
      "|`_/__MapPartitionsRDD[29] at mapPartitions at PythonRDD.scala:436 []`| rdd |\n",
      "|`_/__ShuffledRDD[28] at partitionBy at NativeMethodAccessorImpl.java:0 `| rdd |\n",
      "|`_+-(2)_PairwiseRDD[27] at sortByKey at <timed exec>:2 []`| rdd |\n",
      "|`____/__PythonRDD[26] at sortByKey at <timed exec>:2 []`| rdd |\n",
      "|`____/__MapPartitionsRDD[23] at mapPartitions at PythonRDD.scala:436 []`| rdd |\n",
      "|`____/__ShuffledRDD[22] at partitionBy at NativeMethodAccessorImpl.java`| rdd |\n",
      "|`____+-(2)_PairwiseRDD[21] at reduceByKey at <timed exec>:1 []`| rdd |\n",
      "|`_______/__PythonRDD[20] at reduceByKey at <timed exec>:1 []`| rdd |\n",
      "|`_______/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at `| rdd |\n",
      "|`_______/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeM`| rdd |\n"
     ]
    }
   ],
   "source": [
    "print('word_pairs:')\n",
    "plan2table(word_pairs.toDebugString())\n",
    "print('\\ncounts:')\n",
    "plan2table(counts.toDebugString())\n",
    "print('\\nreverse_counts:')\n",
    "plan2table(reverse_counts.toDebugString())\n",
    "print('\\nsorted_counts:')\n",
    "plan2table(sorted_counts.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted_counts:\n",
    "\n",
    "| Execution plan   | RDD |\n",
    "| :---------------------------------------------------------------- | :------------: |\n",
    "|`(2)_PythonRDD[20] at RDD at PythonRDD.scala:48 []`| **sorted_counts** |\n",
    "|`_/__MapPartitionsRDD[19] at mapPartitions at PythonRDD.scala:436 []`| **---\"---** |\n",
    "|`_/__ShuffledRDD[18] at partitionBy at NativeMethodAccessorImpl.java:0 `| **---\"---** |\n",
    "|`_+-(2)_PairwiseRDD[17] at sortByKey at <timed exec>:2 []`| **---\"---** |\n",
    "|`____/__PythonRDD[16] at sortByKey at <timed exec>:2 []`| ** counts, reverse_counts** |\n",
    "|`____/__MapPartitionsRDD[13] at mapPartitions at PythonRDD.scala:436 []`| **---\"---** |\n",
    "|`____/__ShuffledRDD[12] at partitionBy at NativeMethodAccessorImpl.java`| **---\"---** |\n",
    "|`____+-(2)_PairwiseRDD[11] at reduceByKey at <timed exec>:1 []`| rdd |\n",
    "|`_______/__PythonRDD[10] at reduceByKey at <timed exec>:1 []`| **word_pairs** |\n",
    "|`_______/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at `| **---\"---** |\n",
    "|`_______/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeM`| **---\"---** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### **Step 4** Take the top 5 words. **only now the computer executes the plan!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RDD3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4be829377ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"C=RDD3.take(5)\\nprint('most common words\\\\n','\\\\n'.join(['%d:\\\\t%s'%c for c in C]))\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/yoavfreund/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/yoavfreund/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yoavfreund/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RDD3' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C=RDD3.take(5)\n",
    "print('most common words\\n','\\n'.join(['%d:\\t%s'%c for c in C]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "This was our first pyspark program, hurray!\n",
    "\n",
    "What did we learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) The core data structure of Spark is an RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) An RDD is a distributed array. However, it appears to the programmer as an abstract object on which the programmer can perform `map` and `reduce` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) `map` operations transform an RDD into another RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4) `reduce` and `collect` operations transform an RDD back into a python array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5) Once an RDD is reduce, your program is running on a single computer - the head node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Next\n",
    "Complete the HW notebook, submit it for evaluation, and continue to the next class, in which we will learn more about spark."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "349px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
