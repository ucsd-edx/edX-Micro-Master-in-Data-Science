{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Spark SQL, DataFrames and  DataSets](https://spark.apache.org/docs/1.6.1/sql-programming-guide.html#spark-sql-dataframes-and-datasets-guide)\n",
    "When working with datasets that are disk resident, there is a big advantage to using methods developed for Relational Database Management Systems (RDBMS). The corresponding API is called `pyspark SQL`. For us the main object of interest will be a single database table, which corresponds to the pyspark SQL `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataframes \n",
    "Dataframes are a special type of RDDs. They are similar to, but not the same as, pandas dataframes. They are used to store two dimensional data, similar to the type of data stored in a spreadsheet. Each column in a dataframe can have a different type and each row contains a `record`.\n",
    "\n",
    "Spark DataFrames are similar to `pandas` DataFrames. With the important difference that spark DataFrames are **distributed** data structures, based on RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x109909210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constructing a DataFrame from an RDD of Rows\n",
    "Each Row defines it's own  fields, the schema is *inferred*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, name=u'John'),\n",
       " Row(age=23, name=u'Smith'),\n",
       " Row(age=18, name=u'Sarah')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to create a DataFrame is to first define an RDD from a list of rows\n",
    "some_rdd = sc.parallelize([Row(name=u\"John\", age=19),\n",
    "                           Row(name=u\"Smith\", age=23),\n",
    "                           Row(name=u\"Sarah\", age=18)])\n",
    "some_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The DataFrame is created from the RDD or Rows\n",
    "# Infer schema from the first row, create a DataFrame and print the schema\n",
    "some_df = sqlContext.createDataFrame(some_rdd)\n",
    "some_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'> <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "some_df = [Row(age=19, name=u'John'), Row(age=23, name=u'Smith'), Row(age=18, name=u'Sarah')]\n",
      "some_rdd= [Row(age=19, name=u'John'), Row(age=23, name=u'Smith'), Row(age=18, name=u'Sarah')]\n"
     ]
    }
   ],
   "source": [
    "# A dataframe is an RDD of rows plus information on the schema.\n",
    "# performing **collect()* on either the RDD or the DataFrame gives the same result.\n",
    "print type(some_rdd),type(some_df)\n",
    "print 'some_df =',some_df.collect()\n",
    "print 'some_rdd=',some_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining the Schema explicitly\n",
    "The advantage of creating a DataFrame using a pre-defined schema allows the content of the RDD to be simple tuples, rather than rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_name: string (nullable = false)\n",
      " |-- person_age: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly\n",
    "another_rdd = sc.parallelize([(\"John\", 19), (\"Smith\", 23), (\"Sarah\", 18)])\n",
    "# Schema with two fields - person_name and person_age\n",
    "schema = StructType([StructField(\"person_name\", StringType(), False),\n",
    "                     StructField(\"person_age\", IntegerType(), False)])\n",
    "\n",
    "# Create a DataFrame by applying the schema to the RDD and print the schema\n",
    "another_df = sqlContext.createDataFrame(another_rdd, schema)\n",
    "another_df.printSchema()\n",
    "# root\n",
    "#  |-- age: binteger (nullable = true)\n",
    "#  |-- name: string (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Loading DataFrames from disk\n",
    "There are many maethods to load DataFrames from Disk. Here we will discuss three of these methods\n",
    "1. JSON \n",
    "2. CSV\n",
    "3. Parquet\n",
    "\n",
    "In addition, there are API's for connecting Spark to an external database. We will not discuss this type of connection in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading dataframes from JSON files\n",
    "[JSON](http://www.json.org/) is a very popular readable file format for storing structured data.\n",
    "Among it's many uses are **twitter**, `javascript` communication packets, and many others. In fact this notebook file (with the extension `.ipynb` is in json format. JSON can also be used to store tabular data and can be easily loaded into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\r\n",
      "{\"name\":\"Andy\", \"age\":30}\r\n",
      "{\"name\":\"Justin\", \"age\":19}\r\n"
     ]
    }
   ],
   "source": [
    "# when loading json files you can specify either a single file or a directory containing many json files.\n",
    "path = \"../../Data/people.json\"\n",
    "!cat $path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people is a <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the file(s) pointed to by path\n",
    "people = sqlContext.read.json(path)\n",
    "print 'people is a',type(people)\n",
    "# The inferred schema can be visualized using the printSchema() method.\n",
    "print\n",
    "people.printSchema()\n",
    "# root\n",
    "#  |-- age: IntegerType\n",
    "#  |-- name: StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise: Loading csv files into dataframes\n",
    "\n",
    "Spark 2.0 includes a facility for reading csv files. In this excercise you are to create similar functionality using your own code.\n",
    "\n",
    "You are to write a class called `csv_reader` which has the following methods:\n",
    "\n",
    "* `__init__(self,filepath):` recieves as input the path to a csv file. It throws an exeption `NoSuchFile` if the file does not exist.\n",
    "* `Infer_Schema()` opens the file, reads the first 10 lines (or less if the file is shorter), and infers the schema. The first line of the csv file defines the column names. The following lines should have the same number of columns and all of the elements of the column should be of the same type. The only types allowd are `int`,`float`,`string`. The method infers the types of the columns, checks that they are consistent, and defines a dataframe schema of the form:\n",
    "```python\n",
    "schema = StructType([StructField(\"person_name\", StringType(), False),\n",
    "                     StructField(\"person_age\", IntegerType(), False)])\n",
    "```\n",
    "If everything checks out, the method defines a `self.` variable that stores the schema and returns the schema as it's output. If an error is found an exception `BadCsvFormat` is raised.\n",
    "* `read_DataFrame()`: reads the file, parses it and creates a dataframe using the inferred schema. If one of the lines beyond the first 10 (i.e. a line that was not read by `InferSchema`) is not parsed correctly, the line is not added to the Dataframe. Instead, it is added to an RDD called `bad_lines`.\n",
    "The methods returns the dateFrame and the `bad_lines` RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parquet files\n",
    "[Parquet](http://parquet.apache.org/) is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### More about Parquet\n",
    "Parquet is a column-based file format and uses disk-resident data structure that support efficient access to subsets of the records. As a result, loading a subset of the records from a Parquet file is much more efficient than loading the same subset of records from a csv or json file. In addition, parquet is compatible with HDFS which further accelerates record retrieval in a distributed system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moby-Dick.txt example.csv   people.json   users.parquet\r\n",
      "\u001b[34mWeather\u001b[m\u001b[m       \u001b[35mold_data\u001b[m\u001b[m      table.csv\r\n"
     ]
    }
   ],
   "source": [
    "dir='../../Data'\n",
    "parquet_file=dir+\"/users.parquet\"\n",
    "!ls $dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load a Parquet file\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|  name|favorite_color|\n",
      "+------+--------------+\n",
      "|Alyssa|          null|\n",
      "|   Ben|           red|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.select(\"name\", \"favorite_color\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x  10 yoavfreund  staff  340 Apr 16 22:23 \u001b[34m../../Data/namesAndFavColors.parquet\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 yoavfreund  staff  615 Apr 16 22:21 ../../Data/users.parquet\r\n"
     ]
    }
   ],
   "source": [
    "df2.write.save(dir+\"/namesAndFavColors.parquet\")\n",
    "!ls -ld $dir/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A new interface object has been added in **Spark 2.0** called **SparkSession**. A spark session is initialized using a `builder`. For example\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "         .master(\"local\") \\\n",
    "         .appName(\"Word Count\") \\\n",
    "         .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "         .getOrCreate()\n",
    "```\n",
    "\n",
    "Using a SparkSession a Parquet file is read [as follows:](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.parquet):\n",
    "```python\n",
    "df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using SQL queries on DataFrames\n",
    "You can use [Hive `select` syntax](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select) to select a subset of the rows in a dataframe.\n",
    "Spark supports a [subset](https://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features) of the Hive SQL query language\n",
    "\n",
    "To use sql on a dataframe you need to first `register` it as a `TempTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justin\n"
     ]
    }
   ],
   "source": [
    "# Register this DataFrame as a table.\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "# SQL statements can be run by using the sql methods provided by sqlContext\n",
    "teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "for each in teenagers.collect():\n",
    "    print(each[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A somewhat bigger example\n",
    "This file contains 1/1000 of the original file, which contains about 9 million lines and whose size is about 7GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yoavfreund/projects/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/Data\n",
      "total 2496\n",
      "-rw-r--r--   1 yoavfreund  staff       73 Mar  1 17:51 people.json\n",
      "lrwxr-xr-x   1 yoavfreund  staff      101 Mar  2 14:03 \u001b[35mold_data\u001b[m\u001b[m -> /Users/yoavfreund/academic.papers/Courses/BigDataAnalytics/BigData_spring2016/UCSD_BigData_2016/Data/\n",
      "-rw-r--r--   1 yoavfreund  staff       43 Mar  2 14:04 example.csv\n",
      "-rw-r--r--   1 yoavfreund  staff       75 Mar  2 14:06 table.csv\n",
      "drwxr-xr-x  16 yoavfreund  staff      544 Mar 24 20:15 \u001b[34mWeather\u001b[m\u001b[m\n",
      "-rw-r--r--   1 yoavfreund  staff  1257260 Apr 10 21:33 Moby-Dick.txt\n",
      "-rw-r--r--   1 yoavfreund  staff      615 Apr 16 22:21 users.parquet\n",
      "drwxr-xr-x  10 yoavfreund  staff      340 Apr 16 22:23 \u001b[34mnamesAndFavColors.parquet\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%cd ../../Data\n",
    "import urllib\n",
    "# If these commands don't work, you can download the files using your rowser\n",
    "f=urllib.urlretrieve(\"https://drive.google.com/open?id=0B8IGBNGc5gVANWtlUElzd0JxV1E\")\n",
    "#!curl  'https://drive.google.com/open?id=0B8IGBNGc5gVANWtlUElzd0JxV1E' > test.tgz\n",
    "!ls -lrt \n",
    "#!tar xzvf  weather.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/var/folders/80/c2kfvdvx5cx570r4vlzqgb840000gq/T/tmpVzIBVL',\n",
       " <httplib.HTTPMessage instance at 0x1099d2488>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "parquet_file=dir+\"/Weather_sampled.parquet\"\n",
    "df = sqlContext.read.load(parquet_file)\n",
    "print 'count=',df.count()\n",
    "print 'columns=',df.columns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Count the number of occurances of each measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "L=df.groupBy('measurement').count().collect()\n",
    "D=[(e.measurement,e['count']) for e in L]\n",
    "sorted(D,key=lambda x:x[1], reverse=True)[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading only select lines\n",
    "Suppose we are only interested in snow measurements. We can apply an SQL query directly to the \n",
    "parquet files. As the data is organized in columnar structure, we can do the selection efficiently without loading the whole file to memory.\n",
    "\n",
    "Here the file is small, but in real applications it can consist of hundreds of millions of records. In such cases loading the data first to memory and then filtering it is very wasteful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "query='SELECT station,measurement,year FROM parquet.`%s` WHERE measurement=\"SNOW\"'%parquet_file\n",
    "print query\n",
    "df2 = sqlContext.sql(query)\n",
    "print df2.count(),df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For more on DataFrames see: \n",
    "* [API for the DataFrame class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
    "\n",
    "For more on Spark SQL see:\n",
    "* [API for the pyspark.sql module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "263px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
