{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Count\n",
    "Counting the number of occurances of words in a text is one of the most popular first eercises when learning Map-Reduce Programming. It is the equivalent to `Hello World!` in regular programming.\n",
    "\n",
    "We will do it two way, a simpler way where sorting is done after the RDD is collected, and a more sparky way, where the sorting is also done using an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 yoavfreund  staff  1237539 Feb 27 12:24 ../../Data/Moby-Dick.txt\r\n"
     ]
    }
   ],
   "source": [
    "# First, check that the text file is where we expect it to be\n",
    "%ls -l ../../Data/Moby-Dick.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#start the SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define an RDD that will read the file\n",
    "Note that, as execution is Lazy, this does not necessarily mean that actual reading of the file content has occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 ms, sys: 1.41 ms, total: 2.81 ms\n",
      "Wall time: 546 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(u'../../Data/Moby-Dick.txt')\n",
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Define the code for counting the words\n",
    "* split line by spaces.\n",
    "* map `word` to `(word,1)`\n",
    "* count the number of occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.93 ms, sys: 2.12 ms, total: 10.1 ms\n",
      "Wall time: 17.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .filter(lambda x: x!='')\\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "type(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Have a look a the execution plan\n",
    "Note that the earliest node in the dependency graph is the file `../../Data/Moby-Dick.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[31] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[30] at mapPartitions at PythonRDD.scala:374 []\n",
      " |  ShuffledRDD[29] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(2) PairwiseRDD[28] at reduceByKey at <timed exec>:1 []\n",
      "    |  PythonRDD[27] at reduceByKey at <timed exec>:1 []\n",
      "    |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "    |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print counts.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Count!\n",
    "Finally we count the number of times each word has occured.\n",
    "Now, finally, the Lazy execution model finally performs some actual work, which takes a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count=33301.000000, sum=212113.000000, mean=6.369568\n",
      "CPU times: user 11.4 ms, sys: 3.71 ms, total: 15.1 ms\n",
      "Wall time: 521 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda (w,i): i).reduce(lambda x,y:x+y)\n",
    "print 'Count=%f, sum=%f, mean=%f'%(Count,Sum,float(Sum)/Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the most common words\n",
    "* `counts`: RDD with 33301 pairs of the form `(word,count)`. \n",
    "* Find the 2 most frequent words. \n",
    "* **Method1:** `collect` and sort on head node.\n",
    "* **Method2:** Pure Spark, `collect` only at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Method1: `collect` and sort on head node \n",
    "### Collect the RDD into the driver node\n",
    "* Collect can take significant time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "CPU times: user 21.1 ms, sys: 3.35 ms, total: 24.5 ms\n",
      "Wall time: 90.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C=counts.collect()\n",
    "print type(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sort \n",
    "* RDD collected into list in driver node.\n",
    "* No longer using spark parallelism.\n",
    "* Sort in python\n",
    "* will nor scale to very large documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words\n",
      "to:\t4440\n",
      "a:\t4477\n",
      "and:\t5883\n",
      "of:\t6481\n",
      "the:\t13609\n",
      "\n",
      "Least common words\n",
      "funereal:\t1\n",
      "unscientific:\t1\n",
      "lime-stone,:\t1\n",
      "shouted,:\t1\n",
      "pitch-pot,:\t1\n"
     ]
    }
   ],
   "source": [
    "C.sort(key=lambda x:x[1])\n",
    "print 'most common words\\n','\\n'.join(['%s:\\t%d'%c for c in C[-5:]])\n",
    "print '\\nLeast common words\\n','\\n'.join(['%s:\\t%d'%c for c in C[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Compute the mean number of occurances per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count2=33301.000000, sum2=212113.000000, mean2=6.369568\n"
     ]
    }
   ],
   "source": [
    "Count2=len(C)\n",
    "Sum2=sum([i for w,i in C])\n",
    "print 'count2=%f, sum2=%f, mean2=%f'%(Count2,Sum2,float(Sum2)/Count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Method2:** Pure Spark, `collect` only at the end.\n",
    "* Collect into the head node only the more frquent words.\n",
    "* Requires multiple **stages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 1** split, clean and map to `(word,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 µs, sys: 12 µs, total: 64 µs\n",
      "Wall time: 62 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD=text_file.flatMap(lambda x: x.split(' '))\\\n",
    "    .filter(lambda x: x!='')\\\n",
    "    .map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 2** Count occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 2.5 ms, total: 10.5 ms\n",
      "Wall time: 14.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD1=RDD.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 3** Reverse `(word,count)` to `(count,word)` and sort by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 4 µs, total: 21 µs\n",
      "Wall time: 23.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD2=RDD1.map(lambda (c,v):(v,c))\n",
    "RDD3=RDD2.sortByKey(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Full execution plan\n",
    "\n",
    "We now have a complete plan to compute the most common words in the text. Nothing has been executed yet! Not even one one bye has been read from the file `Moby-Dick.txt` !\n",
    "\n",
    "For more on execution plans and lineage see [jace Klaskowski's blog](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-lineage.html#toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD3:\n",
      "(2) PythonRDD[22] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:374 []\n",
      " |  ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(2) PairwiseRDD[16] at sortByKey at <timed exec>:1 []\n",
      "    |  PythonRDD[15] at sortByKey at <timed exec>:1 []\n",
      "    |  MapPartitionsRDD[12] at mapPartitions at PythonRDD.scala:374 []\n",
      "    |  ShuffledRDD[11] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      "    +-(2) PairwiseRDD[10] at reduceByKey at <timed exec>:1 []\n",
      "       |  PythonRDD[9] at reduceByKey at <timed exec>:1 []\n",
      "       |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "       |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print 'RDD3:'\n",
    "print RDD3.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 4** Take the top 5 words. only **now** the plan is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words\n",
      "13609:\tthe\n",
      "6481:\tof\n",
      "5883:\tand\n",
      "4477:\ta\n",
      "4440:\tto\n",
      "CPU times: user 4.96 ms, sys: 2.17 ms, total: 7.13 ms\n",
      "Wall time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C=RDD3.take(5)\n",
    "print 'most common words\\n','\\n'.join(['%d:\\t%s'%c for c in C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "A `k`-mer is a sequence of `k` consecutive words. \n",
    "\n",
    "For example, the `3`-mers in the line `you are my sunshine my only sunsine` are\n",
    "\n",
    "* `you are my`\n",
    "* `are my sunshine`\n",
    "* `my sunshine my`\n",
    "* `sunshine my only`\n",
    "* `my only sunsine`\n",
    "\n",
    "For the sake of simplicity we consider only the `k`-mers that appear in a single line. In other words, we ignore `k`-mers that span more than one line.\n",
    "\n",
    "Write a function, using spark all the way to the end, to find to top 10 `k`-mers in a given text for a given `k`.\n",
    "\n",
    "Specifically write functions with the following signatures:\n",
    "```python\n",
    "def map_kmers(text,k):\n",
    "    \\\\ text: an RDD of text lines. Lines contain only lower-case letters and spaces. Spaces should be ignored.\n",
    "    \\\\ k: length of `k`-mers\n",
    "    return  singles\n",
    "    \\\\ singles: an RDD of pairs of the form (tuple of k words,1)\n",
    "def count_kmers(singles):\n",
    "    \\\\ singles: as above\n",
    "    return counts\n",
    "    \\\\ count: RDD of the form: (tuple of k words, number of occurances)\n",
    "def sort_counts(count):\n",
    "    \\\\ count: as above\n",
    "    return sorted_counts\n",
    "    \\\\ sorted_counts: RDD of the form (number of occurances, tuple of k words) sorted in decreasing number of occurances.\n",
    "```\n",
    "Combining these operations as follows:\n",
    "```python \n",
    "k=3\n",
    "singles=map_kmers(text,k)\n",
    "count=count_kmers(singles)\n",
    "sorted_counts=sort_counts(count)\n",
    "C=sorted_counts.take(5)\n",
    "print 'most common %d-mers\\n'%k,'\\n'.join(['%d:\\t%s'%c for c in C])\n",
    "```\n",
    "\n",
    "Should pring the 5 most common 3-mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
