{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize spark\n",
    "Here we initialize spark to use 3 of the 4 cores on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Word Count\n",
    "Counting the number of occurances of words in a text is one of the most popular first eercises when learning Map-Reduce Programming. It is the equivalent to `Hello World!` in regular programming.\n",
    "\n",
    "We will do it two way, a simpler way where sorting is done after the RDD is collected, and a more sparky way, where the sorting is also done using an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 yoavfreund  503  1237539 Jan  4 13:20 ../../Distributed Computation/notebooks/Moby-Dick.txt\r\n"
     ]
    }
   ],
   "source": [
    "# First, check that the text file is where we expect it to be\n",
    "filename='../../Distributed\\ Computation/notebooks/Moby-Dick.txt'\n",
    "%ls -l $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the text file into an RDD\n",
    "Note that, as execution is Lazy, this does not necessarily mean that actual reading of the file content has occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 ms, sys: 1.59 ms, total: 2.99 ms\n",
      "Wall time: 537 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(filename)\n",
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the words\n",
    "Next, we count the number of words that occured in the text. Again, this is only setting the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "CPU times: user 9.29 ms, sys: 3.58 ms, total: 12.9 ms\n",
      "Wall time: 178 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "print type(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look a the execution plan\n",
    "Note that the earliest node in the dependency graph is the file `../../Data/Moby-Dick.txt`. It is possible that that even the first element in that file has not yet been read!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[6] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:374 []\n",
      " |  ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(2) PairwiseRDD[3] at reduceByKey at <timed exec>:1 []\n",
      "    |  PythonRDD[2] at reduceByKey at <timed exec>:1 []\n",
      "    |  ../../Distributed\\ Computation/notebooks/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "    |  ../../Distributed\\ Computation/notebooks/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print counts.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count!\n",
    "Finally we count the number of times each word has occured.\n",
    "Note that this cell, finaally, the Lazy execution model finally performs some actual work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=33302.000000, sum=216169.000000, mean=6.491172\n",
      "CPU times: user 11.1 ms, sys: 4.02 ms, total: 15.1 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda (w,i): i).reduce(lambda x,y:x+y)\n",
    "print 'count=%f, sum=%f, mean=%f'%(Count,Sum,float(Sum)/Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the `Sum` RDD into the driver node\n",
    "This also takes significant work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 ms, sys: 6.31 ms, total: 27.9 ms\n",
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C=counts.collect()\n",
    "type(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort \n",
    "Now that we have collected the Sum RDD into the driver node, we no longer rely on Spark. The following two cells\n",
    "are simple python commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words [(u'I', 1724), (u'his', 2415), (u'that', 2680), (u'in', 3824), (u'', 4056), (u'to', 4440), (u'a', 4477), (u'and', 5883), (u'of', 6481), (u'the', 13609)]\n",
      "Least common words [(u'funereal', 1), (u'unscientific', 1), (u'lime-stone,', 1), (u'shouted,', 1), (u'pitch-pot,', 1), (u'cod-liver', 1), (u'prices', 1), (u'prefix', 1), (u'slew.', 1), (u'too?\"', 1)]\n"
     ]
    }
   ],
   "source": [
    "C.sort(key=lambda x:x[1])\n",
    "print 'most common words',C[-10:]\n",
    "print 'Least common words',C[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the mean number of occurances per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count2=33302.000000, sum2=216169.000000, mean2=6.491172\n"
     ]
    }
   ],
   "source": [
    "Count2=len(C)\n",
    "Sum2=sum([i for w,i in C])\n",
    "print 'count2=%f, sum2=%f, mean2=%f'%(Count2,Sum2,float(Sum2)/Count2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count in Pure Spark\n",
    "We now show how to perform word count, including sorting, using RDDs, returning to the driver node just the top 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 µs, sys: 13 µs, total: 48 µs\n",
      "Wall time: 45.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD=text_file.flatMap(lambda x: x.split(' '))\\\n",
    "    .filter(lambda x: x!='')\\\n",
    "    .map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.8 ms, sys: 2.43 ms, total: 10.2 ms\n",
      "Wall time: 16.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD1=RDD.reduceByKey(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 µs, sys: 5 µs, total: 25 µs\n",
      "Wall time: 27.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD2=RDD1.map(lambda (c,v):(v,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.2 ms, sys: 5.14 ms, total: 24.4 ms\n",
      "Wall time: 510 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RDD3=RDD2.sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[19] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:374 []\n",
      " |  ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      " +-(2) PairwiseRDD[16] at sortByKey at <timed exec>:1 []\n",
      "    |  PythonRDD[15] at sortByKey at <timed exec>:1 []\n",
      "    |  MapPartitionsRDD[12] at mapPartitions at PythonRDD.scala:374 []\n",
      "    |  ShuffledRDD[11] at partitionBy at NativeMethodAccessorImpl.java:-2 []\n",
      "    +-(2) PairwiseRDD[10] at reduceByKey at <timed exec>:1 []\n",
      "       |  PythonRDD[9] at reduceByKey at <timed exec>:1 []\n",
      "       |  ../../Distributed\\ Computation/notebooks/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2 []\n",
      "       |  ../../Distributed\\ Computation/notebooks/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:-2 []\n"
     ]
    }
   ],
   "source": [
    "print RDD3.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.53 ms, sys: 4.37 ms, total: 9.9 ms\n",
      "Wall time: 168 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(13609, u'the'),\n",
       " (6481, u'of'),\n",
       " (5883, u'and'),\n",
       " (4477, u'a'),\n",
       " (4440, u'to'),\n",
       " (3824, u'in'),\n",
       " (2680, u'that'),\n",
       " (2415, u'his'),\n",
       " (1724, u'I'),\n",
       " (1646, u'with')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "RDD3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
