{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from DistributedBoosting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0  0  0  1  1  0  0  0  0  \n",
      "0  0  1  0  0  0  0  0  0  0  \n",
      "0  0  0  0  1  1  1  1  1  0  \n",
      "0  0  0  0  1  1  1  1  1  0  \n",
      "0  0  0  0  1  1  0  1  1  0  \n",
      "0  0  0  0  1  1  1  1  1  0  \n",
      "0  0  0  0  1  1  1  1  0  0  \n",
      "0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  0  0  0  1  1  0  \n",
      "0  1  0  0  0  0  0  0  0  0  \n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc=SparkContext()\n",
    "\n",
    "from numpy.random import rand\n",
    "p=0.9\n",
    "data=[]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if np.abs(i-4)<3 and np.abs(j-6)<3:\n",
    "            y=2*(rand()<p)-1\n",
    "        else:\n",
    "            y=2*(rand()>p)-1\n",
    "        print(\"%1.0f \"%((1+y)/2), end=' ')\n",
    "        data.append(LabeledPoint(y,[i,j]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRDD=sc.parallelize(data,numSlices=2)\n",
    "dataRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load DistributedBoosting.py\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"A simple service class to log run time and pretty-print it.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.T=[]\n",
    "    def stamp(self,name):\n",
    "        self.T.append((name,time()))\n",
    "    def str(self):\n",
    "        T=self.T\n",
    "        return '\\n'.join(['%6.2f : %s'%(T[i+1][1]-T[i][1],T[i+1][0]) for i in range(len(T)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Globals for communication within the head node\n",
    "global T,partition_no,iteration, proposals, PS\n",
    "T=Timer()\n",
    "partition_no=0\n",
    "iteration=0                     # Boosting iteration\n",
    "proposals=[]                    # proposed splits for each feature\n",
    "PS=[None]                       # List of RDDs that hold the state of the booster. \n",
    "                                # (only the weight vector changes from iteration to iteration)\n",
    "\n",
    "### Broadcast variables need to be in the global namespace \n",
    "###  so that the partition methods can find them\n",
    "global global_feature_no, Splits_Table, Strong_Classifier,global_best_splitter\n",
    "\n",
    "Splits_Table=None\n",
    "global_feature_no=None\n",
    "global_best_splitter=None\n",
    "Strong_Classifier=[]            # Combined weak classifiers\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Partition fundctions\n",
    "def Prepare_partition_data_structure(A):\n",
    "\n",
    "    global global_feature_no\n",
    "    feature_no = global_feature_no.value\n",
    "    \n",
    "    rows=len(A[1])\n",
    "\n",
    "    columns=np.empty([feature_no,rows])\n",
    "    columns[:]=np.NaN\n",
    "    print('Prepare_partition_data_structure',feature_no,np.shape(columns))\n",
    "    \n",
    "    labels=np.empty(rows)\n",
    "    labels[:]=np.NaN\n",
    "\n",
    "    for j in range(rows):\n",
    "        LP=A[1][j]\n",
    "        labels[j]=LP.label\n",
    "        for i in range(feature_no):\n",
    "            columns[i,j]=LP.features[i]\n",
    "    return {'index':A[0],\\\n",
    "            'labels':labels,\\\n",
    "            'weights':np.ones(len(labels)),\\\n",
    "            'feature_values':columns}\n",
    "\n",
    "def Add_weak_learner_matrix(A):\n",
    "    \"\"\" This procedure adds to each partition the matrix that will be \n",
    "        used to efficiently find the best weak classifier \"\"\"\n",
    "\n",
    "    global global_feature_no\n",
    "    feature_no=global_feature_no.value\n",
    "\n",
    "    index=A['index']%feature_no\n",
    "    SP=Splits_Table.value[index]\n",
    "\n",
    "    Col=A['feature_values'][index,:]\n",
    "\n",
    "    ### The matrix M is organized as follows: \n",
    "    # * There are as many rows as there are thresholds in SP (last one is inf)\n",
    "    # * There are as many columns as there are examples in this partition.\n",
    "    # For threshold i, the i'th rw of M is +1 if Col is smaller than the trehold SP[i] and -1 otherwise\n",
    "\n",
    "    M=np.empty([len(SP),len(Col)])\n",
    "    M[:]=np.NaN\n",
    "\n",
    "    for i in range(len(SP)):\n",
    "        M[i,:]=2*(Col<SP[i])-1\n",
    "\n",
    "    A['M']=M # add M matrix to the data structure.\n",
    "    return A\n",
    "\n",
    "\n",
    "def Find_weak(A):\n",
    "    \"\"\"Find the best split for a single feature on a single partition\"\"\"\n",
    "\n",
    "    global global_feature_no\n",
    "\n",
    "    feature_no=global_feature_no.value\n",
    "\n",
    "    index=A['index']%feature_no\n",
    "    SP=Splits_Table.value[index]\n",
    "\n",
    "    M=A['M']\n",
    "    weights=A['weights']\n",
    "    weighted_Labels=weights*A['labels']\n",
    "    SS=np.dot(M,weighted_Labels)/np.sum(weights)\n",
    "    i_max=np.argmax(np.abs(SS))\n",
    "    answer={'Feature_index':A['index']%feature_no,\\\n",
    "            'Threshold_index':i_max,\\\n",
    "            'Threshold':SP[i_max],\\\n",
    "            'Correlation':SS[i_max],\\\n",
    "            'SS':SS}\n",
    "    return answer\n",
    "\n",
    "# update weights. New splitter is shipped to partition as one of the referenced\n",
    "# Variables\n",
    "\n",
    "def update_weights(A):\n",
    "    \"\"\"Update the weights of the exammples belonging to this \n",
    "    partition according to the new splitter\"\"\"\n",
    "    global global_best_splitter\n",
    "    best_splitter=global_best_splitter.value\n",
    "    \n",
    "    F_index=best_splitter['Feature_index']\n",
    "    Thr=best_splitter['Threshold']\n",
    "    alpha=best_splitter['alpha']\n",
    "    y_hat=2*(A['feature_values'][F_index,:]<Thr)-1\n",
    "    y=A['labels']\n",
    "    weights=A['weights']*exp(-alpha*y_hat*y)\n",
    "    weights /= sum(weights)\n",
    "    A['weights']=weights\n",
    "    return A\n",
    "\n",
    "def calc_scores(Strong_Classifier,Columns,Lbl):\n",
    "    \n",
    "    Scores=np.zeros(len(Lbl))\n",
    "\n",
    "    for h in Strong_Classifier:\n",
    "        index=h['Feature_index']\n",
    "        Thr=h['Threshold']\n",
    "        alpha=h['alpha']\n",
    "        y_hat=2*(Columns[index,:]<Thr)-1\n",
    "        Scores += alpha*y_hat*Lbl\n",
    "    return Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(Splits)? (<ipython-input-10-9dd9b0b7ff3b>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-9dd9b0b7ff3b>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    print Splits\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(Splits)?\n"
     ]
    }
   ],
   "source": [
    "###### Head-Node functions\n",
    "def find_splits(GR,number_of_bins=5,debug=False):\n",
    "    \"\"\"Compute the split points for each feature to create number_of_bins bins\"\"\"\n",
    "    global global_feature_no\n",
    "    feature_no=global_feature_no.value\n",
    "\n",
    "    def find_split_points(A):\n",
    "\n",
    "        j=A['index'] % feature_no\n",
    "        S=np.sort(A['feature_values'][j,:])\n",
    "        L=len(S) \n",
    "        step=int(ceil(float(L)/number_of_bins))\n",
    "        return (j,S[range(step,L,step)])\n",
    "\n",
    "    global partition_no\n",
    "    #Splits=GR.map(find_split_points).collect()\n",
    "    Splits=[]\n",
    "    for A in GR.collect():\n",
    "        Splits.append(find_split_points(A))\n",
    "    print Splits\n",
    "    max_no=np.array([np.finfo(float).max])\n",
    "\n",
    "    # Average the split points across the partitions corresponding to the same feature.\n",
    "    Splits1=[]\n",
    "    for i in range(feature_no):\n",
    "        S=Splits[i][1]\n",
    "        if debug:\n",
    "            print 'no. ',i,' = ',Splits[i]\n",
    "        n=1  # number of copies (for averaging)\n",
    "        j=i+feature_no\n",
    "        while j<partition_no:\n",
    "            if debug:\n",
    "                print 'j=',j\n",
    "            S+=Splits[j][1]\n",
    "            if debug:\n",
    "                print 'no. ',j,' = ',Splits[j]\n",
    "            n+=1.0\n",
    "            j+=feature_no\n",
    "        Splits1.append(np.concatenate([S/n,max_no]))\n",
    "        if debug:\n",
    "            print n\n",
    "            print Splits1[i]\n",
    "            print '='*60\n",
    "\n",
    "    return Splits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(sc,Data):\n",
    "    \"\"\" Given an RDD with labeled Points, create the RDD of data structures used for boosting\n",
    "    \"\"\"\n",
    "\n",
    "    global proposals,Strong_Classifier, global_feature_no, Splits_Table\n",
    "    global Strong_Classifier,global_best_splitter\n",
    "\n",
    "    T=Timer()\n",
    "    T.stamp('Started')\n",
    "\n",
    "    X=Data.first()\n",
    "    feature_no=len(X.features)\n",
    "    global_feature_no=sc.broadcast(feature_no)\n",
    "    partition_no=Data.getNumPartitions()\n",
    "    if partition_no != feature_no:\n",
    "        Data=Data.repartition(feature_no).cache()\n",
    "    print('number of features=',feature_no,'number of partitions=',Data.getNumPartitions())\n",
    "\n",
    "    # Split data into training and test\n",
    "    (trainingData,testData)=Data.randomSplit([0.7,0.3])\n",
    "    print('Sizes: Data1=%d, trainingData=%d, testData=%d'%      (Data.count(),trainingData.cache().count(),testData.cache().count()))\n",
    "    T.stamp('Split into train and test')\n",
    "    # Glom each partition into a local array\n",
    "    G=trainingData.glom()\n",
    "    GTest=testData.glom()  \n",
    "    T.stamp('glom')\n",
    "\n",
    "    # Add an index to each partition to identify it.\n",
    "    def f(splitIndex, iterator): yield splitIndex,next(iterator)\n",
    "    GI=G.mapPartitionsWithIndex(f)\n",
    "    GTI=GTest.mapPartitionsWithIndex(f)\n",
    "    T.stamp('add partition index')\n",
    "\n",
    "    # Prepare the data structure for each partition.\n",
    "    GR=GI.map(Prepare_partition_data_structure)\n",
    "    #for A in GI.collect():\n",
    "    #    Prepare_partition_data_structure(A)\n",
    "        \n",
    "    print('number of elements in GR=', GR.cache().count())\n",
    "    T.stamp('Prepare_partition_data_structure')\n",
    "\n",
    "    Splits=find_splits(GR)\n",
    "    print('Split points=',Splits)\n",
    "    T.stamp('Compute Split points')\n",
    "\n",
    "    #broadcast split points\n",
    "    global Splits_Table\n",
    "    Splits_Table=sc.broadcast(Splits)\n",
    "    T.stamp('Broadcast split points')\n",
    "\n",
    "    # Create matrix for each partition to make finding the weak rules correlation a matter of taking a matrix product\n",
    "\n",
    "    global PS, iteration\n",
    "    iteration=0\n",
    "    PS[0]=GR.map(Add_weak_learner_matrix)\n",
    "    print('number of partitions in PS=',PS[0].cache().count())\n",
    "    T.stamp('Add_weak_learner_matrix')\n",
    "\n",
    "    return PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(PS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init(sc,dataRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS[0].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_iteration(k=1):\n",
    "    \"\"\" perform k boosting iterations \"\"\"\n",
    "    global global_Strong_Classifier,iteration,global_best_splitter\n",
    "\n",
    "    for i in range(iteration,iteration+k):\n",
    "        T.stamp('Start main loop %d'%i)\n",
    "\n",
    "        prop=PS[i].map(Find_weak).collect()\n",
    "        proposals.append(prop)\n",
    "        corrs=[p['Correlation'] for p in prop]\n",
    "        best_splitter_index=np.argmax(np.abs(corrs))\n",
    "        best_splitter = prop[best_splitter_index]\n",
    "        corr=best_splitter['Correlation']\n",
    "        best_splitter['alpha']=0.5*np.log((1+corr)/(1-corr))\n",
    "        global_best_splitter=sc.broadcast(best_splitter)\n",
    "        Strong_Classifier.append(best_splitter)\n",
    "        global_Strong_Classifier=sc.broadcast(Strong_Classifier)\n",
    "        T.stamp('found best splitter %d'%i)\n",
    "        newPS=PS[i].map(update_weights).cache()\n",
    "        newPS.count()\n",
    "        #for A in PS[i].collect():\n",
    "        #    update_weights(A)\n",
    "        PS.append(newPS)\n",
    "        T.stamp('Updated Weights %d'%i)\n",
    "    iteration+=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
