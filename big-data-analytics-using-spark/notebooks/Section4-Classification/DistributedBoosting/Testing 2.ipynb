{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['time']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from DistributedBoosting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0  0  0  0  0  0  0  0  1  \n",
      "0  0  0  0  0  0  0  0  0  1  \n",
      "0  1  0  0  0  1  1  0  1  0  \n",
      "0  0  0  0  1  1  1  1  1  0  \n",
      "0  0  0  0  1  1  1  1  1  1  \n",
      "1  0  0  0  1  1  0  1  1  1  \n",
      "0  0  0  0  1  1  1  1  1  1  \n",
      "0  0  0  0  0  0  0  0  0  0  \n",
      "0  0  0  0  1  0  0  0  0  0  \n",
      "0  0  0  0  0  1  0  0  0  0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import rand\n",
    "p=0.9\n",
    "data=[]\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if np.abs(i-4)<3 and np.abs(j-6)<3:\n",
    "            y=2*(rand()<p)-1\n",
    "        else:\n",
    "            y=2*(rand()>p)-1\n",
    "        print(\"%1.0f \"%((1+y)/2), end=' ')\n",
    "        data.append(LabeledPoint(y,[i,j]))\n",
    "    print()\n",
    "\n",
    "dataRDD=sc.parallelize(data,numSlices=2)\n",
    "dataRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %load DistributedBoosting.py\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"A simple service class to log run time and pretty-print it.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.T=[]\n",
    "    def stamp(self,name):\n",
    "        self.T.append((name,time()))\n",
    "    def str(self):\n",
    "        T=self.T\n",
    "        return '\\n'.join(['%6.2f : %s'%(T[i+1][1]-T[i][1],T[i+1][0]) for i in range(len(T)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###### Globals\n",
    "global T,iteration,GR,proposals,Strong_Classifier, feature_no, partition_no, Splits_Table\n",
    "global Strong_Classifier,global_best_splitter,PS\n",
    "\n",
    "T=Timer()\n",
    "feature_no=None                 # Tracks processing time\n",
    "global_feature_no=None\n",
    "partition_no=0\n",
    "iteration=0                     # Boosting iteration\n",
    "PS=[None]                       # RDD that hold state of boosting process for each partition.\n",
    "proposals=[]                    # proposed splits for each feature\n",
    "Strong_Classifier=[]            # Combined weak classifiers\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##### Partition fundctions\n",
    "def Prepare_partition_data_structure(A):\n",
    "\n",
    "    rows=len(A[1])\n",
    "\n",
    "    columns=np.empty([feature_no,rows])\n",
    "    columns[:]=np.NaN\n",
    "    print('Prepare_partition_data_structure',feature_no,np.shape(columns))\n",
    "    \n",
    "    labels=np.empty(rows)\n",
    "    labels[:]=np.NaN\n",
    "\n",
    "    for j in range(rows):\n",
    "        LP=A[1][j]\n",
    "        labels[j]=LP.label\n",
    "        for i in range(feature_no):\n",
    "            columns[i,j]=LP.features[i]\n",
    "    return {'index':A[0],\\\n",
    "            'labels':labels,\\\n",
    "            'weights':np.ones(len(labels)),\\\n",
    "            'feature_values':columns}\n",
    "\n",
    "def Add_weak_learner_matrix(A):\n",
    "    \"\"\" This procedure adds to each partition the matrix that will be \n",
    "        used to efficiently find the best weak classifier \"\"\"\n",
    "\n",
    "    try:\n",
    "        feature_no\n",
    "    except:\n",
    "        feature_no=global_feature_no.value\n",
    "\n",
    "    index=A['index']%feature_no\n",
    "    SP=Splits_Table.value[index]\n",
    "\n",
    "    Col=A['feature_values'][index,:]\n",
    "\n",
    "    ### The matrix M is organized as follows: \n",
    "    # * There are as many rows as there are thresholds in SP (last one is inf)\n",
    "    # * There are as many columns as there are examples in this partition.\n",
    "    # For threshold i, the i'th rw of M is +1 if Col is smaller than the trehold SP[i] and -1 otherwise\n",
    "\n",
    "    M=np.empty([len(SP),len(Col)])\n",
    "    M[:]=np.NaN\n",
    "\n",
    "    for i in range(len(SP)):\n",
    "        M[i,:]=2*(Col<SP[i])-1\n",
    "\n",
    "    A['M']=M # add M matrix to the data structure.\n",
    "    return A\n",
    "\n",
    "\n",
    "def Find_weak(A):\n",
    "    \"\"\"Find the best split for a single feature on a single partition\"\"\"\n",
    "\n",
    "    try:\n",
    "        feature_no\n",
    "    except:\n",
    "        feature_no=global_feature_no.value\n",
    "\n",
    "    index=A['index']%feature_no\n",
    "    SP=Splits_Table.value[index]\n",
    "\n",
    "    M=A['M']\n",
    "    weights=A['weights']\n",
    "    weighted_Labels=weights*A['labels']\n",
    "    SS=np.dot(M,weighted_Labels)/np.sum(weights)\n",
    "    i_max=np.argmax(np.abs(SS))\n",
    "    answer={'Feature_index':A['index']%feature_no,\\\n",
    "            'Threshold_index':i_max,\\\n",
    "            'Threshold':SP[i_max],\\\n",
    "            'Correlation':SS[i_max],\\\n",
    "            'SS':SS}\n",
    "    return answer\n",
    "\n",
    "# update weights. New splitter is shipped to partition as one of the referenced\n",
    "# Variables\n",
    "\n",
    "def update_weights(A):\n",
    "    \"\"\"Update the weights of the exammples belonging to this \n",
    "    partition according to the new splitter\"\"\"\n",
    "    best_splitter=global_best_splitter\n",
    "    F_index=best_splitter['Feature_index']\n",
    "    Thr=best_splitter['Threshold']\n",
    "    alpha=best_splitter['alpha']\n",
    "    y_hat=2*(A['feature_values'][F_index,:]<Thr)-1\n",
    "    y=A['labels']\n",
    "    weights=A['weights']*exp(-alpha*y_hat*y)\n",
    "    weights /= sum(weights)\n",
    "    A['weights']=weights\n",
    "    return A\n",
    "\n",
    "def calc_scores(Strong_Classifier,Columns,Lbl):\n",
    "    \n",
    "    Scores=np.zeros(len(Lbl))\n",
    "\n",
    "    for h in Strong_Classifier:\n",
    "        index=h['Feature_index']\n",
    "        Thr=h['Threshold']\n",
    "        alpha=h['alpha']\n",
    "        y_hat=2*(Columns[index,:]<Thr)-1\n",
    "        Scores += alpha*y_hat*Lbl\n",
    "    return Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###### Head-Node functions\n",
    "def init(sc,Data):\n",
    "    \"\"\" Given an RDD with labeled Points, create the RDD of data structures used for boosting\n",
    "    \"\"\"\n",
    "\n",
    "    global T,iteration,GR,proposals,Strong_Classifier, feature_no, partition_no, Splits_Table\n",
    "    global Strong_Classifier,global_best_splitter\n",
    "\n",
    "    T=Timer()\n",
    "    T.stamp('Started')\n",
    "\n",
    "    X=Data.first()\n",
    "    feature_no=len(X.features)\n",
    "#    print 'global_feature_no = sc.broadcast(feature_no)',feature_no\n",
    "    partition_no=Data.getNumPartitions()\n",
    "    if partition_no != feature_no:\n",
    "        Data=Data.repartition(feature_no).cache()\n",
    "    print('number of features=',feature_no,'number of partitions=',Data.getNumPartitions())\n",
    "\n",
    "    # Split data into training and test\n",
    "    (trainingData,testData)=Data.randomSplit([0.7,0.3])\n",
    "    print('Sizes: Data1=%d, trainingData=%d, testData=%d'%      (Data.count(),trainingData.cache().count(),testData.cache().count()))\n",
    "    T.stamp('Split into train and test')\n",
    "    # Glom each partition into a local array\n",
    "    G=trainingData.glom()\n",
    "    GTest=testData.glom()  \n",
    "    T.stamp('glom')\n",
    "\n",
    "    # Add an index to each partition to identify it.\n",
    "    def f(splitIndex, iterator): yield splitIndex,next(iterator)\n",
    "    GI=G.mapPartitionsWithIndex(f)\n",
    "    GTI=GTest.mapPartitionsWithIndex(f)\n",
    "    T.stamp('add partition index')\n",
    "\n",
    "    return GI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features= 2 number of partitions= 2\n",
      "Sizes: Data1=100, trainingData=69, testData=31\n"
     ]
    }
   ],
   "source": [
    "GI=init(sc,dataRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [LabeledPoint(-1.0, [0.0,0.0]),\n",
       "   LabeledPoint(-1.0, [0.0,2.0]),\n",
       "   LabeledPoint(-1.0, [0.0,3.0]),\n",
       "   LabeledPoint(-1.0, [0.0,4.0]),\n",
       "   LabeledPoint(-1.0, [0.0,7.0]),\n",
       "   LabeledPoint(-1.0, [0.0,8.0]),\n",
       "   LabeledPoint(1.0, [0.0,9.0]),\n",
       "   LabeledPoint(-1.0, [1.0,0.0]),\n",
       "   LabeledPoint(-1.0, [1.0,2.0]),\n",
       "   LabeledPoint(-1.0, [1.0,3.0]),\n",
       "   LabeledPoint(-1.0, [1.0,4.0]),\n",
       "   LabeledPoint(-1.0, [1.0,6.0]),\n",
       "   LabeledPoint(-1.0, [1.0,7.0]),\n",
       "   LabeledPoint(-1.0, [1.0,8.0]),\n",
       "   LabeledPoint(1.0, [1.0,9.0]),\n",
       "   LabeledPoint(-1.0, [2.0,0.0]),\n",
       "   LabeledPoint(1.0, [2.0,1.0]),\n",
       "   LabeledPoint(-1.0, [2.0,2.0]),\n",
       "   LabeledPoint(-1.0, [2.0,3.0]),\n",
       "   LabeledPoint(1.0, [2.0,5.0]),\n",
       "   LabeledPoint(1.0, [2.0,6.0]),\n",
       "   LabeledPoint(1.0, [2.0,8.0]),\n",
       "   LabeledPoint(-1.0, [3.0,1.0]),\n",
       "   LabeledPoint(-1.0, [3.0,2.0]),\n",
       "   LabeledPoint(1.0, [3.0,4.0]),\n",
       "   LabeledPoint(1.0, [3.0,6.0]),\n",
       "   LabeledPoint(1.0, [3.0,7.0]),\n",
       "   LabeledPoint(1.0, [3.0,8.0]),\n",
       "   LabeledPoint(-1.0, [4.0,0.0]),\n",
       "   LabeledPoint(-1.0, [4.0,3.0]),\n",
       "   LabeledPoint(1.0, [4.0,4.0]),\n",
       "   LabeledPoint(1.0, [4.0,6.0]),\n",
       "   LabeledPoint(1.0, [4.0,9.0])]),\n",
       " (1,\n",
       "  [LabeledPoint(1.0, [5.0,0.0]),\n",
       "   LabeledPoint(-1.0, [5.0,1.0]),\n",
       "   LabeledPoint(-1.0, [5.0,2.0]),\n",
       "   LabeledPoint(-1.0, [5.0,3.0]),\n",
       "   LabeledPoint(1.0, [5.0,4.0]),\n",
       "   LabeledPoint(-1.0, [5.0,6.0]),\n",
       "   LabeledPoint(1.0, [5.0,8.0]),\n",
       "   LabeledPoint(1.0, [5.0,9.0]),\n",
       "   LabeledPoint(-1.0, [6.0,0.0]),\n",
       "   LabeledPoint(-1.0, [6.0,1.0]),\n",
       "   LabeledPoint(-1.0, [6.0,2.0]),\n",
       "   LabeledPoint(-1.0, [6.0,3.0]),\n",
       "   LabeledPoint(1.0, [6.0,4.0]),\n",
       "   LabeledPoint(1.0, [6.0,6.0]),\n",
       "   LabeledPoint(1.0, [6.0,8.0]),\n",
       "   LabeledPoint(1.0, [6.0,9.0]),\n",
       "   LabeledPoint(-1.0, [7.0,0.0]),\n",
       "   LabeledPoint(-1.0, [7.0,3.0]),\n",
       "   LabeledPoint(-1.0, [7.0,4.0]),\n",
       "   LabeledPoint(-1.0, [7.0,5.0]),\n",
       "   LabeledPoint(-1.0, [7.0,7.0]),\n",
       "   LabeledPoint(-1.0, [7.0,8.0]),\n",
       "   LabeledPoint(-1.0, [7.0,9.0]),\n",
       "   LabeledPoint(-1.0, [8.0,0.0]),\n",
       "   LabeledPoint(-1.0, [8.0,3.0]),\n",
       "   LabeledPoint(1.0, [8.0,4.0]),\n",
       "   LabeledPoint(-1.0, [8.0,5.0]),\n",
       "   LabeledPoint(-1.0, [8.0,6.0]),\n",
       "   LabeledPoint(-1.0, [8.0,7.0]),\n",
       "   LabeledPoint(-1.0, [8.0,8.0]),\n",
       "   LabeledPoint(-1.0, [9.0,4.0]),\n",
       "   LabeledPoint(1.0, [9.0,5.0]),\n",
       "   LabeledPoint(-1.0, [9.0,6.0]),\n",
       "   LabeledPoint(-1.0, [9.0,7.0]),\n",
       "   LabeledPoint(-1.0, [9.0,8.0]),\n",
       "   LabeledPoint(-1.0, [9.0,9.0])])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GI.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-90779a94a670>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-90779a94a670>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    T.stamp('Compute Split points')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def init2(GI):\n",
    "    # Prepare the data structure for each partition.\n",
    "    GR=GI.map(Prepare_partition_data_structure)\n",
    "    print('number of elements in GR=', GR.cache().count())\n",
    "    T.stamp('Prepare_partition_data_structure')\n",
    "    return GR\n",
    "\n",
    "#compute the split points for each feature\n",
    "    Splits=find_splits(GR)\n",
    "    print('Split points=',Splits\n",
    "    T.stamp('Compute Split points')\n",
    "\n",
    "    #broadcast split points\n",
    "    global Splits_Table\n",
    "    Splits_Table=sc.broadcast(Splits)\n",
    "    T.stamp('Broadcast split points')\n",
    "\n",
    "    # Create matrix for each partition to make finding the weak rules correlation a matter of taking a matrix product\n",
    "\n",
    "    iteration=0\n",
    "    global PS\n",
    "    PS[0]=GR.map(Add_weak_learner_matrix)\n",
    "    print 'number of partitions in PS=',PS[0].cache().count()\n",
    "    T.stamp('Add_weak_learner_matrix')\n",
    "\n",
    "    return PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 9, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/srihariveeraraaghavan/CSE_255/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/notebooks/Section4-Classification/DistributedBoosting/DistributedBoosting.py\", line 42, in Prepare_partition_data_structure\n    columns=np.empty([feature_no,rows])\nTypeError: 'NoneType' object cannot be interpreted as an integer\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1109)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/srihariveeraraaghavan/CSE_255/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/notebooks/Section4-Classification/DistributedBoosting/DistributedBoosting.py\", line 42, in Prepare_partition_data_structure\n    columns=np.empty([feature_no,rows])\nTypeError: 'NoneType' object cannot be interpreted as an integer\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1109)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c6f8e14c783b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mGR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CSE_255/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/notebooks/Section4-Classification/DistributedBoosting/DistributedBoosting.py\u001b[0m in \u001b[0;36minit2\u001b[0;34m(GI)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# Prepare the data structure for each partition.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mGR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPrepare_partition_data_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of elements in GR='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prepare_partition_data_structure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \"\"\"\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \"\"\"\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 9, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/srihariveeraraaghavan/CSE_255/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/notebooks/Section4-Classification/DistributedBoosting/DistributedBoosting.py\", line 42, in Prepare_partition_data_structure\n    columns=np.empty([feature_no,rows])\nTypeError: 'NoneType' object cannot be interpreted as an integer\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1109)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/srihariveeraraaghavan/CSE_255/edX-Micro-Master-in-Data-Science/big-data-analytics-using-spark/notebooks/Section4-Classification/DistributedBoosting/DistributedBoosting.py\", line 42, in Prepare_partition_data_structure\n    columns=np.empty([feature_no,rows])\nTypeError: 'NoneType' object cannot be interpreted as an integer\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:378)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1109)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "GR=init2(GI)\n",
    "GR.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def boosting_iteration(k=1):\n",
    "    \"\"\" perform k boosting iterations \"\"\"\n",
    "    for i in range(iteration,iteration+k):\n",
    "        T.stamp('Start main loop %d'%i)\n",
    "\n",
    "        prop=PS[i].map(Find_weak).collect()\n",
    "        proposals.append(prop)\n",
    "        corrs=[p['Correlation'] for p in prop]\n",
    "        best_splitter_index=np.argmax(np.abs(corrs))\n",
    "        best_splitter = prop[best_splitter_index]\n",
    "        Strong_Classifier.append(best_splitter)\n",
    "        global global_Strong_Classifier\n",
    "        global_Strong_Classifier=sc.broadcast(Strong_Classifier)\n",
    "        T.stamp('found best splitter %d'%i)\n",
    "\n",
    "        corr=best_splitter['Correlation']\n",
    "        best_splitter['alpha']=0.5*np.log((1+corr)/(1-corr))\n",
    "        global global_best_splitter\n",
    "        global_best_splitter = sc.broadcast(best_splitter)\n",
    "        PS.append(PS[i].map(update_weights))\n",
    "        T.stamp('Updated Weights %d'%i)\n",
    "    iteration+=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_splits(GR,number_of_bins=10,debug=False):\n",
    "    \"\"\"Compute the split points for each feature to create number_of_bins bins\"\"\"\n",
    "    def find_split_points(A):\n",
    "\n",
    "        try:\n",
    "            feature_no\n",
    "        except:\n",
    "            feature_no=global_feature_no.value\n",
    "\n",
    "        j=A['index'] % feature_no\n",
    "        S=np.sort(A['feature_values'][j,:       ])\n",
    "        L=len(S) \n",
    "        step=L/number_of_bins+2*number_of_bins\n",
    "        return (j,S[range(0,L,step)])\n",
    "\n",
    "    global partition_no\n",
    "    Splits=GR.map(find_split_points).collect()\n",
    "    max_no=np.array([np.finfo(float).max])\n",
    "\n",
    "    # Average the split points across the partitions corresponding to the same feature.\n",
    "    Splits1=[]\n",
    "    for i in range(feature_no):\n",
    "        S=Splits[i][1]\n",
    "        if debug:\n",
    "            print 'no. ',i,' = ',Splits[i]\n",
    "        n=1  # number of copies (for averaging)\n",
    "        j=i+feature_no\n",
    "        while j<partition_no:\n",
    "            if debug:\n",
    "                print 'j=',j\n",
    "            S+=Splits[j][1]\n",
    "            if debug:\n",
    "                print 'no. ',j,' = ',Splits[j]\n",
    "            n+=1.0\n",
    "            j+=feature_no\n",
    "        Splits1.append(np.concatenate([S/n,max_no]))\n",
    "        if debug:\n",
    "            print n\n",
    "            print Splits1[i]\n",
    "            print '='*60\n",
    "\n",
    "    return Splits1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
