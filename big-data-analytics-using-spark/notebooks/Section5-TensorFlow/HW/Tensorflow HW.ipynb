{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PA9: Neural Networks with Tensorflow\n",
    "\n",
    "In this assignment, you will:\n",
    "\n",
    "1. Implement neural networks as a powerful approach to supervised machine learning,\n",
    "2. Practice using state-of-the-art software tools and programming paradigms for machine learning,\n",
    "3. Investigate the impact of parameters to learning on neural network performance as evaluated on an empirical data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will learn use a well known dataset:\n",
    "\n",
    "[Higgs](https://archive.ics.uci.edu/ml/datasets/HIGGS). Some information regarding this dataset: The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes.\n",
    "\n",
    "For local testing you will use the sample dataset provided to you with this notebook.\n",
    "When submitting on EdX, your code will be evaluated on a much larger sample of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file format for each of the above data set is as follows:\n",
    "\n",
    "• The first row contains a comma-separated list of the names of the label and attributes\n",
    "\n",
    "• Each successive row represents a single instance\n",
    "\n",
    "• The first entry of each instance is the label to be learned, and all other entries (following the commas) are attribute values.\n",
    "\n",
    "• All attributes are numerical i.e. real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: \n",
    "\n",
    "Your goal is to complete the below function named train_nn that behaves as follows:\n",
    "\n",
    "1) It should take as input six parameters:\n",
    "    \n",
    "    a. The path to a file containing a data set (e.g., higgs_sample.csv)\n",
    "    \n",
    "    b. The number of neurons to use in the hidden layer\n",
    "    \n",
    "    c. The learning rate to use during backpropagation\n",
    "    \n",
    "    d. The number of iterations to use during training\n",
    "    \n",
    "    e. The percentage of instances to use for a training set\n",
    "    \n",
    "    f. A random seed as an integer\n",
    "    \n",
    "For example, if the call to train_nn looks like train_nn(higgs_sample.csv 20 0.001 1000 0.75 12345) which will create a neural network with 20 neurons in the hidden layer, train the network using a learning rate = 0.001 and 1000 iterations through higgs_sample.csv with a random seed of 12345, where 75% of the data will be used for training (and the remaining 25% will be used for testing)\n",
    "\n",
    "2) You should create a neural network in Tensorflow that will be learned from the training data. The key parameters to the architecture of the neural network are based on your inputted parameters and the size of your data set:\n",
    "    \n",
    "    a. The number of attributes in the input layer is the length of each instance’s\n",
    "    attribute list (which is the same for all instances)\n",
    "    \n",
    "    b. The number of neurons in a hidden layer will be inputted to the program as a\n",
    "    parameter. Each hidden neuron should use tf.sigmoid as its activation function.\n",
    "    \n",
    "    c. The number of output neurons will be 1 since it is a binary classification task, and that should use tf.sigmoid as its activation function\n",
    "    \n",
    "3) You should use different cost/loss functions that the network tries to minimize depending on the number of labels:\n",
    "    \n",
    "    a. For binary classification we will use the sum of squared error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$SSE(X) = \\sum_{j=1}^{n}({y_j - \\hat{y}_j})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    The function tf.reduce_sum will allow you to sum across all instances.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) For the implementation of Backpropagation, you should use tf.train.AdamOptimizer\n",
    "\n",
    "For more on optimizers, you may follow this link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) You should train your network using your inputted learning rate and for the inputted number of iterations. The iterations are simply a loop that calls Backpropagation a fixed number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "\n",
    "- Biases?\n",
    "- Mean normalize?\n",
    "- How to evaluate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking a the data\n",
    "input = pd.read_csv(\"higgs_small.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.528340</td>\n",
       "      <td>0.990371</td>\n",
       "      <td>-0.003816</td>\n",
       "      <td>-0.001638</td>\n",
       "      <td>0.995049</td>\n",
       "      <td>-0.007613</td>\n",
       "      <td>0.987114</td>\n",
       "      <td>-0.003000</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.998344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007575</td>\n",
       "      <td>-0.004029</td>\n",
       "      <td>0.992721</td>\n",
       "      <td>1.032611</td>\n",
       "      <td>1.023153</td>\n",
       "      <td>1.050193</td>\n",
       "      <td>1.010189</td>\n",
       "      <td>0.973081</td>\n",
       "      <td>1.031873</td>\n",
       "      <td>0.959199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499199</td>\n",
       "      <td>0.561837</td>\n",
       "      <td>1.004841</td>\n",
       "      <td>1.006189</td>\n",
       "      <td>0.595360</td>\n",
       "      <td>1.006997</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>1.008685</td>\n",
       "      <td>1.008430</td>\n",
       "      <td>1.027402</td>\n",
       "      <td>...</td>\n",
       "      <td>1.009173</td>\n",
       "      <td>1.007090</td>\n",
       "      <td>1.396788</td>\n",
       "      <td>0.652455</td>\n",
       "      <td>0.371610</td>\n",
       "      <td>0.164857</td>\n",
       "      <td>0.398267</td>\n",
       "      <td>0.523552</td>\n",
       "      <td>0.363394</td>\n",
       "      <td>0.313257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274697</td>\n",
       "      <td>-2.434976</td>\n",
       "      <td>-1.742508</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>-1.743944</td>\n",
       "      <td>0.139976</td>\n",
       "      <td>-2.968735</td>\n",
       "      <td>-1.741237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.497265</td>\n",
       "      <td>-1.742691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110875</td>\n",
       "      <td>0.303144</td>\n",
       "      <td>0.133012</td>\n",
       "      <td>0.295983</td>\n",
       "      <td>0.048125</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>0.350939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590936</td>\n",
       "      <td>-0.741244</td>\n",
       "      <td>-0.868047</td>\n",
       "      <td>0.575635</td>\n",
       "      <td>-0.881465</td>\n",
       "      <td>0.676336</td>\n",
       "      <td>-0.688235</td>\n",
       "      <td>-0.867542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.725017</td>\n",
       "      <td>-0.877028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791306</td>\n",
       "      <td>0.846631</td>\n",
       "      <td>0.985775</td>\n",
       "      <td>0.767261</td>\n",
       "      <td>0.673792</td>\n",
       "      <td>0.819170</td>\n",
       "      <td>0.769964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854835</td>\n",
       "      <td>-0.002976</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.890268</td>\n",
       "      <td>-0.011024</td>\n",
       "      <td>0.892163</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-0.003822</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.009698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.895600</td>\n",
       "      <td>0.950719</td>\n",
       "      <td>0.989742</td>\n",
       "      <td>0.917302</td>\n",
       "      <td>0.874004</td>\n",
       "      <td>0.947037</td>\n",
       "      <td>0.871038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.236776</td>\n",
       "      <td>0.735292</td>\n",
       "      <td>0.868220</td>\n",
       "      <td>1.290871</td>\n",
       "      <td>0.865868</td>\n",
       "      <td>1.167809</td>\n",
       "      <td>0.683233</td>\n",
       "      <td>0.871223</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710770</td>\n",
       "      <td>0.869386</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.025925</td>\n",
       "      <td>1.083218</td>\n",
       "      <td>1.020762</td>\n",
       "      <td>1.141633</td>\n",
       "      <td>1.139816</td>\n",
       "      <td>1.139032</td>\n",
       "      <td>1.057478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.805887</td>\n",
       "      <td>2.433894</td>\n",
       "      <td>1.743236</td>\n",
       "      <td>7.998711</td>\n",
       "      <td>1.743229</td>\n",
       "      <td>7.064657</td>\n",
       "      <td>2.969674</td>\n",
       "      <td>1.741454</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>2.498009</td>\n",
       "      <td>1.743372</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>18.428827</td>\n",
       "      <td>10.038273</td>\n",
       "      <td>4.565248</td>\n",
       "      <td>7.442589</td>\n",
       "      <td>11.994177</td>\n",
       "      <td>7.318191</td>\n",
       "      <td>6.015647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2              3   \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.528340       0.990371      -0.003816      -0.001638   \n",
       "std         0.499199       0.561837       1.004841       1.006189   \n",
       "min         0.000000       0.274697      -2.434976      -1.742508   \n",
       "25%         0.000000       0.590936      -0.741244      -0.868047   \n",
       "50%         1.000000       0.854835      -0.002976       0.000971   \n",
       "75%         1.000000       1.236776       0.735292       0.868220   \n",
       "max         1.000000       7.805887       2.433894       1.743236   \n",
       "\n",
       "                  4              5              6              7   \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.995049      -0.007613       0.987114      -0.003000   \n",
       "std         0.595360       1.006997       0.473118       1.008685   \n",
       "min         0.001283      -1.743944       0.139976      -2.968735   \n",
       "25%         0.575635      -0.881465       0.676336      -0.688235   \n",
       "50%         0.890268      -0.011024       0.892163      -0.000025   \n",
       "75%         1.290871       0.865868       1.167809       0.683233   \n",
       "max         7.998711       1.743229       7.064657       2.969674   \n",
       "\n",
       "                  8              9       ...                   19  \\\n",
       "count  100000.000000  100000.000000      ...        100000.000000   \n",
       "mean        0.000438       0.998344      ...            -0.007575   \n",
       "std         1.008430       1.027402      ...             1.009173   \n",
       "min        -1.741237       0.000000      ...            -2.497265   \n",
       "25%        -0.867542       0.000000      ...            -0.725017   \n",
       "50%        -0.003822       1.086538      ...            -0.010455   \n",
       "75%         0.871223       2.173076      ...             0.710770   \n",
       "max         1.741454       2.173076      ...             2.498009   \n",
       "\n",
       "                  20             21             22             23  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       -0.004029       0.992721       1.032611       1.023153   \n",
       "std         1.007090       1.396788       0.652455       0.371610   \n",
       "min        -1.742691       0.000000       0.110875       0.303144   \n",
       "25%        -0.877028       0.000000       0.791306       0.846631   \n",
       "50%        -0.009698       0.000000       0.895600       0.950719   \n",
       "75%         0.869386       3.101961       1.025925       1.083218   \n",
       "max         1.743372       3.101961      18.428827      10.038273   \n",
       "\n",
       "                  24             25             26             27  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        1.050193       1.010189       0.973081       1.031873   \n",
       "std         0.164857       0.398267       0.523552       0.363394   \n",
       "min         0.133012       0.295983       0.048125       0.303350   \n",
       "25%         0.985775       0.767261       0.673792       0.819170   \n",
       "50%         0.989742       0.917302       0.874004       0.947037   \n",
       "75%         1.020762       1.141633       1.139816       1.139032   \n",
       "max         4.565248       7.442589      11.994177       7.318191   \n",
       "\n",
       "                  28  \n",
       "count  100000.000000  \n",
       "mean        0.959199  \n",
       "std         0.313257  \n",
       "min         0.350939  \n",
       "25%         0.769964  \n",
       "50%         0.871038  \n",
       "75%         1.057478  \n",
       "max         6.015647  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bogus as of now\n",
    "working_locally = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename_queue = tf.train.string_input_producer([\"higgs_small.csv\"])\n",
    "\n",
    "\n",
    "line_reader = tf.TextLineReader()\n",
    "key, csv_row = line_reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_defaults = [[0.0]]*29\n",
    "all_columns = tf.decode_csv(csv_row, record_defaults=record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the features back into a tensor.\n",
    "features = tf.stack(all_columns[1:])\n",
    "labels = tf.stack(all_columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 50\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "num_examples= 100000\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 20 # 1st layer number of features\n",
    "n_hidden_2 = 15 # 2nd layer number of features\n",
    "n_input = 28 \n",
    "n_classes = 1 \n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with SIGMOID activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden layer with SIGMOID activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    # Output layer with SIGMOID activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    out_layer_sigmoid = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer_sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_sum((y-pred)**2)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer_5/input_producer_5_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](input_producer_5, input_producer_5/Const, ^input_producer_5/Assert/Assert)]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10000,) for Tensor 'Placeholder_23:0', which has shape '(?, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-f87130ca3bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         _, c = sess.run([optimizer, cost], feed_dict={x: x_batch,\n\u001b[0;32m---> 22\u001b[0;31m                                                       y: y_batch})\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Compute average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cse255/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cse255/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1111\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1112\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (10000,) for Tensor 'Placeholder_23:0', which has shape '(?, 1)'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #tf.initialize_all_variables().run()\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in range(num_examples-90000):\n",
    "            example, label = sess.run([features, labels])\n",
    "            x_batch.append(example)\n",
    "            y_batch.append(label)\n",
    "            \n",
    "        y_batch = np.asarray(y_batch,)\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: x_batch,\n",
    "                                                      y: y_batch})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print (\"Optimization Finished!\")\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
