{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PA9: Neural Networks with Tensorflow\n",
    "\n",
    "In this assignment, you will:\n",
    "\n",
    "1. Implement neural networks as a powerful approach to supervised machine learning,\n",
    "2. Practice using state-of-the-art software tools and programming paradigms for machine learning,\n",
    "3. Investigate the impact of parameters to learning on neural network performance as evaluated on an empirical data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will learn use a well known dataset:\n",
    "\n",
    "[Higgs](https://archive.ics.uci.edu/ml/datasets/HIGGS). Some information regarding this dataset: The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes.\n",
    "\n",
    "For local testing you will use the sample dataset provided to you with this notebook.\n",
    "When submitting on EdX, your code will be evaluated on a much larger sample of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file format for each of the above data set is as follows:\n",
    "\n",
    "• The first row contains a comma-separated list of the names of the label and attributes\n",
    "\n",
    "• Each successive row represents a single instance\n",
    "\n",
    "• The first entry of each instance is the label to be learned, and all other entries (following the commas) are attribute values.\n",
    "\n",
    "• All attributes are numerical i.e. real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: \n",
    "\n",
    "Your goal is to complete the below function named train_nn that behaves as follows:\n",
    "\n",
    "1) It should take as input six parameters:\n",
    "    \n",
    "    a. The path to a file containing a data set (e.g., higgs_sample.csv)\n",
    "    \n",
    "    b. The number of neurons to use in the hidden layer\n",
    "    \n",
    "    c. The learning rate to use during backpropagation\n",
    "    \n",
    "    d. The number of iterations to use during training\n",
    "    \n",
    "    e. The percentage of instances to use for a training set\n",
    "    \n",
    "    f. A random seed as an integer\n",
    "    \n",
    "    \n",
    "For example, if the call to train_nn looks like train_nn(higgs_sample.csv 20 0.001 1000 0.75 12345) which will create a neural network with 20 neurons in the hidden layer, train the network using a learning rate = 0.001 and 1000 iterations through higgs_sample.csv with a random seed of 12345, where 75% of the data will be used for training (and the remaining 25% will be used for testing)\n",
    "\n",
    "2) You should create a neural network in Tensorflow that will be learned from the training data. The key parameters to the architecture of the neural network are based on your inputted parameters and the size of your data set:\n",
    "    \n",
    "    a. The number of attributes in the input layer is the length of each instance’s\n",
    "    attribute list (which is the same for all instances)\n",
    "    \n",
    "    b. The number of neurons in a hidden layer will be inputted to the program as a\n",
    "    parameter. Each hidden neuron should use tf.sigmoid as its activation function.\n",
    "    \n",
    "    c. The number of output neurons will be 1 since it is a binary classification task, and that should use tf.sigmoid as its activation function\n",
    "    \n",
    "3) You should use different cost/loss functions that the network tries to minimize depending on the number of labels:\n",
    "    \n",
    "    a. For binary classification we will use the cross entropy loss function:\n",
    "    \n",
    "    TODO: get latex version from here: https://stackoverflow.com/questions/46291253/tensorflow-sigmoid-and-cross-entropy-vs-sigmoid-cross-entropy-with-logits\n",
    "    \n",
    "    b. You will use full batch gradient descent (No mini batching is required, but you may optionlly do it) TODO: change this if grading scheme does not conform to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: edit this to use cross entropy loss\n",
    "\n",
    "$$SSE(X) = \\sum_{j=1}^{n}({y_j - \\hat{y}_j})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    The function tf.reduce_sum will allow you to sum across all instances.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) For the implementation of Backpropagation, you should use tf.train.AdamOptimizer\n",
    "\n",
    "For more on optimizers, you may follow this link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) You should train your network using your inputted learning rate and for the inputted number of iterations. The iterations are simply a loop that calls Backpropagation a fixed number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "\n",
    "- Biases?\n",
    "- Mean normalize?\n",
    "- How to evaluate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name = \"higgs_small.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking a the data\n",
    "input = pd.read_csv(file_name,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.529500</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>-0.016681</td>\n",
       "      <td>-0.003486</td>\n",
       "      <td>0.991385</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>0.992058</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>1.004939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>-0.014789</td>\n",
       "      <td>0.999142</td>\n",
       "      <td>1.029148</td>\n",
       "      <td>1.021455</td>\n",
       "      <td>1.050877</td>\n",
       "      <td>1.012534</td>\n",
       "      <td>0.967713</td>\n",
       "      <td>1.031224</td>\n",
       "      <td>0.957864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499154</td>\n",
       "      <td>0.574965</td>\n",
       "      <td>1.002943</td>\n",
       "      <td>1.010838</td>\n",
       "      <td>0.595460</td>\n",
       "      <td>1.004828</td>\n",
       "      <td>0.477408</td>\n",
       "      <td>1.004115</td>\n",
       "      <td>1.015907</td>\n",
       "      <td>1.026965</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005459</td>\n",
       "      <td>1.000998</td>\n",
       "      <td>1.396992</td>\n",
       "      <td>0.637225</td>\n",
       "      <td>0.369623</td>\n",
       "      <td>0.165939</td>\n",
       "      <td>0.404927</td>\n",
       "      <td>0.523195</td>\n",
       "      <td>0.366220</td>\n",
       "      <td>0.313337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275063</td>\n",
       "      <td>-2.425236</td>\n",
       "      <td>-1.742508</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>-1.743755</td>\n",
       "      <td>0.159488</td>\n",
       "      <td>-2.941008</td>\n",
       "      <td>-1.741237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.496432</td>\n",
       "      <td>-1.742136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172241</td>\n",
       "      <td>0.342467</td>\n",
       "      <td>0.461183</td>\n",
       "      <td>0.384411</td>\n",
       "      <td>0.080986</td>\n",
       "      <td>0.388779</td>\n",
       "      <td>0.444956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596061</td>\n",
       "      <td>-0.744166</td>\n",
       "      <td>-0.872486</td>\n",
       "      <td>0.571250</td>\n",
       "      <td>-0.885740</td>\n",
       "      <td>0.679817</td>\n",
       "      <td>-0.682789</td>\n",
       "      <td>-0.892627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708360</td>\n",
       "      <td>-0.885352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790251</td>\n",
       "      <td>0.847034</td>\n",
       "      <td>0.985821</td>\n",
       "      <td>0.768254</td>\n",
       "      <td>0.675036</td>\n",
       "      <td>0.822794</td>\n",
       "      <td>0.770775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858770</td>\n",
       "      <td>-0.028786</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.886284</td>\n",
       "      <td>-0.019940</td>\n",
       "      <td>0.897522</td>\n",
       "      <td>-0.009928</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>-0.014137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894663</td>\n",
       "      <td>0.949071</td>\n",
       "      <td>0.989790</td>\n",
       "      <td>0.917218</td>\n",
       "      <td>0.869291</td>\n",
       "      <td>0.945466</td>\n",
       "      <td>0.871492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.248305</td>\n",
       "      <td>0.714839</td>\n",
       "      <td>0.881675</td>\n",
       "      <td>1.294626</td>\n",
       "      <td>0.856703</td>\n",
       "      <td>1.170305</td>\n",
       "      <td>0.680263</td>\n",
       "      <td>0.878984</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719931</td>\n",
       "      <td>0.846634</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.024671</td>\n",
       "      <td>1.079560</td>\n",
       "      <td>1.021323</td>\n",
       "      <td>1.145685</td>\n",
       "      <td>1.123704</td>\n",
       "      <td>1.132049</td>\n",
       "      <td>1.055831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.695388</td>\n",
       "      <td>2.429998</td>\n",
       "      <td>1.743236</td>\n",
       "      <td>5.824007</td>\n",
       "      <td>1.742818</td>\n",
       "      <td>7.064657</td>\n",
       "      <td>2.969674</td>\n",
       "      <td>1.741454</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>2.495511</td>\n",
       "      <td>1.742817</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>13.098125</td>\n",
       "      <td>7.391968</td>\n",
       "      <td>3.682260</td>\n",
       "      <td>6.583121</td>\n",
       "      <td>8.255083</td>\n",
       "      <td>4.749469</td>\n",
       "      <td>4.316365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.529500      0.997924     -0.016681     -0.003486      0.991385   \n",
       "std        0.499154      0.574965      1.002943      1.010838      0.595460   \n",
       "min        0.000000      0.275063     -2.425236     -1.742508      0.012355   \n",
       "25%        0.000000      0.596061     -0.744166     -0.872486      0.571250   \n",
       "50%        1.000000      0.858770     -0.028786      0.000643      0.886284   \n",
       "75%        1.000000      1.248305      0.714839      0.881675      1.294626   \n",
       "max        1.000000      6.695388      2.429998      1.743236      5.824007   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      -0.009822      0.992058     -0.001468      0.003751      1.004939   \n",
       "std        1.004828      0.477408      1.004115      1.015907      1.026965   \n",
       "min       -1.743755      0.159488     -2.941008     -1.741237      0.000000   \n",
       "25%       -0.885740      0.679817     -0.682789     -0.892627      0.000000   \n",
       "50%       -0.019940      0.897522     -0.009928      0.020396      1.086538   \n",
       "75%        0.856703      1.170305      0.680263      0.878984      2.173076   \n",
       "max        1.742818      7.064657      2.969674      1.741454      2.173076   \n",
       "\n",
       "           ...                 19            20            21            22  \\\n",
       "count      ...       10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       ...           0.001222     -0.014789      0.999142      1.029148   \n",
       "std        ...           1.005459      1.000998      1.396992      0.637225   \n",
       "min        ...          -2.496432     -1.742136      0.000000      0.172241   \n",
       "25%        ...          -0.708360     -0.885352      0.000000      0.790251   \n",
       "50%        ...          -0.001294     -0.014137      0.000000      0.894663   \n",
       "75%        ...           0.719931      0.846634      3.101961      1.024671   \n",
       "max        ...           2.495511      1.742817      3.101961     13.098125   \n",
       "\n",
       "                 23            24            25            26            27  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       1.021455      1.050877      1.012534      0.967713      1.031224   \n",
       "std        0.369623      0.165939      0.404927      0.523195      0.366220   \n",
       "min        0.342467      0.461183      0.384411      0.080986      0.388779   \n",
       "25%        0.847034      0.985821      0.768254      0.675036      0.822794   \n",
       "50%        0.949071      0.989790      0.917218      0.869291      0.945466   \n",
       "75%        1.079560      1.021323      1.145685      1.123704      1.132049   \n",
       "max        7.391968      3.682260      6.583121      8.255083      4.749469   \n",
       "\n",
       "                 28  \n",
       "count  10000.000000  \n",
       "mean       0.957864  \n",
       "std        0.313337  \n",
       "min        0.444956  \n",
       "25%        0.770775  \n",
       "50%        0.871492  \n",
       "75%        1.055831  \n",
       "max        4.316365  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bogus as of now\n",
    "working_locally = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename_queue = tf.train.string_input_producer([file_name])\n",
    "\n",
    "\n",
    "line_reader = tf.TextLineReader()\n",
    "key, csv_row = line_reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_defaults = [[0.0]]*29\n",
    "all_columns = tf.decode_csv(csv_row, record_defaults=record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the features back into a tensor.\n",
    "features = tf.stack(all_columns[1:])\n",
    "labels = tf.stack(all_columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.05\n",
    "training_epochs = 400\n",
    "batch_size = 10000\n",
    "display_step = 1\n",
    "num_examples= 10000\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 32 # 1st layer number of features\n",
    "n_hidden_2 = 8 # 2nd layer number of features\n",
    "n_input = 28 \n",
    "n_classes = 1 \n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "    mean, std = train.mean(), train.std()\n",
    "    train = (train - mean) / std\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with SIGMOID activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden layer with SIGMOID activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    # Output layer with SIGMOID activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "#     out_layer_sigmoid = tf.nn.sigmoid(out_layer)\n",
    "#     return out_layer_sigmoid\n",
    "    return out_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# cost = tf.reduce_sum((y-pred)**2)\n",
    "\n",
    "# pred_onehot = tf.round(pred)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=pred)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "pred_onehot = tf.round(tf.nn.sigmoid(pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.383473992 acc= [0.5295]\n",
      "(10000, 29)\n",
      "Epoch: 0002 cost= 1.087249756 acc= [0.5294]\n",
      "Epoch: 0003 cost= 0.827981651 acc= [0.5194]\n",
      "Epoch: 0004 cost= 0.794910431 acc= [0.4905]\n",
      "Epoch: 0005 cost= 0.877075970 acc= [0.4866]\n",
      "Epoch: 0006 cost= 0.874924004 acc= [0.4854]\n",
      "Epoch: 0007 cost= 0.814799666 acc= [0.4909]\n",
      "Epoch: 0008 cost= 0.752783000 acc= [0.5006]\n",
      "Epoch: 0009 cost= 0.720396578 acc= [0.5091]\n",
      "Epoch: 0010 cost= 0.715383708 acc= [0.5208]\n",
      "Epoch: 0011 cost= 0.720889330 acc= [0.5236]\n",
      "Epoch: 0012 cost= 0.725045204 acc= [0.5298]\n",
      "Epoch: 0013 cost= 0.723843157 acc= [0.5312]\n",
      "Epoch: 0014 cost= 0.717828214 acc= [0.5335]\n",
      "Epoch: 0015 cost= 0.709551454 acc= [0.5378]\n",
      "Epoch: 0016 cost= 0.701934695 acc= [0.5363]\n",
      "Epoch: 0017 cost= 0.697039485 acc= [0.5344]\n",
      "Epoch: 0018 cost= 0.695339739 acc= [0.5354]\n",
      "Epoch: 0019 cost= 0.695858002 acc= [0.5297]\n",
      "Epoch: 0020 cost= 0.696946621 acc= [0.5229]\n",
      "Epoch: 0021 cost= 0.697177768 acc= [0.5184]\n",
      "Epoch: 0022 cost= 0.695911229 acc= [0.519]\n",
      "Epoch: 0023 cost= 0.693354011 acc= [0.5246]\n",
      "Epoch: 0024 cost= 0.690259576 acc= [0.5323]\n",
      "Epoch: 0025 cost= 0.687493145 acc= [0.5443]\n",
      "Epoch: 0026 cost= 0.685660124 acc= [0.5521]\n",
      "Epoch: 0027 cost= 0.684912384 acc= [0.5672]\n",
      "Epoch: 0028 cost= 0.684969842 acc= [0.5583]\n",
      "Epoch: 0029 cost= 0.685304582 acc= [0.5503]\n",
      "Epoch: 0030 cost= 0.685387373 acc= [0.5488]\n",
      "Epoch: 0031 cost= 0.684888661 acc= [0.5494]\n",
      "Epoch: 0032 cost= 0.683769703 acc= [0.5515]\n",
      "Epoch: 0033 cost= 0.682247221 acc= [0.5585]\n",
      "Epoch: 0034 cost= 0.680667400 acc= [0.5664]\n",
      "Epoch: 0035 cost= 0.679347575 acc= [0.575]\n",
      "Epoch: 0036 cost= 0.678445041 acc= [0.5813]\n",
      "Epoch: 0037 cost= 0.677907348 acc= [0.5811]\n",
      "Epoch: 0038 cost= 0.677519917 acc= [0.5778]\n",
      "Epoch: 0039 cost= 0.677022636 acc= [0.5779]\n",
      "Epoch: 0040 cost= 0.676244259 acc= [0.5783]\n",
      "Epoch: 0041 cost= 0.675181866 acc= [0.582]\n",
      "Epoch: 0042 cost= 0.673991799 acc= [0.5833]\n",
      "Epoch: 0043 cost= 0.672893941 acc= [0.5894]\n",
      "Epoch: 0044 cost= 0.672051787 acc= [0.5947]\n",
      "Epoch: 0045 cost= 0.671480477 acc= [0.5916]\n",
      "Epoch: 0046 cost= 0.671048403 acc= [0.5915]\n",
      "Epoch: 0047 cost= 0.670560181 acc= [0.5908]\n",
      "Epoch: 0048 cost= 0.669879973 acc= [0.5919]\n",
      "Epoch: 0049 cost= 0.669009507 acc= [0.595]\n",
      "Epoch: 0050 cost= 0.668076873 acc= [0.5979]\n",
      "Epoch: 0051 cost= 0.667236924 acc= [0.6039]\n",
      "Epoch: 0052 cost= 0.666563869 acc= [0.604]\n",
      "Epoch: 0053 cost= 0.665997684 acc= [0.6034]\n",
      "Epoch: 0054 cost= 0.665395141 acc= [0.6055]\n",
      "Epoch: 0055 cost= 0.664634109 acc= [0.6081]\n",
      "Epoch: 0056 cost= 0.663697839 acc= [0.609]\n",
      "Epoch: 0057 cost= 0.662665486 acc= [0.6126]\n",
      "Epoch: 0058 cost= 0.661646783 acc= [0.6123]\n",
      "Epoch: 0059 cost= 0.660743117 acc= [0.6107]\n",
      "Epoch: 0060 cost= 0.659966409 acc= [0.611]\n",
      "Epoch: 0061 cost= 0.658941150 acc= [0.6133]\n",
      "Epoch: 0062 cost= 0.657605052 acc= [0.6153]\n",
      "Epoch: 0063 cost= 0.656435668 acc= [0.6169]\n",
      "Epoch: 0064 cost= 0.655490518 acc= [0.6178]\n",
      "Epoch: 0065 cost= 0.654456198 acc= [0.6184]\n",
      "Epoch: 0066 cost= 0.653217256 acc= [0.6213]\n",
      "Epoch: 0067 cost= 0.652027905 acc= [0.6225]\n",
      "Epoch: 0068 cost= 0.651069522 acc= [0.6216]\n",
      "Epoch: 0069 cost= 0.650001645 acc= [0.6246]\n",
      "Epoch: 0070 cost= 0.648751199 acc= [0.6257]\n",
      "Epoch: 0071 cost= 0.647576928 acc= [0.6298]\n",
      "Epoch: 0072 cost= 0.646508813 acc= [0.6303]\n",
      "Epoch: 0073 cost= 0.645354748 acc= [0.6311]\n",
      "Epoch: 0074 cost= 0.644150376 acc= [0.6321]\n",
      "Epoch: 0075 cost= 0.643078327 acc= [0.6332]\n",
      "Epoch: 0076 cost= 0.642065644 acc= [0.6338]\n",
      "Epoch: 0077 cost= 0.640958667 acc= [0.636]\n",
      "Epoch: 0078 cost= 0.639850438 acc= [0.636]\n",
      "Epoch: 0079 cost= 0.638815403 acc= [0.6383]\n",
      "Epoch: 0080 cost= 0.637719810 acc= [0.6401]\n",
      "Epoch: 0081 cost= 0.636556268 acc= [0.641]\n",
      "Epoch: 0082 cost= 0.635445237 acc= [0.6435]\n",
      "Epoch: 0083 cost= 0.634338498 acc= [0.6468]\n",
      "Epoch: 0084 cost= 0.633171201 acc= [0.6477]\n",
      "Epoch: 0085 cost= 0.632032275 acc= [0.649]\n",
      "Epoch: 0086 cost= 0.630913079 acc= [0.649]\n",
      "Epoch: 0087 cost= 0.629737139 acc= [0.6518]\n",
      "Epoch: 0088 cost= 0.628574073 acc= [0.6527]\n",
      "Epoch: 0089 cost= 0.627450287 acc= [0.6534]\n",
      "Epoch: 0090 cost= 0.626303315 acc= [0.6547]\n",
      "Epoch: 0091 cost= 0.625175059 acc= [0.6556]\n",
      "Epoch: 0092 cost= 0.624067366 acc= [0.6569]\n",
      "Epoch: 0093 cost= 0.622918606 acc= [0.6585]\n",
      "Epoch: 0094 cost= 0.621775925 acc= [0.6612]\n",
      "Epoch: 0095 cost= 0.620622396 acc= [0.6638]\n",
      "Epoch: 0096 cost= 0.619428813 acc= [0.6638]\n",
      "Epoch: 0097 cost= 0.618241191 acc= [0.6657]\n",
      "Epoch: 0098 cost= 0.617021680 acc= [0.667]\n",
      "Epoch: 0099 cost= 0.615783036 acc= [0.6687]\n",
      "Epoch: 0100 cost= 0.614540458 acc= [0.6709]\n",
      "Epoch: 0101 cost= 0.613269031 acc= [0.6714]\n",
      "Epoch: 0102 cost= 0.612013102 acc= [0.6732]\n",
      "Epoch: 0103 cost= 0.610760272 acc= [0.6745]\n",
      "Epoch: 0104 cost= 0.609549403 acc= [0.676]\n",
      "Epoch: 0105 cost= 0.608373225 acc= [0.6774]\n",
      "Epoch: 0106 cost= 0.607237160 acc= [0.6788]\n",
      "Epoch: 0107 cost= 0.606132448 acc= [0.6804]\n",
      "Epoch: 0108 cost= 0.605044901 acc= [0.682]\n",
      "Epoch: 0109 cost= 0.603961825 acc= [0.6835]\n",
      "Epoch: 0110 cost= 0.602872372 acc= [0.6837]\n",
      "Epoch: 0111 cost= 0.601767063 acc= [0.6855]\n",
      "Epoch: 0112 cost= 0.600659072 acc= [0.6862]\n",
      "Epoch: 0113 cost= 0.599538326 acc= [0.6864]\n",
      "Epoch: 0114 cost= 0.598422945 acc= [0.6873]\n",
      "Epoch: 0115 cost= 0.597300589 acc= [0.6897]\n",
      "Epoch: 0116 cost= 0.596176744 acc= [0.6906]\n",
      "Epoch: 0117 cost= 0.595061660 acc= [0.6923]\n",
      "Epoch: 0118 cost= 0.593945384 acc= [0.6926]\n",
      "Epoch: 0119 cost= 0.592834651 acc= [0.6929]\n",
      "Epoch: 0120 cost= 0.591743171 acc= [0.6924]\n",
      "Epoch: 0121 cost= 0.590669632 acc= [0.6942]\n",
      "Epoch: 0122 cost= 0.589613140 acc= [0.6955]\n",
      "Epoch: 0123 cost= 0.588571072 acc= [0.697]\n",
      "Epoch: 0124 cost= 0.587542653 acc= [0.6968]\n",
      "Epoch: 0125 cost= 0.586526573 acc= [0.6984]\n",
      "Epoch: 0126 cost= 0.585521996 acc= [0.6999]\n",
      "Epoch: 0127 cost= 0.584530354 acc= [0.7018]\n",
      "Epoch: 0128 cost= 0.583558440 acc= [0.7007]\n",
      "Epoch: 0129 cost= 0.582658768 acc= [0.7018]\n",
      "Epoch: 0130 cost= 0.582086146 acc= [0.7006]\n",
      "Epoch: 0131 cost= 0.582132339 acc= [0.7022]\n",
      "Epoch: 0132 cost= 0.582015634 acc= [0.6978]\n",
      "Epoch: 0133 cost= 0.579289138 acc= [0.7035]\n",
      "Epoch: 0134 cost= 0.579791129 acc= [0.7043]\n",
      "Epoch: 0135 cost= 0.579423010 acc= [0.7013]\n",
      "Epoch: 0136 cost= 0.577108324 acc= [0.7046]\n",
      "Epoch: 0137 cost= 0.578348160 acc= [0.7058]\n",
      "Epoch: 0138 cost= 0.576333225 acc= [0.7047]\n",
      "Epoch: 0139 cost= 0.575773239 acc= [0.7047]\n",
      "Epoch: 0140 cost= 0.575687706 acc= [0.7075]\n",
      "Epoch: 0141 cost= 0.573826730 acc= [0.7102]\n",
      "Epoch: 0142 cost= 0.574325681 acc= [0.7058]\n",
      "Epoch: 0143 cost= 0.572725236 acc= [0.71]\n",
      "Epoch: 0144 cost= 0.572751522 acc= [0.711]\n",
      "Epoch: 0145 cost= 0.572019875 acc= [0.7087]\n",
      "Epoch: 0146 cost= 0.571116507 acc= [0.71]\n",
      "Epoch: 0147 cost= 0.571150362 acc= [0.7109]\n",
      "Epoch: 0148 cost= 0.569847345 acc= [0.7108]\n",
      "Epoch: 0149 cost= 0.569810629 acc= [0.7116]\n",
      "Epoch: 0150 cost= 0.568973362 acc= [0.7132]\n",
      "Epoch: 0151 cost= 0.568379402 acc= [0.7142]\n",
      "Epoch: 0152 cost= 0.568132699 acc= [0.7138]\n",
      "Epoch: 0153 cost= 0.567190647 acc= [0.7136]\n",
      "Epoch: 0154 cost= 0.567030787 acc= [0.7136]\n",
      "Epoch: 0155 cost= 0.566343486 acc= [0.7148]\n",
      "Epoch: 0156 cost= 0.565713882 acc= [0.7168]\n",
      "Epoch: 0157 cost= 0.565461934 acc= [0.7162]\n",
      "Epoch: 0158 cost= 0.564655006 acc= [0.7182]\n",
      "Epoch: 0159 cost= 0.564237833 acc= [0.7172]\n",
      "Epoch: 0160 cost= 0.563793540 acc= [0.7177]\n",
      "Epoch: 0161 cost= 0.563067257 acc= [0.7193]\n",
      "Epoch: 0162 cost= 0.562691987 acc= [0.7192]\n",
      "Epoch: 0163 cost= 0.562147439 acc= [0.7198]\n",
      "Epoch: 0164 cost= 0.561506808 acc= [0.7217]\n",
      "Epoch: 0165 cost= 0.561099112 acc= [0.7214]\n",
      "Epoch: 0166 cost= 0.560528338 acc= [0.7217]\n",
      "Epoch: 0167 cost= 0.559920788 acc= [0.7224]\n",
      "Epoch: 0168 cost= 0.559466898 acc= [0.7235]\n",
      "Epoch: 0169 cost= 0.558902681 acc= [0.7215]\n",
      "Epoch: 0170 cost= 0.558292568 acc= [0.7227]\n",
      "Epoch: 0171 cost= 0.557791710 acc= [0.7234]\n",
      "Epoch: 0172 cost= 0.557251096 acc= [0.7232]\n",
      "Epoch: 0173 cost= 0.556638956 acc= [0.7236]\n",
      "Epoch: 0174 cost= 0.556084096 acc= [0.7241]\n",
      "Epoch: 0175 cost= 0.555563271 acc= [0.7236]\n",
      "Epoch: 0176 cost= 0.554987788 acc= [0.7244]\n",
      "Epoch: 0177 cost= 0.554394126 acc= [0.7254]\n",
      "Epoch: 0178 cost= 0.553851068 acc= [0.7251]\n",
      "Epoch: 0179 cost= 0.553329706 acc= [0.727]\n",
      "Epoch: 0180 cost= 0.552777469 acc= [0.7266]\n",
      "Epoch: 0181 cost= 0.552211642 acc= [0.7273]\n",
      "Epoch: 0182 cost= 0.551669717 acc= [0.7282]\n",
      "Epoch: 0183 cost= 0.551154971 acc= [0.7281]\n",
      "Epoch: 0184 cost= 0.550641596 acc= [0.7288]\n",
      "Epoch: 0185 cost= 0.550110638 acc= [0.7296]\n",
      "Epoch: 0186 cost= 0.549570143 acc= [0.7291]\n",
      "Epoch: 0187 cost= 0.549033701 acc= [0.7294]\n",
      "Epoch: 0188 cost= 0.548508584 acc= [0.7293]\n",
      "Epoch: 0189 cost= 0.547991574 acc= [0.7292]\n",
      "Epoch: 0190 cost= 0.547476828 acc= [0.7288]\n",
      "Epoch: 0191 cost= 0.546959400 acc= [0.7292]\n",
      "Epoch: 0192 cost= 0.546437502 acc= [0.7291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0193 cost= 0.545914650 acc= [0.7284]\n",
      "Epoch: 0194 cost= 0.545391202 acc= [0.7289]\n",
      "Epoch: 0195 cost= 0.544872284 acc= [0.7283]\n",
      "Epoch: 0196 cost= 0.544358492 acc= [0.73]\n",
      "Epoch: 0197 cost= 0.543856561 acc= [0.7292]\n",
      "Epoch: 0198 cost= 0.543368280 acc= [0.7313]\n",
      "Epoch: 0199 cost= 0.542913973 acc= [0.731]\n",
      "Epoch: 0200 cost= 0.542507410 acc= [0.7319]\n",
      "Epoch: 0201 cost= 0.542226732 acc= [0.7321]\n",
      "Epoch: 0202 cost= 0.542076826 acc= [0.7329]\n",
      "Epoch: 0203 cost= 0.542250931 acc= [0.7312]\n",
      "Epoch: 0204 cost= 0.542149305 acc= [0.7329]\n",
      "Epoch: 0205 cost= 0.541797042 acc= [0.7313]\n",
      "Epoch: 0206 cost= 0.540237546 acc= [0.7335]\n",
      "Epoch: 0207 cost= 0.538941026 acc= [0.7355]\n",
      "Epoch: 0208 cost= 0.538586795 acc= [0.7366]\n",
      "Epoch: 0209 cost= 0.538789570 acc= [0.7358]\n",
      "Epoch: 0210 cost= 0.538709104 acc= [0.7352]\n",
      "Epoch: 0211 cost= 0.537672222 acc= [0.7374]\n",
      "Epoch: 0212 cost= 0.536627650 acc= [0.7377]\n",
      "Epoch: 0213 cost= 0.536200643 acc= [0.7381]\n",
      "Epoch: 0214 cost= 0.536193013 acc= [0.7395]\n",
      "Epoch: 0215 cost= 0.535989463 acc= [0.7372]\n",
      "Epoch: 0216 cost= 0.535190165 acc= [0.7398]\n",
      "Epoch: 0217 cost= 0.534354508 acc= [0.7396]\n",
      "Epoch: 0218 cost= 0.533893347 acc= [0.7398]\n",
      "Epoch: 0219 cost= 0.533728540 acc= [0.7399]\n",
      "Epoch: 0220 cost= 0.533484280 acc= [0.7396]\n",
      "Epoch: 0221 cost= 0.532888472 acc= [0.7411]\n",
      "Epoch: 0222 cost= 0.532196701 acc= [0.7402]\n",
      "Epoch: 0223 cost= 0.531665206 acc= [0.7413]\n",
      "Epoch: 0224 cost= 0.531356633 acc= [0.7404]\n",
      "Epoch: 0225 cost= 0.531105638 acc= [0.742]\n",
      "Epoch: 0226 cost= 0.530711710 acc= [0.7411]\n",
      "Epoch: 0227 cost= 0.530187011 acc= [0.7417]\n",
      "Epoch: 0228 cost= 0.529615045 acc= [0.7422]\n",
      "Epoch: 0229 cost= 0.529131651 acc= [0.7425]\n",
      "Epoch: 0230 cost= 0.528761625 acc= [0.7428]\n",
      "Epoch: 0231 cost= 0.528445363 acc= [0.7421]\n",
      "Epoch: 0232 cost= 0.528113663 acc= [0.7441]\n",
      "Epoch: 0233 cost= 0.527708769 acc= [0.7428]\n",
      "Epoch: 0234 cost= 0.527256429 acc= [0.745]\n",
      "Epoch: 0235 cost= 0.526773036 acc= [0.745]\n",
      "Epoch: 0236 cost= 0.526306450 acc= [0.7449]\n",
      "Epoch: 0237 cost= 0.525874197 acc= [0.7466]\n",
      "Epoch: 0238 cost= 0.525478005 acc= [0.7468]\n",
      "Epoch: 0239 cost= 0.525107145 acc= [0.7467]\n",
      "Epoch: 0240 cost= 0.524750471 acc= [0.7465]\n",
      "Epoch: 0241 cost= 0.524403811 acc= [0.7465]\n",
      "Epoch: 0242 cost= 0.524059415 acc= [0.7479]\n",
      "Epoch: 0243 cost= 0.523734570 acc= [0.7472]\n",
      "Epoch: 0244 cost= 0.523416400 acc= [0.7474]\n",
      "Epoch: 0245 cost= 0.523146391 acc= [0.746]\n",
      "Epoch: 0246 cost= 0.522882342 acc= [0.7464]\n",
      "Epoch: 0247 cost= 0.522700369 acc= [0.7457]\n",
      "Epoch: 0248 cost= 0.522464335 acc= [0.7477]\n",
      "Epoch: 0249 cost= 0.522281647 acc= [0.7467]\n",
      "Epoch: 0250 cost= 0.521857202 acc= [0.7476]\n",
      "Epoch: 0251 cost= 0.521359861 acc= [0.7472]\n",
      "Epoch: 0252 cost= 0.520599067 acc= [0.749]\n",
      "Epoch: 0253 cost= 0.519859254 acc= [0.7486]\n",
      "Epoch: 0254 cost= 0.519213259 acc= [0.7512]\n",
      "Epoch: 0255 cost= 0.518753529 acc= [0.7514]\n",
      "Epoch: 0256 cost= 0.518454969 acc= [0.7506]\n",
      "Epoch: 0257 cost= 0.518255055 acc= [0.7515]\n",
      "Epoch: 0258 cost= 0.518117607 acc= [0.7487]\n",
      "Epoch: 0259 cost= 0.517952025 acc= [0.7512]\n",
      "Epoch: 0260 cost= 0.517807126 acc= [0.7492]\n",
      "Epoch: 0261 cost= 0.517482638 acc= [0.7514]\n",
      "Epoch: 0262 cost= 0.517103016 acc= [0.7498]\n",
      "Epoch: 0263 cost= 0.516474009 acc= [0.7521]\n",
      "Epoch: 0264 cost= 0.515817761 acc= [0.7504]\n",
      "Epoch: 0265 cost= 0.515154600 acc= [0.7541]\n",
      "Epoch: 0266 cost= 0.514618337 acc= [0.7532]\n",
      "Epoch: 0267 cost= 0.514220119 acc= [0.7543]\n",
      "Epoch: 0268 cost= 0.513933480 acc= [0.7536]\n",
      "Epoch: 0269 cost= 0.513724923 acc= [0.7528]\n",
      "Epoch: 0270 cost= 0.513556302 acc= [0.7535]\n",
      "Epoch: 0271 cost= 0.513455272 acc= [0.7522]\n",
      "Epoch: 0272 cost= 0.513327241 acc= [0.753]\n",
      "Epoch: 0273 cost= 0.513265550 acc= [0.7519]\n",
      "Epoch: 0274 cost= 0.512987196 acc= [0.7543]\n",
      "Epoch: 0275 cost= 0.512653291 acc= [0.7523]\n",
      "Epoch: 0276 cost= 0.511963964 acc= [0.7546]\n",
      "Epoch: 0277 cost= 0.511234701 acc= [0.7534]\n",
      "Epoch: 0278 cost= 0.510499954 acc= [0.7547]\n",
      "Epoch: 0279 cost= 0.509954572 acc= [0.7545]\n",
      "Epoch: 0280 cost= 0.509614527 acc= [0.7548]\n",
      "Epoch: 0281 cost= 0.509429276 acc= [0.7555]\n",
      "Epoch: 0282 cost= 0.509336650 acc= [0.7553]\n",
      "Epoch: 0283 cost= 0.509244561 acc= [0.7555]\n",
      "Epoch: 0284 cost= 0.509167552 acc= [0.7541]\n",
      "Epoch: 0285 cost= 0.508930087 acc= [0.7557]\n",
      "Epoch: 0286 cost= 0.508636057 acc= [0.7538]\n",
      "Epoch: 0287 cost= 0.508101165 acc= [0.7562]\n",
      "Epoch: 0288 cost= 0.507526517 acc= [0.7557]\n",
      "Epoch: 0289 cost= 0.506918252 acc= [0.758]\n",
      "Epoch: 0290 cost= 0.506418347 acc= [0.7577]\n",
      "Epoch: 0291 cost= 0.506049335 acc= [0.7584]\n",
      "Epoch: 0292 cost= 0.505797505 acc= [0.7588]\n",
      "Epoch: 0293 cost= 0.505625129 acc= [0.7585]\n",
      "Epoch: 0294 cost= 0.505487382 acc= [0.7582]\n",
      "Epoch: 0295 cost= 0.505381465 acc= [0.7586]\n",
      "Epoch: 0296 cost= 0.505236924 acc= [0.7574]\n",
      "Epoch: 0297 cost= 0.505102217 acc= [0.7586]\n",
      "Epoch: 0298 cost= 0.504840434 acc= [0.7572]\n",
      "Epoch: 0299 cost= 0.504546523 acc= [0.7585]\n",
      "Epoch: 0300 cost= 0.504095793 acc= [0.7584]\n",
      "Epoch: 0301 cost= 0.503627658 acc= [0.7597]\n",
      "Epoch: 0302 cost= 0.503128111 acc= [0.7596]\n",
      "Epoch: 0303 cost= 0.502690613 acc= [0.7595]\n",
      "Epoch: 0304 cost= 0.502324879 acc= [0.7607]\n",
      "Epoch: 0305 cost= 0.502033710 acc= [0.7608]\n",
      "Epoch: 0306 cost= 0.501800418 acc= [0.7605]\n",
      "Epoch: 0307 cost= 0.501606941 acc= [0.7605]\n",
      "Epoch: 0308 cost= 0.501446903 acc= [0.7593]\n",
      "Epoch: 0309 cost= 0.501305640 acc= [0.7613]\n",
      "Epoch: 0310 cost= 0.501204491 acc= [0.7604]\n",
      "Epoch: 0311 cost= 0.501101494 acc= [0.762]\n",
      "Epoch: 0312 cost= 0.501053691 acc= [0.7614]\n",
      "Epoch: 0313 cost= 0.500943542 acc= [0.7628]\n",
      "Epoch: 0314 cost= 0.500867248 acc= [0.7612]\n",
      "Epoch: 0315 cost= 0.500610948 acc= [0.7629]\n",
      "Epoch: 0316 cost= 0.500317752 acc= [0.7611]\n",
      "Epoch: 0317 cost= 0.499809176 acc= [0.7632]\n",
      "Epoch: 0318 cost= 0.499283165 acc= [0.7622]\n",
      "Epoch: 0319 cost= 0.498738974 acc= [0.7651]\n",
      "Epoch: 0320 cost= 0.498302013 acc= [0.7639]\n",
      "Epoch: 0321 cost= 0.497991711 acc= [0.7643]\n",
      "Epoch: 0322 cost= 0.497795999 acc= [0.7649]\n",
      "Epoch: 0323 cost= 0.497677743 acc= [0.7639]\n",
      "Epoch: 0324 cost= 0.497590542 acc= [0.7642]\n",
      "Epoch: 0325 cost= 0.497518897 acc= [0.7639]\n",
      "Epoch: 0326 cost= 0.497404188 acc= [0.7643]\n",
      "Epoch: 0327 cost= 0.497279972 acc= [0.7631]\n",
      "Epoch: 0328 cost= 0.497059971 acc= [0.765]\n",
      "Epoch: 0329 cost= 0.496816009 acc= [0.7631]\n",
      "Epoch: 0330 cost= 0.496473432 acc= [0.7653]\n",
      "Epoch: 0331 cost= 0.496120691 acc= [0.7641]\n",
      "Epoch: 0332 cost= 0.495737046 acc= [0.7663]\n",
      "Epoch: 0333 cost= 0.495382965 acc= [0.767]\n",
      "Epoch: 0334 cost= 0.495060652 acc= [0.7668]\n",
      "Epoch: 0335 cost= 0.494781733 acc= [0.767]\n",
      "Epoch: 0336 cost= 0.494539559 acc= [0.7676]\n",
      "Epoch: 0337 cost= 0.494325578 acc= [0.768]\n",
      "Epoch: 0338 cost= 0.494133353 acc= [0.7676]\n",
      "Epoch: 0339 cost= 0.493958682 acc= [0.7675]\n",
      "Epoch: 0340 cost= 0.493806005 acc= [0.7676]\n",
      "Epoch: 0341 cost= 0.493675977 acc= [0.7671]\n",
      "Epoch: 0342 cost= 0.493594229 acc= [0.767]\n",
      "Epoch: 0343 cost= 0.493556529 acc= [0.7667]\n",
      "Epoch: 0344 cost= 0.493629247 acc= [0.767]\n",
      "Epoch: 0345 cost= 0.493755460 acc= [0.7659]\n",
      "Epoch: 0346 cost= 0.494054675 acc= [0.767]\n",
      "Epoch: 0347 cost= 0.494221181 acc= [0.7655]\n",
      "Epoch: 0348 cost= 0.494381249 acc= [0.7656]\n",
      "Epoch: 0349 cost= 0.493884087 acc= [0.7651]\n",
      "Epoch: 0350 cost= 0.493102878 acc= [0.7669]\n",
      "Epoch: 0351 cost= 0.492003947 acc= [0.7682]\n",
      "Epoch: 0352 cost= 0.491215438 acc= [0.7704]\n",
      "Epoch: 0353 cost= 0.490938187 acc= [0.7706]\n",
      "Epoch: 0354 cost= 0.491081148 acc= [0.7695]\n",
      "Epoch: 0355 cost= 0.491366416 acc= [0.7697]\n",
      "Epoch: 0356 cost= 0.491424412 acc= [0.7675]\n",
      "Epoch: 0357 cost= 0.491203576 acc= [0.77]\n",
      "Epoch: 0358 cost= 0.490613490 acc= [0.769]\n",
      "Epoch: 0359 cost= 0.489992976 acc= [0.7718]\n",
      "Epoch: 0360 cost= 0.489537686 acc= [0.7717]\n",
      "Epoch: 0361 cost= 0.489354938 acc= [0.7715]\n",
      "Epoch: 0362 cost= 0.489365488 acc= [0.7733]\n",
      "Epoch: 0363 cost= 0.489397466 acc= [0.7704]\n",
      "Epoch: 0364 cost= 0.489338577 acc= [0.774]\n",
      "Epoch: 0365 cost= 0.489085108 acc= [0.7706]\n",
      "Epoch: 0366 cost= 0.488726944 acc= [0.7739]\n",
      "Epoch: 0367 cost= 0.488324314 acc= [0.7713]\n",
      "Epoch: 0368 cost= 0.487990528 acc= [0.773]\n",
      "Epoch: 0369 cost= 0.487762541 acc= [0.7729]\n",
      "Epoch: 0370 cost= 0.487626940 acc= [0.7728]\n",
      "Epoch: 0371 cost= 0.487537414 acc= [0.774]\n",
      "Epoch: 0372 cost= 0.487436861 acc= [0.7722]\n",
      "Epoch: 0373 cost= 0.487300187 acc= [0.774]\n",
      "Epoch: 0374 cost= 0.487097859 acc= [0.7729]\n",
      "Epoch: 0375 cost= 0.486857802 acc= [0.7744]\n",
      "Epoch: 0376 cost= 0.486584067 acc= [0.774]\n",
      "Epoch: 0377 cost= 0.486312270 acc= [0.7738]\n",
      "Epoch: 0378 cost= 0.486054212 acc= [0.7752]\n",
      "Epoch: 0379 cost= 0.485821277 acc= [0.7739]\n",
      "Epoch: 0380 cost= 0.485612601 acc= [0.7747]\n",
      "Epoch: 0381 cost= 0.485423595 acc= [0.7761]\n",
      "Epoch: 0382 cost= 0.485248446 acc= [0.7735]\n",
      "Epoch: 0383 cost= 0.485081345 acc= [0.775]\n",
      "Epoch: 0384 cost= 0.484920949 acc= [0.7738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0385 cost= 0.484762996 acc= [0.7752]\n",
      "Epoch: 0386 cost= 0.484613776 acc= [0.7737]\n",
      "Epoch: 0387 cost= 0.484467775 acc= [0.7757]\n",
      "Epoch: 0388 cost= 0.484340221 acc= [0.7737]\n",
      "Epoch: 0389 cost= 0.484219134 acc= [0.7753]\n",
      "Epoch: 0390 cost= 0.484133095 acc= [0.7739]\n",
      "Epoch: 0391 cost= 0.484053463 acc= [0.7753]\n",
      "Epoch: 0392 cost= 0.484027624 acc= [0.7738]\n",
      "Epoch: 0393 cost= 0.483984709 acc= [0.7757]\n",
      "Epoch: 0394 cost= 0.483995318 acc= [0.7739]\n",
      "Epoch: 0395 cost= 0.483905822 acc= [0.7765]\n",
      "Epoch: 0396 cost= 0.483815730 acc= [0.7747]\n",
      "Epoch: 0397 cost= 0.483515710 acc= [0.7766]\n",
      "Epoch: 0398 cost= 0.483158350 acc= [0.7748]\n",
      "Epoch: 0399 cost= 0.482643664 acc= [0.7773]\n",
      "Epoch: 0400 cost= 0.482151121 acc= [0.7766]\n",
      "Optimization Finished!\n",
      "CPU times: user 14.9 s, sys: 865 ms, total: 15.8 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.Session() as sess:\n",
    "    #tf.initialize_all_variables().run()\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    x_full = []\n",
    "    y_full = []\n",
    "    for i in range(num_examples):\n",
    "        example, label = sess.run([features, labels])\n",
    "        x_full.append(example)\n",
    "        \n",
    "        y_full.append(label)\n",
    "    x_full = normalize(np.array(x_full))\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "#         total_batch = int(num_examples/batch_size)\n",
    "        # Loop\n",
    "        \n",
    "        y_batch = np.array(y_full)\n",
    "        full = np.hstack([x_full, np.reshape(y_batch, (y_batch.shape[0], 1))])\n",
    "        if epoch == 1:\n",
    "            print(full.shape)\n",
    "        permuted_indices = np.random.choice(full.shape[0], full.shape[0], replace=False)\n",
    "        permuted = full[permuted_indices]\n",
    "        x_batch = permuted[:,:-1]\n",
    "        #print(np.vstack(x_batch[:1,:]))\n",
    "        y_batch = np.reshape(permuted[:,-1],(permuted[:,-1].shape[0], 1))\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c, batch_preds = sess.run([optimizer, cost, pred_onehot], feed_dict={x: x_batch,\n",
    "                                                      y: y_batch})\n",
    "        # Compute average loss\n",
    "        avg_cost = c\n",
    "        acc = sum(np.array(batch_preds)==np.array(y_batch))/(num_examples)\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost), \"acc=\", acc)\n",
    "    print (\"Optimization Finished!\")\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
