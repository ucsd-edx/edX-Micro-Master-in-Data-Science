{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PA9: Neural Networks with Tensorflow\n",
    "\n",
    "In this assignment, you will:\n",
    "\n",
    "1. Implement neural networks as a powerful approach to supervised machine learning,\n",
    "2. Practice using state-of-the-art software tools and programming paradigms for machine learning,\n",
    "3. Investigate the impact of parameters to learning on neural network performance as evaluated on an empirical data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will learn use a well known dataset:\n",
    "\n",
    "[Higgs](https://archive.ics.uci.edu/ml/datasets/HIGGS). Some information regarding this dataset: The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes.\n",
    "\n",
    "For local testing you will use the sample dataset provided to you with this notebook.\n",
    "When submitting on EdX, your code will be evaluated on a much larger sample of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file format for each of the above data set is as follows:\n",
    "\n",
    "• The first row contains a comma-separated list of the names of the label and attributes\n",
    "\n",
    "• Each successive row represents a single instance\n",
    "\n",
    "• The first entry of each instance is the label to be learned, and all other entries (following the commas) are attribute values.\n",
    "\n",
    "• All attributes are numerical i.e. real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: \n",
    "\n",
    "Your goal is to complete the below function named train_nn that behaves as follows:\n",
    "\n",
    "1) It should take as input six parameters:\n",
    "    \n",
    "    a. The path to a file containing a data set (e.g., higgs_sample.csv)\n",
    "    \n",
    "    b. The number of neurons to use in the hidden layer\n",
    "    \n",
    "    c. The learning rate to use during backpropagation\n",
    "    \n",
    "    d. The number of iterations to use during training\n",
    "    \n",
    "    e. The percentage of instances to use for a training set\n",
    "    \n",
    "    f. A random seed as an integer\n",
    "    \n",
    "    \n",
    "For example, if the call to train_nn looks like train_nn(higgs_sample.csv 20 0.001 1000 0.75 12345) which will create a neural network with 20 neurons in the hidden layer, train the network using a learning rate = 0.001 and 1000 iterations through higgs_sample.csv with a random seed of 12345, where 75% of the data will be used for training (and the remaining 25% will be used for testing)\n",
    "\n",
    "2) You should create a neural network in Tensorflow that will be learned from the training data. The key parameters to the architecture of the neural network are based on your inputted parameters and the size of your data set:\n",
    "    \n",
    "    a. The number of attributes in the input layer is the length of each instance’s\n",
    "    attribute list (which is the same for all instances)\n",
    "    \n",
    "    b. The number of neurons in a hidden layer will be inputted to the program as a\n",
    "    parameter. Each hidden neuron should use tf.sigmoid as its activation function.\n",
    "    \n",
    "    c. The number of output neurons will be 1 since it is a binary classification task, and that should use tf.sigmoid as its activation function\n",
    "    \n",
    "3) You should use different cost/loss functions that the network tries to minimize depending on the number of labels:\n",
    "    \n",
    "    a. For binary classification we will use the cross entropy loss function:\n",
    "    \n",
    "    TODO: get latex version from here: https://stackoverflow.com/questions/46291253/tensorflow-sigmoid-and-cross-entropy-vs-sigmoid-cross-entropy-with-logits\n",
    "    \n",
    "    b. You will use full batch gradient descent (No mini batching is required, but you may optionlly do it) TODO: change this if grading scheme does not conform to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: edit this to use cross entropy loss\n",
    "\n",
    "$$SSE(X) = \\sum_{j=1}^{n}({y_j - \\hat{y}_j})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    The function tf.reduce_sum will allow you to sum across all instances.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) For the implementation of Backpropagation, you should use tf.train.AdamOptimizer\n",
    "\n",
    "For more on optimizers, you may follow this link: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) You should train your network using your inputted learning rate and for the inputted number of iterations. The iterations are simply a loop that calls Backpropagation a fixed number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "\n",
    "- Biases?\n",
    "- Mean normalize?\n",
    "- How to evaluate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name = \"higgs_small.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking a the data\n",
    "input = pd.read_csv(file_name,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.529500</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>-0.016681</td>\n",
       "      <td>-0.003486</td>\n",
       "      <td>0.991385</td>\n",
       "      <td>-0.009822</td>\n",
       "      <td>0.992058</td>\n",
       "      <td>-0.001468</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>1.004939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>-0.014789</td>\n",
       "      <td>0.999142</td>\n",
       "      <td>1.029148</td>\n",
       "      <td>1.021455</td>\n",
       "      <td>1.050877</td>\n",
       "      <td>1.012534</td>\n",
       "      <td>0.967713</td>\n",
       "      <td>1.031224</td>\n",
       "      <td>0.957864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499154</td>\n",
       "      <td>0.574965</td>\n",
       "      <td>1.002943</td>\n",
       "      <td>1.010838</td>\n",
       "      <td>0.595460</td>\n",
       "      <td>1.004828</td>\n",
       "      <td>0.477408</td>\n",
       "      <td>1.004115</td>\n",
       "      <td>1.015907</td>\n",
       "      <td>1.026965</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005459</td>\n",
       "      <td>1.000998</td>\n",
       "      <td>1.396992</td>\n",
       "      <td>0.637225</td>\n",
       "      <td>0.369623</td>\n",
       "      <td>0.165939</td>\n",
       "      <td>0.404927</td>\n",
       "      <td>0.523195</td>\n",
       "      <td>0.366220</td>\n",
       "      <td>0.313337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275063</td>\n",
       "      <td>-2.425236</td>\n",
       "      <td>-1.742508</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>-1.743755</td>\n",
       "      <td>0.159488</td>\n",
       "      <td>-2.941008</td>\n",
       "      <td>-1.741237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.496432</td>\n",
       "      <td>-1.742136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172241</td>\n",
       "      <td>0.342467</td>\n",
       "      <td>0.461183</td>\n",
       "      <td>0.384411</td>\n",
       "      <td>0.080986</td>\n",
       "      <td>0.388779</td>\n",
       "      <td>0.444956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596061</td>\n",
       "      <td>-0.744166</td>\n",
       "      <td>-0.872486</td>\n",
       "      <td>0.571250</td>\n",
       "      <td>-0.885740</td>\n",
       "      <td>0.679817</td>\n",
       "      <td>-0.682789</td>\n",
       "      <td>-0.892627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708360</td>\n",
       "      <td>-0.885352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790251</td>\n",
       "      <td>0.847034</td>\n",
       "      <td>0.985821</td>\n",
       "      <td>0.768254</td>\n",
       "      <td>0.675036</td>\n",
       "      <td>0.822794</td>\n",
       "      <td>0.770775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.858770</td>\n",
       "      <td>-0.028786</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.886284</td>\n",
       "      <td>-0.019940</td>\n",
       "      <td>0.897522</td>\n",
       "      <td>-0.009928</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>1.086538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>-0.014137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894663</td>\n",
       "      <td>0.949071</td>\n",
       "      <td>0.989790</td>\n",
       "      <td>0.917218</td>\n",
       "      <td>0.869291</td>\n",
       "      <td>0.945466</td>\n",
       "      <td>0.871492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.248305</td>\n",
       "      <td>0.714839</td>\n",
       "      <td>0.881675</td>\n",
       "      <td>1.294626</td>\n",
       "      <td>0.856703</td>\n",
       "      <td>1.170305</td>\n",
       "      <td>0.680263</td>\n",
       "      <td>0.878984</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719931</td>\n",
       "      <td>0.846634</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.024671</td>\n",
       "      <td>1.079560</td>\n",
       "      <td>1.021323</td>\n",
       "      <td>1.145685</td>\n",
       "      <td>1.123704</td>\n",
       "      <td>1.132049</td>\n",
       "      <td>1.055831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.695388</td>\n",
       "      <td>2.429998</td>\n",
       "      <td>1.743236</td>\n",
       "      <td>5.824007</td>\n",
       "      <td>1.742818</td>\n",
       "      <td>7.064657</td>\n",
       "      <td>2.969674</td>\n",
       "      <td>1.741454</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>2.495511</td>\n",
       "      <td>1.742817</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>13.098125</td>\n",
       "      <td>7.391968</td>\n",
       "      <td>3.682260</td>\n",
       "      <td>6.583121</td>\n",
       "      <td>8.255083</td>\n",
       "      <td>4.749469</td>\n",
       "      <td>4.316365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.529500      0.997924     -0.016681     -0.003486      0.991385   \n",
       "std        0.499154      0.574965      1.002943      1.010838      0.595460   \n",
       "min        0.000000      0.275063     -2.425236     -1.742508      0.012355   \n",
       "25%        0.000000      0.596061     -0.744166     -0.872486      0.571250   \n",
       "50%        1.000000      0.858770     -0.028786      0.000643      0.886284   \n",
       "75%        1.000000      1.248305      0.714839      0.881675      1.294626   \n",
       "max        1.000000      6.695388      2.429998      1.743236      5.824007   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean      -0.009822      0.992058     -0.001468      0.003751      1.004939   \n",
       "std        1.004828      0.477408      1.004115      1.015907      1.026965   \n",
       "min       -1.743755      0.159488     -2.941008     -1.741237      0.000000   \n",
       "25%       -0.885740      0.679817     -0.682789     -0.892627      0.000000   \n",
       "50%       -0.019940      0.897522     -0.009928      0.020396      1.086538   \n",
       "75%        0.856703      1.170305      0.680263      0.878984      2.173076   \n",
       "max        1.742818      7.064657      2.969674      1.741454      2.173076   \n",
       "\n",
       "           ...                 19            20            21            22  \\\n",
       "count      ...       10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       ...           0.001222     -0.014789      0.999142      1.029148   \n",
       "std        ...           1.005459      1.000998      1.396992      0.637225   \n",
       "min        ...          -2.496432     -1.742136      0.000000      0.172241   \n",
       "25%        ...          -0.708360     -0.885352      0.000000      0.790251   \n",
       "50%        ...          -0.001294     -0.014137      0.000000      0.894663   \n",
       "75%        ...           0.719931      0.846634      3.101961      1.024671   \n",
       "max        ...           2.495511      1.742817      3.101961     13.098125   \n",
       "\n",
       "                 23            24            25            26            27  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       1.021455      1.050877      1.012534      0.967713      1.031224   \n",
       "std        0.369623      0.165939      0.404927      0.523195      0.366220   \n",
       "min        0.342467      0.461183      0.384411      0.080986      0.388779   \n",
       "25%        0.847034      0.985821      0.768254      0.675036      0.822794   \n",
       "50%        0.949071      0.989790      0.917218      0.869291      0.945466   \n",
       "75%        1.079560      1.021323      1.145685      1.123704      1.132049   \n",
       "max        7.391968      3.682260      6.583121      8.255083      4.749469   \n",
       "\n",
       "                 28  \n",
       "count  10000.000000  \n",
       "mean       0.957864  \n",
       "std        0.313337  \n",
       "min        0.444956  \n",
       "25%        0.770775  \n",
       "50%        0.871492  \n",
       "75%        1.055831  \n",
       "max        4.316365  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bogus as of now\n",
    "working_locally = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "filename_queue = tf.train.string_input_producer([file_name])\n",
    "\n",
    "\n",
    "line_reader = tf.TextLineReader()\n",
    "key, csv_row = line_reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_defaults = [[0.0]]*29\n",
    "all_columns = tf.decode_csv(csv_row, record_defaults=record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the features back into a tensor.\n",
    "features = tf.stack(all_columns[1:])\n",
    "labels = tf.stack(all_columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.05\n",
    "training_epochs = 200\n",
    "batch_size = 10000\n",
    "display_step = 1\n",
    "num_examples= 10000\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 8 # 1st layer number of features\n",
    "n_hidden_2 = 8 # 2nd layer number of features\n",
    "n_input = 28 \n",
    "n_classes = 1 \n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "    mean, std = train.mean(), train.std()\n",
    "    train = (train - mean) / std\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with SIGMOID activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden layer with SIGMOID activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    # Output layer with SIGMOID activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "#     out_layer_sigmoid = tf.nn.sigmoid(out_layer)\n",
    "#     return out_layer_sigmoid\n",
    "    return out_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# cost = tf.reduce_sum((y-pred)**2)\n",
    "\n",
    "# pred_onehot = tf.round(pred)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=pred)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "pred_onehot = tf.round(tf.nn.sigmoid(pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.741882443 acc= [0.5293]\n",
      "Epoch: 0002 cost= 0.706146300 acc= [0.4998]\n",
      "Epoch: 0003 cost= 0.724873781 acc= [0.4805]\n",
      "Epoch: 0004 cost= 0.715813696 acc= [0.4847]\n",
      "Epoch: 0005 cost= 0.699784935 acc= [0.505]\n",
      "Epoch: 0006 cost= 0.695649087 acc= [0.5229]\n",
      "Epoch: 0007 cost= 0.701150537 acc= [0.5295]\n",
      "Epoch: 0008 cost= 0.703196466 acc= [0.5298]\n",
      "Epoch: 0009 cost= 0.698196352 acc= [0.5308]\n",
      "Epoch: 0010 cost= 0.691985667 acc= [0.5305]\n",
      "Epoch: 0011 cost= 0.689982712 acc= [0.5365]\n",
      "Epoch: 0012 cost= 0.692115843 acc= [0.5245]\n",
      "Epoch: 0013 cost= 0.694351435 acc= [0.4968]\n",
      "Epoch: 0014 cost= 0.693728447 acc= [0.4978]\n",
      "Epoch: 0015 cost= 0.690812528 acc= [0.5293]\n",
      "Epoch: 0016 cost= 0.688159823 acc= [0.5436]\n",
      "Epoch: 0017 cost= 0.687606335 acc= [0.5409]\n",
      "Epoch: 0018 cost= 0.688722551 acc= [0.537]\n",
      "Epoch: 0019 cost= 0.689552546 acc= [0.5368]\n",
      "Epoch: 0020 cost= 0.688778698 acc= [0.5382]\n",
      "Epoch: 0021 cost= 0.686872959 acc= [0.5418]\n",
      "Epoch: 0022 cost= 0.685292065 acc= [0.549]\n",
      "Epoch: 0023 cost= 0.684923053 acc= [0.5503]\n",
      "Epoch: 0024 cost= 0.685320616 acc= [0.5527]\n",
      "Epoch: 0025 cost= 0.685342014 acc= [0.5558]\n",
      "Epoch: 0026 cost= 0.684381127 acc= [0.5561]\n",
      "Epoch: 0027 cost= 0.682886600 acc= [0.5561]\n",
      "Epoch: 0028 cost= 0.681752205 acc= [0.5616]\n",
      "Epoch: 0029 cost= 0.681342602 acc= [0.5621]\n",
      "Epoch: 0030 cost= 0.681161880 acc= [0.5628]\n",
      "Epoch: 0031 cost= 0.680491209 acc= [0.5646]\n",
      "Epoch: 0032 cost= 0.679223359 acc= [0.5676]\n",
      "Epoch: 0033 cost= 0.677911222 acc= [0.5728]\n",
      "Epoch: 0034 cost= 0.677052557 acc= [0.5713]\n",
      "Epoch: 0035 cost= 0.676503301 acc= [0.5684]\n",
      "Epoch: 0036 cost= 0.675716639 acc= [0.5713]\n",
      "Epoch: 0037 cost= 0.674475551 acc= [0.5737]\n",
      "Epoch: 0038 cost= 0.673136115 acc= [0.5781]\n",
      "Epoch: 0039 cost= 0.672101378 acc= [0.5832]\n",
      "Epoch: 0040 cost= 0.671274424 acc= [0.5876]\n",
      "Epoch: 0041 cost= 0.670239329 acc= [0.5879]\n",
      "Epoch: 0042 cost= 0.668889105 acc= [0.5916]\n",
      "Epoch: 0043 cost= 0.667547166 acc= [0.5883]\n",
      "Epoch: 0044 cost= 0.666436613 acc= [0.5886]\n",
      "Epoch: 0045 cost= 0.665344775 acc= [0.5912]\n",
      "Epoch: 0046 cost= 0.664012134 acc= [0.5938]\n",
      "Epoch: 0047 cost= 0.662568629 acc= [0.5963]\n",
      "Epoch: 0048 cost= 0.661279857 acc= [0.601]\n",
      "Epoch: 0049 cost= 0.660067201 acc= [0.6019]\n",
      "Epoch: 0050 cost= 0.658676386 acc= [0.6039]\n",
      "Epoch: 0051 cost= 0.657171488 acc= [0.6053]\n",
      "Epoch: 0052 cost= 0.655795097 acc= [0.6051]\n",
      "Epoch: 0053 cost= 0.654485166 acc= [0.6044]\n",
      "Epoch: 0054 cost= 0.653032422 acc= [0.6059]\n",
      "Epoch: 0055 cost= 0.651553333 acc= [0.6098]\n",
      "Epoch: 0056 cost= 0.650221407 acc= [0.6125]\n",
      "Epoch: 0057 cost= 0.648867071 acc= [0.6152]\n",
      "Epoch: 0058 cost= 0.647404909 acc= [0.6176]\n",
      "Epoch: 0059 cost= 0.646027923 acc= [0.6189]\n",
      "Epoch: 0060 cost= 0.644676089 acc= [0.6193]\n",
      "Epoch: 0061 cost= 0.643220603 acc= [0.6232]\n",
      "Epoch: 0062 cost= 0.641865671 acc= [0.6231]\n",
      "Epoch: 0063 cost= 0.640577197 acc= [0.625]\n",
      "Epoch: 0064 cost= 0.639232874 acc= [0.6291]\n",
      "Epoch: 0065 cost= 0.637997389 acc= [0.6293]\n",
      "Epoch: 0066 cost= 0.636752665 acc= [0.6303]\n",
      "Epoch: 0067 cost= 0.635498345 acc= [0.6327]\n",
      "Epoch: 0068 cost= 0.634346664 acc= [0.6372]\n",
      "Epoch: 0069 cost= 0.633172274 acc= [0.6379]\n",
      "Epoch: 0070 cost= 0.632094145 acc= [0.6369]\n",
      "Epoch: 0071 cost= 0.631013513 acc= [0.6388]\n",
      "Epoch: 0072 cost= 0.629936695 acc= [0.6397]\n",
      "Epoch: 0073 cost= 0.628866017 acc= [0.64]\n",
      "Epoch: 0074 cost= 0.627738357 acc= [0.6422]\n",
      "Epoch: 0075 cost= 0.626651406 acc= [0.6441]\n",
      "Epoch: 0076 cost= 0.625558794 acc= [0.6469]\n",
      "Epoch: 0077 cost= 0.624518692 acc= [0.6476]\n",
      "Epoch: 0078 cost= 0.623455465 acc= [0.6485]\n",
      "Epoch: 0079 cost= 0.622440338 acc= [0.6473]\n",
      "Epoch: 0080 cost= 0.621441364 acc= [0.6479]\n",
      "Epoch: 0081 cost= 0.620481670 acc= [0.65]\n",
      "Epoch: 0082 cost= 0.619518042 acc= [0.651]\n",
      "Epoch: 0083 cost= 0.618564248 acc= [0.6522]\n",
      "Epoch: 0084 cost= 0.617580295 acc= [0.6535]\n",
      "Epoch: 0085 cost= 0.616571367 acc= [0.6548]\n",
      "Epoch: 0086 cost= 0.615519166 acc= [0.6578]\n",
      "Epoch: 0087 cost= 0.614460886 acc= [0.6607]\n",
      "Epoch: 0088 cost= 0.613368869 acc= [0.6613]\n",
      "Epoch: 0089 cost= 0.612276196 acc= [0.6629]\n",
      "Epoch: 0090 cost= 0.611174583 acc= [0.6628]\n",
      "Epoch: 0091 cost= 0.610065997 acc= [0.666]\n",
      "Epoch: 0092 cost= 0.608955681 acc= [0.6681]\n",
      "Epoch: 0093 cost= 0.607833505 acc= [0.6705]\n",
      "Epoch: 0094 cost= 0.606694043 acc= [0.6713]\n",
      "Epoch: 0095 cost= 0.605550349 acc= [0.6733]\n",
      "Epoch: 0096 cost= 0.604399443 acc= [0.6751]\n",
      "Epoch: 0097 cost= 0.603251636 acc= [0.6743]\n",
      "Epoch: 0098 cost= 0.602138460 acc= [0.6752]\n",
      "Epoch: 0099 cost= 0.601059020 acc= [0.6788]\n",
      "Epoch: 0100 cost= 0.600017726 acc= [0.6805]\n",
      "Epoch: 0101 cost= 0.599015594 acc= [0.6845]\n",
      "Epoch: 0102 cost= 0.598067582 acc= [0.6857]\n",
      "Epoch: 0103 cost= 0.597232878 acc= [0.6863]\n",
      "Epoch: 0104 cost= 0.596606970 acc= [0.6849]\n",
      "Epoch: 0105 cost= 0.596050680 acc= [0.6888]\n",
      "Epoch: 0106 cost= 0.595149636 acc= [0.687]\n",
      "Epoch: 0107 cost= 0.594061613 acc= [0.6895]\n",
      "Epoch: 0108 cost= 0.593650699 acc= [0.6918]\n",
      "Epoch: 0109 cost= 0.593260944 acc= [0.6891]\n",
      "Epoch: 0110 cost= 0.592280090 acc= [0.6915]\n",
      "Epoch: 0111 cost= 0.591711879 acc= [0.6917]\n",
      "Epoch: 0112 cost= 0.591446519 acc= [0.6896]\n",
      "Epoch: 0113 cost= 0.590680957 acc= [0.6938]\n",
      "Epoch: 0114 cost= 0.590036035 acc= [0.6929]\n",
      "Epoch: 0115 cost= 0.589778244 acc= [0.691]\n",
      "Epoch: 0116 cost= 0.589287281 acc= [0.6935]\n",
      "Epoch: 0117 cost= 0.588681340 acc= [0.6938]\n",
      "Epoch: 0118 cost= 0.588373065 acc= [0.6943]\n",
      "Epoch: 0119 cost= 0.588121653 acc= [0.6948]\n",
      "Epoch: 0120 cost= 0.587682307 acc= [0.6949]\n",
      "Epoch: 0121 cost= 0.587290347 acc= [0.6959]\n",
      "Epoch: 0122 cost= 0.587096572 acc= [0.6949]\n",
      "Epoch: 0123 cost= 0.586864650 acc= [0.6966]\n",
      "Epoch: 0124 cost= 0.586487889 acc= [0.696]\n",
      "Epoch: 0125 cost= 0.586182415 acc= [0.6962]\n",
      "Epoch: 0126 cost= 0.586001635 acc= [0.6967]\n",
      "Epoch: 0127 cost= 0.585789859 acc= [0.6966]\n",
      "Epoch: 0128 cost= 0.585491240 acc= [0.6983]\n",
      "Epoch: 0129 cost= 0.585195899 acc= [0.6972]\n",
      "Epoch: 0130 cost= 0.584987521 acc= [0.6981]\n",
      "Epoch: 0131 cost= 0.584812820 acc= [0.6989]\n",
      "Epoch: 0132 cost= 0.584584475 acc= [0.6994]\n",
      "Epoch: 0133 cost= 0.584316611 acc= [0.6995]\n",
      "Epoch: 0134 cost= 0.584069490 acc= [0.6992]\n",
      "Epoch: 0135 cost= 0.583870232 acc= [0.6992]\n",
      "Epoch: 0136 cost= 0.583684206 acc= [0.6987]\n",
      "Epoch: 0137 cost= 0.583468616 acc= [0.6989]\n",
      "Epoch: 0138 cost= 0.583226442 acc= [0.6989]\n",
      "Epoch: 0139 cost= 0.582986891 acc= [0.6997]\n",
      "Epoch: 0140 cost= 0.582776845 acc= [0.699]\n",
      "Epoch: 0141 cost= 0.582593918 acc= [0.6994]\n",
      "Epoch: 0142 cost= 0.582417190 acc= [0.6995]\n",
      "Epoch: 0143 cost= 0.582231343 acc= [0.7]\n",
      "Epoch: 0144 cost= 0.582034647 acc= [0.6996]\n",
      "Epoch: 0145 cost= 0.581839621 acc= [0.7009]\n",
      "Epoch: 0146 cost= 0.581656218 acc= [0.7008]\n",
      "Epoch: 0147 cost= 0.581486344 acc= [0.7007]\n",
      "Epoch: 0148 cost= 0.581324339 acc= [0.6997]\n",
      "Epoch: 0149 cost= 0.581163108 acc= [0.701]\n",
      "Epoch: 0150 cost= 0.580998838 acc= [0.6996]\n",
      "Epoch: 0151 cost= 0.580829978 acc= [0.7009]\n",
      "Epoch: 0152 cost= 0.580661058 acc= [0.7002]\n",
      "Epoch: 0153 cost= 0.580493748 acc= [0.7012]\n",
      "Epoch: 0154 cost= 0.580329955 acc= [0.7015]\n",
      "Epoch: 0155 cost= 0.580168962 acc= [0.7016]\n",
      "Epoch: 0156 cost= 0.580009282 acc= [0.7023]\n",
      "Epoch: 0157 cost= 0.579849601 acc= [0.7014]\n",
      "Epoch: 0158 cost= 0.579689384 acc= [0.7015]\n",
      "Epoch: 0159 cost= 0.579529464 acc= [0.7017]\n",
      "Epoch: 0160 cost= 0.579370737 acc= [0.7015]\n",
      "Epoch: 0161 cost= 0.579215705 acc= [0.7017]\n",
      "Epoch: 0162 cost= 0.579065740 acc= [0.7022]\n",
      "Epoch: 0163 cost= 0.578926802 acc= [0.7006]\n",
      "Epoch: 0164 cost= 0.578797638 acc= [0.7025]\n",
      "Epoch: 0165 cost= 0.578690648 acc= [0.7003]\n",
      "Epoch: 0166 cost= 0.578589737 acc= [0.7022]\n",
      "Epoch: 0167 cost= 0.578509450 acc= [0.7008]\n",
      "Epoch: 0168 cost= 0.578381956 acc= [0.7026]\n",
      "Epoch: 0169 cost= 0.578220129 acc= [0.7011]\n",
      "Epoch: 0170 cost= 0.577968359 acc= [0.7028]\n",
      "Epoch: 0171 cost= 0.577712715 acc= [0.7017]\n",
      "Epoch: 0172 cost= 0.577505171 acc= [0.7031]\n",
      "Epoch: 0173 cost= 0.577375710 acc= [0.7035]\n",
      "Epoch: 0174 cost= 0.577296317 acc= [0.7018]\n",
      "Epoch: 0175 cost= 0.577214539 acc= [0.7035]\n",
      "Epoch: 0176 cost= 0.577099621 acc= [0.7023]\n",
      "Epoch: 0177 cost= 0.576920927 acc= [0.7041]\n",
      "Epoch: 0178 cost= 0.576715589 acc= [0.7038]\n",
      "Epoch: 0179 cost= 0.576516271 acc= [0.7048]\n",
      "Epoch: 0180 cost= 0.576357424 acc= [0.7044]\n",
      "Epoch: 0181 cost= 0.576237082 acc= [0.705]\n",
      "Epoch: 0182 cost= 0.576133072 acc= [0.7057]\n",
      "Epoch: 0183 cost= 0.576024234 acc= [0.7051]\n",
      "Epoch: 0184 cost= 0.575889051 acc= [0.706]\n",
      "Epoch: 0185 cost= 0.575733364 acc= [0.7049]\n",
      "Epoch: 0186 cost= 0.575558126 acc= [0.706]\n",
      "Epoch: 0187 cost= 0.575384498 acc= [0.7057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0188 cost= 0.575222671 acc= [0.7076]\n",
      "Epoch: 0189 cost= 0.575077534 acc= [0.7083]\n",
      "Epoch: 0190 cost= 0.574945748 acc= [0.7071]\n",
      "Epoch: 0191 cost= 0.574821651 acc= [0.7074]\n",
      "Epoch: 0192 cost= 0.574701488 acc= [0.7071]\n",
      "Epoch: 0193 cost= 0.574581444 acc= [0.7088]\n",
      "Epoch: 0194 cost= 0.574463546 acc= [0.7066]\n",
      "Epoch: 0195 cost= 0.574341834 acc= [0.7079]\n",
      "Epoch: 0196 cost= 0.574223161 acc= [0.7071]\n",
      "Epoch: 0197 cost= 0.574096501 acc= [0.7082]\n",
      "Epoch: 0198 cost= 0.573971689 acc= [0.7074]\n",
      "Epoch: 0199 cost= 0.573833168 acc= [0.7083]\n",
      "Epoch: 0200 cost= 0.573693454 acc= [0.7075]\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #tf.initialize_all_variables().run()\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "#         total_batch = int(num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in range(num_examples):\n",
    "            example, label = sess.run([features, labels])\n",
    "            x_batch.append(example)\n",
    "            y_batch.append(label)\n",
    "            \n",
    "        y_batch = np.reshape(np.array(y_batch),(num_examples,1))\n",
    "        x_batch = normalize(np.array(x_batch))\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c, batch_preds = sess.run([optimizer, cost, pred_onehot], feed_dict={x: x_batch,\n",
    "                                                      y: y_batch})\n",
    "        # Compute average loss\n",
    "        avg_cost = c\n",
    "        acc = sum(np.array(batch_preds)==np.array(y_batch))/(num_examples)\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost), \"acc=\", acc)\n",
    "    print (\"Optimization Finished!\")\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
