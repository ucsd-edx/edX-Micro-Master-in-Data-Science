{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson adapted from Aymeric Damian's excellent [Tensorflow tutorials](https://github.com/aymericdamien/TensorFlow-Examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks Using Estimators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![CNN](http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Parameters for our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-76e74a3f2450>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.25 # Dropout, probability to drop a unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 1** - Implement a ConvNet function using Layers API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 2** - Define model in terms of ConvNet function and input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 3** - Build Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 4** - Train, Evaluate, Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## ConvNet Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Input: \n",
    "- x_dict -- dict\n",
    "- n_classes -- int\n",
    "- dropout -- boolean\n",
    "- reuse -- boolean\n",
    "- is_training -- boolean\n",
    "\n",
    "Output: logits -- array of `n_classes` logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<font color=\"blue\">What is dropout?</font>\n",
    "\n",
    "Dropout is a technique developed in 2014 to \"learn better by learning less\". A dropout layer between two Dense layers would not permit the forward or backward flow of information with a pre-defined probability at training time. In other words, Dropout randomly drops neural units in the layers that sandwich the Dropout layer during training. The learned wisdom behind Dropout is that it aids in regularization by forcing the Neural network to learn without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Randomly drop neurons in layers surrounding a dropout layer when training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Aids in regularization by forcing NN to learn without overfitting\n",
    "\n",
    "<center><img src=\"../resources/img/Dropout.png\" alt=\"nn\" style=\"width: 400px;\"/>\n",
    "Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from\n",
    "overfitting”, JMLR 2014</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training and Testing with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Different DataFlow Graph for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Retain all weights learned during Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use all neurons while Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At test time, all neurons in the network are active. So the computational graph varies between the training and testing phase, however the weights applied at each neuron remain the same. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Inbuilt TensorFlow support for Dropout -- `reuse` flag in `variable_scope`:\n",
    "```\n",
    "with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "```\n",
    "\n",
    "Usually, each call to a model creating function, creates a new tensorflow graph. The `reuse` flag prevents this in the variable's scope and reuses all variables in that scope if already present in a tensorflow graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Resize Input to match picture format\n",
    "- Picture format = 28 x 28, 1 channel\n",
    "\n",
    "```python\n",
    "#If a component of shape is =1, the size of that dimension is computed so total size remains constant\n",
    "x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our input tensor has the shape `784 x batch_size`.\n",
    "\n",
    "We should reshape the input tensor so that the ConvNet can learn filters that are spatially aware. We'll reshape this input tensor to match the picture's format: `Height x Width x Channel`.\n",
    "\n",
    "```x = tf.reshape(x, shape=[-1, 28, 28, 1])```\n",
    " \n",
    "Tensor input is now 4-D: `Batch Size, Height, Width, Channel`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolution and Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../resources/img/ConvNet_Layers.png\" alt=\"nn\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2D Convolution**\n",
    "\n",
    "- Element wise multiplication of 2D filter as it slides across the length and breadth of input image  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2D Max Pooling**\n",
    "\n",
    "- Reduce the spatial size of input by subsampling every `n x n` slice of a Convolved 2D Tensor \n",
    "- Replace each slice with max value in the slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "ConvNets are built using the fundamental operations of Convolution and Pooling.\n",
    "\n",
    "The 2D Convolution operation takes a large input 2D Tensor and multiplies each element, the weight, in a smaller 2D window, canonically called the Convolutional Filter.\n",
    "\n",
    "The Pooling operation's function is to reduce the spatial size of the input image, reducing the number of parameters and avoiding overfitting. The most commonly used Pooling operation for images is a 2D MAX Pooling operation that downsamples every `n x n` slice of the Convolved Tensor by replacing the slice with the Maximum value in the `n x n` slice.\n",
    "\n",
    "For a more detailed, but probably the clearest writing about ConvNets, check out Andrej Karpathy's CSE231n lesson notes about the layers in ConvNets: http://cs231n.github.io/convolutional-networks/#pool   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TensorFlow provides support for Convolution and Pooling through the Layers API\n",
    "\n",
    "- Convolution:\n",
    "```\n",
    "conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "```\n",
    "Specifies a Convolutional layer with 32 filters, and shape `5 x 5`\n",
    "\n",
    "- Pooling:\n",
    "```\n",
    "conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "```\n",
    "Specifies a Max Pooling layer with a stride of 2, and shape `2 x 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we need to flatten the convolved and downsampled image into a 1 dimensional Tensor so that we can pass it through Dense layers and output an array of 10 logits.\n",
    "\n",
    "```python\n",
    "fc1 = tf.contrib.layers.flatten(conv2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lastly, we add Dense layers that sandwich a Dropout layer\n",
    "```python\n",
    "fc1 = tf.layers.dense(fc1, 1024)\n",
    "fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "out = tf.layers.dense(fc1, n_classes)\n",
    "```\n",
    "\n",
    "The `training` flag in the Dropout layer is a toggle to enable the layer when we are Training the model, and disable it at test time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Putting it all together,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input becomes 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer 2 with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our model function is very similar to the model function we wrote for the simple feed-forward Neural Network. Except that we use the ConvNet function to get our logits.\n",
    "\n",
    "We also need to call the ConvNet function in two different ways because Dropout has different behavior at training and testing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training:\n",
    "```python\n",
    "logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "```\n",
    "\n",
    "- Testing:\n",
    "```python\n",
    "logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp3pd3iblx\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp3pd3iblx', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f2cc80e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We still need an input function. Our input function is actually the same as when we built a feedforward neural network. This is the thing about the Estimator API. It makes the model independent of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_fn(mode):\n",
    "    # Two seperate branches because of the differences in how we deal with the input data\n",
    "    if mode == 'train':\n",
    "        # When training, shuffling is OK\n",
    "        return tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "            batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "    elif mode == \"evaluation\":\n",
    "        # When testing, don't shuffle\n",
    "        return tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "            batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_fn = get_input_fn('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp3pd3iblx/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3323598, step = 1\n",
      "INFO:tensorflow:global_step/sec: 11.5501\n",
      "INFO:tensorflow:loss = 0.113578126, step = 101 (8.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5978\n",
      "INFO:tensorflow:loss = 0.08898497, step = 201 (8.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6361\n",
      "INFO:tensorflow:loss = 0.04875037, step = 301 (8.595 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6093\n",
      "INFO:tensorflow:loss = 0.031833526, step = 401 (8.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5871\n",
      "INFO:tensorflow:loss = 0.074561805, step = 501 (8.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6206\n",
      "INFO:tensorflow:loss = 0.024791414, step = 601 (8.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5946\n",
      "INFO:tensorflow:loss = 0.020787725, step = 701 (8.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.636\n",
      "INFO:tensorflow:loss = 0.00441584, step = 801 (8.594 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5999\n",
      "INFO:tensorflow:loss = 0.03145544, step = 901 (8.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6291\n",
      "INFO:tensorflow:loss = 0.01248792, step = 1001 (8.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6215\n",
      "INFO:tensorflow:loss = 0.030297048, step = 1101 (8.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6206\n",
      "INFO:tensorflow:loss = 0.0064920066, step = 1201 (8.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6123\n",
      "INFO:tensorflow:loss = 0.01737084, step = 1301 (8.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6712\n",
      "INFO:tensorflow:loss = 0.03313896, step = 1401 (8.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6656\n",
      "INFO:tensorflow:loss = 0.02415792, step = 1501 (8.572 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6456\n",
      "INFO:tensorflow:loss = 0.0047726277, step = 1601 (8.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6525\n",
      "INFO:tensorflow:loss = 0.13795972, step = 1701 (8.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6325\n",
      "INFO:tensorflow:loss = 0.050592154, step = 1801 (8.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.638\n",
      "INFO:tensorflow:loss = 0.0044508483, step = 1901 (8.593 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmp3pd3iblx/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.011745476.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f8f2cc80c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_fn = get_input_fn('evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-26-08:17:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp3pd3iblx/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-26-08:17:46\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9903, global_step = 2000, loss = 0.03594563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9903, 'global_step': 2000, 'loss': 0.03594563}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp3pd3iblx/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Model predictions are  [1, 3]\n",
      "Model predictions are  [2, 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_images = 4\n",
    "offset = random.randint(0, 10000)\n",
    "actual_labels = mnist.test.images[offset:offset + n_images]\n",
    "predictions = list(model.predict(input_fn))[offset: offset + n_images]\n",
    "\n",
    "# Display\n",
    "f = plt.figure(figsize=(8,4))\n",
    "for i in range(n_images):\n",
    "    sp = f.add_subplot(2, 2, i+1)\n",
    "    sp.axis('Off')\n",
    "    plt.imshow(np.reshape(actual_labels[i], [28, 28]), cmap='gray')\n",
    "print('Model predictions are ' ,predictions[:2])\n",
    "print('Model predictions are ' ,predictions[2:4])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
