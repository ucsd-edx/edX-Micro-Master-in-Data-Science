{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Linear Regression using TensorFlow\n",
    "\n",
    "* This notebook is adapted from [Aymeric Damian's notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/Linear_regression.ipynb) \n",
    "\n",
    "* Clone the full collection from [here](https://github.com/aymericdamien/TensorFlow-Examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We are assuming that this is not the first time you are using Linear and logistic regression, if it isnt, \n",
    "we recommend you got back and review:\n",
    "* [linear regression](http://www.dataschool.io/linear-regression-in-python/)\n",
    "* [Logistic Regression](http://www.dataschool.io/guide-to-logistic-regression/)\n",
    "\n",
    "A logistic regression learning algorithm example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the others are considered to be dependent variables. For our example, we want to relate the variable Y to the variable X using a linear regression model. \n",
    "\n",
    "Specification of the model:\n",
    "$y$ = $b$ + $w_1$$x_1$ + ... +  $w_p$$x_p$\n",
    "- $y$ is the regressed variable\n",
    "- $w$'s are the weights\n",
    "- $b$ is the bias term\n",
    "- $x$'s are the features used to model y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For this example, we will use some toy data so that we can get a hang of how to build a model without worrying about how to manage the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "New TensorFlow concepts:\n",
    "1. Placeholders $\\sim$ input ports\n",
    "1. Variables $\\sim$ Variables\n",
    "1. Namescopes $\\sim$ subroutine name-spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### [Placeholders](https://www.tensorflow.org/api_guides/python/io_ops#Placeholders) \n",
    "Define entry points for training data. Similar to *ports* in computer systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the computational graph for linear regression with 1 explanatory variable\n",
    "# p = 1\n",
    "\n",
    "# Input to the graph\n",
    "y = tf.placeholder(dtype = tf.float32, name = 'InputData') # Placeholders - https://www.tensorflow.org/api_guides/python/io_ops#Placeholders\n",
    "x = tf.placeholder(dtype = tf.float32, name = 'LabelData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### [Variables](https://www.tensorflow.org/programmers_guide/variables)\n",
    "Similar to variables in a programming language such as python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameters are defined using variables\n",
    "# Variables retain their value even outside the bounds of a session's run call\n",
    "w = tf.Variable(initial_value = rng.randn(), name = \"weight\") \n",
    "b = tf.Variable(initial_value = rng.randn(), name = \"bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name_scope\n",
    "Defines a name-space which is used by a function. and defines a node in the TensorFlow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Connecting up the nodes in our linear model\n",
    "# y = b + Wx\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.add(b, tf.multiply(w, x))\n",
    "\n",
    "# prediction holds the tensor that is the output of the operation add which takes tensors b, and the output of the multiply operation between the weight w, and the input x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First we will build the computational graph for linear regression based on the algebraic equation that the model is defined by. We will use two new TensorFlow concepts, placeholders and variables, to build our graph. \n",
    "\n",
    "Placeholders are entry points into the graph allowing for training data to be passed into the graph.\n",
    "\n",
    "Variables are used to represent parameters of the graph which need to retain their value between runs (iterations) while training in a session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the Regression parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Defining loss for our model\n",
    "# Loss is the mean squared error between actual $y$ and predicted $y$\n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_sum( input_tensor = tf.pow(prediction-y, 2))/(2*n_samples)\n",
    "# reduce_sum is a function to compute the sum across dimensions of a tensor. In this case, the input tensor is a 1 x n_samples dimensional tensor of the prediction errors corresponding to the training samples  \n",
    "# https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "#Our previous definitions implicitly creates the relation between the loss and the variables w and b \n",
    "\n",
    "# We can use gradient descent to train our linear model\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_sum( input_tensor = tf.pow(prediction-y, 2))/(2*n_samples)\n",
    "\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The computational graph that we just defined:\n",
    "\n",
    "<center><img src=\"../../resources/img/LinearRegression.png\" alt=\"An example image of an ML model visualized using TensorBoard\" style=\"width:800px;\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# We also need an operation to initialize our global variables (w and b)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "3. reduce_sum operation: https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "4. Gradient descent optimizer: https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We now have a complete computational graph. Each run of the optimizer takes one sample of X and Y as input, makes a prediction. The optimizer updates the free variables in its loss function based on the prediction for that input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Executing the Computational Graph in a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training the computational graph\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for each epoch:\n",
    "        for each (training sample:sample_x, training label:sample_y):\n",
    "            sess.run(optimizer, feed_dict={x: sample_x, y: sample_y})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evaluating the computational graph to calculate training loss:\n",
    "```\n",
    "with tf.Session() as sess:\n",
    "    c = sess.run(loss, feed_dict={x:train_X, y:train_Y})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# We can now run the computational graph in a session\n",
    "training_epochs = 1000              # We will run our model 1000 times\n",
    "display_step = 1                 # Display the loss every 100 runs\n",
    "final_w, final_b = 0,0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (sample_x, sample_y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={x: sample_x, y: sample_y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c , summary= sess.run([loss, merged_summary_op], feed_dict={x: train_X, y:train_Y})\n",
    "            summary_writer.add_summary(summary, epoch)\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(c), \\\n",
    "                \"w=\", sess.run(w), \"b=\", sess.run(b))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    training_loss = sess.run(loss, feed_dict={x: train_X, y:train_Y})\n",
    "    print(\"Training loss=\", training_loss, \"w=\", sess.run(w), \"b=\", sess.run(b), '\\n')\n",
    "    final_w, final_b = sess.run(w), sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Graphic display\n",
    "plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "plt.plot(train_X, final_w * train_X + final_b, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) refers to a classifier that classifies an observation into one of two classes, and multinomial logistic regression is used when classifying into more than two classes, but the two terms are used interchangebly. We will look at an example where we want to classify handwritten digits into one of 10 classes: 0-9\n",
    "\n",
    "The logistic regression model works in a similar fashion to a linear regression model except that the final sum of the product between the weights and dependent variable is passed through a function that transforms the input to lie between 0 and 1. This function is called the logistic function, giving the model its name.\n",
    "\n",
    "We can create a logistic regressor in the same way as we created a linear regression computational graph.\n",
    "\n",
    "We will use the MNIST database of [handwritten digits](http://yann.lecun.com/exdb/mnist/) for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TensorFlow provides easy access to some sample data sets. We can access the mnist dataset a TensorFlow dataset that contains 60,000 training images and their corresponding labels as well as 10,000 testing images and their corresponding labels.\n",
    "- Each image is 28 pixels by 28 pixels\n",
    "- Each image represents a digit between 0 and 9\n",
    "- The labels are one-hot encoded => each label is a 1x10 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Logistic regression model is a classifier\n",
    "* Outputs a conditional probability: $p(\\ label\\ $| $\\ data)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "p(y=j | X) = \\frac{1}{Z}g(\\Theta_j \\cdot X +b_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T05:44:07.505094Z",
     "start_time": "2018-05-26T05:44:07.501534Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The prediction is given as \n",
    "\n",
    "$$ g(z) = \\frac{e^z}{e^z + e^{-z}}  = \\frac{1}{1+e^{-2z}}$$ \n",
    "\n",
    "is the [soft max](https://en.wikipedia.org/wiki/softmax_function) and it \"squashes\" the input variable $z$ to the range $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It squashes the output of the linear function $\\sum_{i=0}^{n} \\theta_{i} x_{i}$ to vary between zero and one.\n",
    "\n",
    "The sigmoid function is softmax function with just $K$ = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['plt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pylab\n",
    "%pylab inline\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "def logistic(x):\n",
    "    return (1 / (1 + np.exp(-2*x)))\n",
    "x = np.arange(-8, 8, 0.1)\n",
    "y = [logistic(f) for f in x]\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The logistic regression model is trained by finding the $\\theta$ that maximizes the conditional log likelihood:\n",
    "$$ LL(\\Theta) = \\sum_{j=1}^m \\log P(l_j | X_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Flow Diagram\n",
    "<img src=\"../resources/img/TensorFlow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Logistic Regression in Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "rng = np.random\n",
    "logs_path = 'logs/lesson1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-2f44eee0991e>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Processing Input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First we will build the **computational graph** for logistic regression.\n",
    "\n",
    "We will use two new TensorFlow concepts:\n",
    "\n",
    "* **Placeholders** are entry points into the graph allowing for training data to be passed into the graph.\n",
    "\n",
    "* **Variables** are used to represent parameters of the graph which need to retain their value between runs (iterations) while training in a session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# Placeholder1: flattened images of dimension 28*28 = 784\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputData\") \n",
    "# Placeholder2: one-hot encoded labels for the 10 classes\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\")\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For more information, see:\n",
    "\n",
    "1. **Placeholders**: https://www.tensorflow.org/api_guides/python/io_ops#Placeholders\n",
    "2. **Variables**: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Placeholders receive input\n",
    "    * Placeholder1: flattened images of dimension 28*28 = 784\n",
    "    * Placeholder2: one-hot encoded labels for the 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Allowing batch sizes that are not known a-priori\n",
    "\n",
    "* Specifying `None` as a dimension in a placeholder allows for variable batch sizes.\n",
    "* For example `[None, 784]` defines a sequence of vectors of dimension 784. The length of the sequence is not specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Variables change state\n",
    "    * `W` and `b` are the variables in logistic regression\n",
    "    * Initialized with random values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To quote [TensorFlow's programmer's guide](https://www.tensorflow.org/programmers_guide/variables):\n",
    ">A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    ">Variables are manipulated via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it. Unlike tf.Tensor objects, a tf.Variable exists outside the context of a single session.run call.\n",
    "\n",
    ">Internally, a tf.Variable stores a persistent tensor. Specific ops allow you to read and modify the values of this tensor. These modifications are visible across multiple tf.Sessions, so multiple workers can see the same values for a tf.Variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Each placeholder or variable is an edge in the TensorFlow computation graph\n",
    "    * Edges represent Tensors\n",
    "    * Tensors are n-dimensional arrays (Matrices are 2 dimensional tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Each operation on Tensors is a node in the TensorFlow graph\n",
    "    * Nodes take Tensors as input\n",
    "    * Return Tensors as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In particular, the operation in our code is:\n",
    "```python\n",
    "prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax\n",
    "```\n",
    "* it is a composition of:\n",
    "   * `tf.matmul(x, W))` : performs dot product between the input vector `x` and the weights vector `W`\n",
    "   * `tf.add(b, tf.matmul(x,W))` : returns the tensor sum between the tensors b and the output of the inner computation \n",
    "   * `tf.nn.softmax(A)` : applies the softmax function on each value of the input tensor (default is along the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T18:43:30.091679Z",
     "start_time": "2018-05-26T18:43:29.933793Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4245b1581df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lets run the code we just described\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inputFeatures\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# mnist data image of shape 28*28=784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"actualLabel\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 0-9 digits recognition => 10 classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Lets run the code we just described\n",
    "\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputFeatures\") # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\") # 0-9 digits recognition => 10 classes\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss and Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our model is complete, but our computational graph is not yet complete. To complete the computational graph, we need to define a <b>loss function</b> and an <b>optimization strategy</b> to allow for the training of the free variables, `b` and `W` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use **Cross-entropy loss** as our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cross-Entropy Loss for binary classification:\n",
    "\\begin{equation*}\n",
    "loss = −\\left(y\\log{p} + \\left(1−y\\right)\\log\\left(1−p\\right)\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "This is the cross-entropy loss per example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sample and when the classes are zero and one, the above loss reduces to:\n",
    "\n",
    "$$\n",
    "batch\\_loss = -\\sum_{n=1}^{batch\\_size}y*\\log\\left({1-p}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Tensorflow provides various inbuilt optimizers that allow for the optimization of objective functions. \n",
    "\n",
    "These inbuilt optimizers are mostly directed toward neural network optimization, but a user can specify their own optimization functions by extending a base class. \n",
    "\n",
    "The base class provides access to various methods that calculate the gradients at all points in our computational graph. \n",
    "\n",
    "However, for most industrial projects the set of optimizers provided by TensorFlow are sufficient. \n",
    "\n",
    "To optimize this linear regressor, we will use the inbuilt **Gradient Descent Optimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "with tf.name_scope('Loss'):\n",
    "    #Loss is the cross entropy loss between actual label and predicted value\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('SGD'):\n",
    "    #use gradient descent to train our linear model\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For more information, see:\n",
    "3. <b>reduce_sum</b> : https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "4. <b>Gradient Descent Optimizer</b> : https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Common loss functions are already implemented by TensorFlow\n",
    "    - ex. cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* `GradientDescentOptimizer` automatically computes the gradients of the function being optimized\n",
    "* Autograd - Big innovation that makes software like Tensorflow and PyTorch possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 5\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1))\n",
    "                          \n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "                          \n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We now have a complete computational graph. \n",
    "\n",
    "Each run of the optimizer takes a group of  Xs as input and makes a prediction. \n",
    "\n",
    "The prediction is compared against the inputted Ys to get the loss. \n",
    "\n",
    "The optimizer updates the free variables in its loss function based on the loss for that input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Executing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "TensorFlow uses the **`tf.Session`** class to represent a connection between the client program---typically a Python program, although a similar interface is available in other languages---and the C++ runtime. \n",
    "\n",
    "A `tf.Session` object provides access to devices in the local machine, and remote devices using the distributed TensorFlow runtime. \n",
    "\n",
    "It also caches information about your `tf.Graph` so that you can efficiently run the same computation multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use **Batch Gradient Descent** to optimize our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Training the computational graph in a session\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    #initialize all variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    for each epoch in range(training_epochs):\n",
    "        for each batch in range(total_batches):\n",
    "            # Runs the parts of the graph needed to return the variables in fetches\n",
    "            # using batch_xs and batch_ys as input\n",
    "            sess.run(fetches=[optimizer,loss], feed_dict={x: batch_xs, y: batch_ys})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "The `run` method runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every Operation and evaluate every Tensor in fetches, substituting the values in feed_dict for the corresponding input values\n",
    "\n",
    "Notice that we pass a batch of Xs and Ys to the feed_dict parameter when we run the optimizer in a session.\n",
    "\n",
    "The `feed_dict` parameter in the `run` function of a session accepts key, value entries where the value is a Python scalar, string, list, or numpy ndarray that can be converted to the same dtype as that tensor represented by the key. Additionally, if the key is a tf.placeholder, the shape of the value will be checked for compatibility with the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 loss= 0.465512747\n",
      "Epoch: 0010 loss= 0.392430843\n",
      "Epoch: 0015 loss= 0.362738007\n",
      "Epoch: 0020 loss= 0.345414007\n",
      "Epoch: 0025 loss= 0.333736597\n",
      "Optimization Finished!\n",
      "Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path + \"/logistic\", graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # there would be 600 batches\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_loss += c / total_batch\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # Test using the same session\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The advantages of Tensorflow lie in its extensibility and ease of prototyping more complex machine learning models easily.\n",
    "\n",
    "For example, we can modify the above logistic regression model into a regularized logistic regression model with the modification of the loss function to include the regularization term.  \n",
    "\n",
    "```python\n",
    "lamb = 0.01 #This is the hyperparameter that controls the strength of the regularization\n",
    "\n",
    "# Minimize error using cross entropy loss\n",
    "# reduce_mean calculates the mean across dimensions of a tensor\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1)  + lamb * (tf.nn.l2_loss(W) + tf.nn.l2_loss(b)))\n",
    "                     \n",
    "```\n",
    "We use an L2 regularizer by just applying TF's inbuilt L2 regularizer on the parameters of our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorboard to View Graph Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the computational graph that we have just defined on Tensorboard. We have installed a jupyter extension that makes connecting to Tensorboard very simple. To do this, \n",
    "\n",
    "In your Jupyter directory tree view, select the log directory for lesson 1 and click the <font color = \"red\">**Tensorboard**</font> button as shown in the picture.\n",
    "<img src = \"../resources/img/TensorboardInit1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, go to the <font color = \"red\">**Running**</font> tab, and choose the Tensorboard instance corresponding to the correct log directory as shown in the screenshot.\n",
    "<img src = \"../resources/img/TensorboardInit2.PNG\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
