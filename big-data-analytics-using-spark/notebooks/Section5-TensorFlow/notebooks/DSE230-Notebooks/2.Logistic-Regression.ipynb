{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression using TensorFlow\n",
    "\n",
    "* This notebook is adapted from [Aymeric Damian's logistic regression notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb) \n",
    "\n",
    "* Clone the full collection [here](https://github.com/aymericdamien/TensorFlow-Examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### multi-label logistic regression\n",
    "Is the next step after linear regression. \n",
    "* Like linear regression, there are only an input layer and and output layer.\n",
    "* Unlike linear regression, the relation of output to input is not linear.\n",
    "* Also, we have ten output nodes, instead of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) refers to a soft-classifier over $k$ classes based on a linear function of the input. We will look at an example where we want to classify handwritten digits into one of $k=10$ classes: $0-9$\n",
    "\n",
    "The logistic regression model works in a similar fashion to a linear regression model except that the final sum of the product between the weights and dependent variable is passed through a function that transforms the unbounded outputs of the linear operation into a normalized conditional probability over the $k$ classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The logistic model\n",
    "\n",
    "We use the logistic model as a classifier which maps each digit image to an integer number between 0 and 9 which corresponds to the identity of the digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The inputs (placeholders) are:\n",
    "\n",
    "* $X$ - a 784 dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $y$ - the label corresponding to $X$. Encoded using 1-hot encoding. I.e. **0**=(1,0,...0), **1**=(0,1,0,....,0) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are 10 sets of parameters, one for each digit $j=0,\\ldots,9$ :\n",
    "* $W_j$= a 784 dimensional vector\n",
    "* $b_j$= a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The logistic funcion defines a distribution over the digits. We predict with the digit with the highest probability.\n",
    "$$\n",
    "p(y=j | X) = g(s_j)\\;\\;\\mbox{ where }\\;\\; s_j=W_j \\cdot X +b_j $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T05:44:07.505094Z",
     "start_time": "2018-05-26T05:44:07.501534Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mbox{and  } g(s_j) = \\frac{\\exp(s_j)}{\\sum_{i=0}^9 \\exp(s_i)}\n",
    "$$is the [softmax function](https://en.wikipedia.org/wiki/softmax_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The cross-entropy cost\n",
    "As our model outputs a vector of 10 conditional probabilities we use the negative cross-entropy as the cost function: \n",
    "$$ Cost \\left(\\{W_j,b_j\\}_{j=0}^9\\right)\n",
    "=-\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=0}^n y^i_j \\log g(s^i_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Flow Diagram\n",
    "<img src=\"img/TensorFlow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coding Logistic Regression in Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T15:41:49.946464Z",
     "start_time": "2018-06-05T15:41:46.788871Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import warnings\n",
    "## Tensorflow produces a lot of warnings. We generally want to suppress them. The below code does exactly that. \n",
    "warnings.filterwarnings('ignore')\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "rng = np.random\n",
    "logs_path = 'logs/Logistic_regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T15:41:56.782891Z",
     "start_time": "2018-06-05T15:41:49.950627Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining the logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "# Placeholder1: flattened images of dimension 28*28 = 784\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputData\") \n",
    "# Placeholder2: one-hot encoded labels for the 10 classes\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\")\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The operation defined in out model is:\n",
    "```python\n",
    "prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax\n",
    "```\n",
    "* it is a composition of:\n",
    "   * `tf.matmul(x, W))` : performs dot product between the input vector `x` and the weights matrix `W`, yielding a vector of dimension 10.\n",
    "   * `tf.add(b, tf.matmul(x,W))` : returns the tensor sum between the tensors b and the output of the inner computation \n",
    "   * `tf.nn.softmax(A)` : applies the softmax function on each value of the input tensor (default is along the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T15:41:56.818448Z",
     "start_time": "2018-06-05T15:41:56.785222Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets run the code we just described\n",
    "\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputFeatures\") # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\") # 0-9 digits recognition => 10 classes\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adding a regularization term\n",
    "One way to reduce over-fitting is to add a **regularization term** to the loss. \n",
    "This term is also referred to as **weight decay** because it pushes the weights towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We use an L2 regularizer, given the weight vectors $W_j$ and the biases $b_j$ \n",
    "the regularization term is \n",
    "$$ l2\\left(\\{W_j,b_j\\}_{j=0}^9\\right) = \\sum_{j=0}^9  \\left[\\sum_{i=1}^{784} W_{ji}^2 +b_j^2\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T15:41:56.926238Z",
     "start_time": "2018-06-05T15:41:56.821218Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 5\n",
    "lamb = 0.01 #This is the hyperparameter that controls the strength of the regularization\n",
    "\n",
    "# Minimize error using cross entropy loss\n",
    "# reduce_mean calculates the mean across dimensions of a tensor\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1)  \n",
    "                      + lamb * (tf.nn.l2_loss(W) + tf.nn.l2_loss(b)))\n",
    "                     \n",
    "# Logging commands\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "                          \n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T15:41:56.932458Z",
     "start_time": "2018-06-05T15:41:56.928501Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Executing the optimization in a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T15:41:56.973231Z",
     "start_time": "2018-06-05T15:41:56.934557Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(logs_path + \"/logistic\", graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T15:42:12.645925Z",
     "start_time": "2018-06-05T15:41:56.975343Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 loss= 0.561818586\n",
      "Epoch: 0010 loss= 0.561724161\n",
      "Epoch: 0015 loss= 0.561663737\n",
      "Epoch: 0020 loss= 0.561848054\n",
      "Epoch: 0025 loss= 0.562010598\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size) # there would be 600 batches\n",
    "\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, c = sess.run([optimizer, loss], feed_dict={x: batch_xs,\n",
    "                                                      y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_loss += c / total_batch\n",
    "\n",
    "    # Display logs per epoch step\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Optional excercise\n",
    "* Add to the log print line the two components of the loss: the entropy loss and the regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T15:42:12.686216Z",
     "start_time": "2018-06-05T15:42:12.647947Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87766665\n"
     ]
    }
   ],
   "source": [
    "# Calculate test set accuracy, i.e. number of mistakes final model makes on test set\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "# Calculate accuracy for 3000 examples\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]},session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Tensorboard to View Graph Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can have a look at the computational graph that we have just defined on Tensorboard. We have installed a jupyter extension that makes connecting to Tensorboard very simple. To do this, \n",
    "\n",
    "In your Jupyter directory tree view, select the log directory for lesson 1 and click the <font color = \"red\">**Tensorboard**</font> button as shown in the picture.\n",
    "<img src = \"img/TensorboardInit1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, go to the <font color = \"red\">**Running**</font> tab, and choose the Tensorboard instance corresponding to the correct log directory as shown in the screenshot.\n",
    "<img src = \"img/TensorboardInit2.PNG\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
