{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression using TensorFlow\n",
    "\n",
    "* This notebook is adapted from [Aymeric Damian's logistic regression notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb) \n",
    "\n",
    "* Clone the full collection [here](https://github.com/aymericdamien/TensorFlow-Examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### multi-label logistic regression\n",
    "Is the next step after linear regression. \n",
    "* Like linear regression, there are only an input layer and and output layer.\n",
    "* Unlike linear regression, the relation of output to input is not linear.\n",
    "* Also, we have ten output nodes, instead of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) refers to a soft-classifier over $k$ classes based on a linear function of the input. We will look at an example where we want to classify handwritten digits into one of $k=10$ classes: $0-9$\n",
    "\n",
    "The logistic regression model works in a similar fashion to a linear regression model except that the final sum of the product between the weights and dependent variable is passed through a function that transforms the unbounded outputs of the linear operation into a normalized conditional probability over the $k$ classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The logistic model\n",
    "\n",
    "We use the logistic model as a classifier which maps each digit image to an integer number between 0 and 9 which corresponds to the identity of the digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The inputs (placeholders) are:\n",
    "\n",
    "* $X$ - a 784 dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $y$ - the label corresponding to $X$. Encoded using 1-hot encoding. I.e. **0**=(1,0,...0), **1**=(0,1,0,....,0) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are 10 sets of parameters, one for each digit $j=0,\\ldots,9$ :\n",
    "* $W_j$= a 784 dimensional vector\n",
    "* $b_j$= a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The logistic funcion defines a distribution over the digits. We predict with the digit with the highest probability.\n",
    "$$\n",
    "p(y=j | X) = g(s_j)\\;\\;\\mbox{ where }\\;\\; s_j=W_j \\cdot X +b_j $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T05:44:07.505094Z",
     "start_time": "2018-05-26T05:44:07.501534Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mbox{and  } g(s_j) = \\frac{\\exp(s_j)}{\\sum_{i=0}^9 \\exp(s_i)}\n",
    "$$is the [softmax function](https://en.wikipedia.org/wiki/softmax_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The cross-entropy cost\n",
    "As our model outputs a vector of 10 conditional probabilities we use the negative cross-entropy as the cost function: \n",
    "$$ Cost \\left(\\{W_j,b_j\\}_{j=0}^9\\right)\n",
    "=-\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=0}^n y^i_j \\log g(s^i_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Flow Diagram\n",
    "<img src=\"img/TensorFlow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coding Logistic Regression in Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T18:56:21.579499Z",
     "start_time": "2018-05-29T18:56:20.062245Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "rng = np.random\n",
    "logs_path = 'logs/Logistic_regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T18:56:28.527211Z",
     "start_time": "2018-05-29T18:56:21.581551Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-2f44eee0991e>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining the logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "# Placeholder1: flattened images of dimension 28*28 = 784\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputData\") \n",
    "# Placeholder2: one-hot encoded labels for the 10 classes\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\")\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The operation defined in out model is:\n",
    "```python\n",
    "prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax\n",
    "```\n",
    "* it is a composition of:\n",
    "   * `tf.matmul(x, W))` : performs dot product between the input vector `x` and the weights vector `W`\n",
    "   * `tf.add(b, tf.matmul(x,W))` : returns the tensor sum between the tensors b and the output of the inner computation \n",
    "   * `tf.nn.softmax(A)` : applies the softmax function on each value of the input tensor (default is along the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T18:56:28.554894Z",
     "start_time": "2018-05-29T18:56:28.529756Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets run the code we just described\n",
    "\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputFeatures\") # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\") # 0-9 digits recognition => 10 classes\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T18:56:28.631288Z",
     "start_time": "2018-05-29T18:56:28.556787Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 5\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1))\n",
    "                          \n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "                          \n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T18:56:28.638634Z",
     "start_time": "2018-05-29T18:56:28.633792Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Executing the optimization in a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T18:56:46.175892Z",
     "start_time": "2018-05-29T18:56:28.641981Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 loss= 0.465426942\n",
      "Epoch: 0010 loss= 0.392385709\n",
      "Epoch: 0015 loss= 0.362727999\n",
      "Epoch: 0020 loss= 0.345417339\n",
      "Epoch: 0025 loss= 0.333688519\n",
      "Optimization Finished!\n",
      "Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path + \"/logistic\", graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # there would be 600 batches\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_loss += c / total_batch\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # Test using the same session\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Adding regularization\n",
    "It is easy to extend Tensorflow to use more complex machine learning models.\n",
    "\n",
    "For example, we can modify the above logistic regression model into a regularized logistic regression model with the modification of the loss function to include the regularization term.  \n",
    "\n",
    "```python\n",
    "lamb = 0.01 #This is the hyperparameter that controls the strength of the regularization\n",
    "\n",
    "# Minimize error using cross entropy loss\n",
    "# reduce_mean calculates the mean across dimensions of a tensor\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1)  + lamb * (tf.nn.l2_loss(W) + tf.nn.l2_loss(b)))\n",
    "                     \n",
    "```\n",
    "We use an L2 regularizer by just applying TF's inbuilt L2 regularizer on the parameters of our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Tensorboard to View Graph Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can have a look at the computational graph that we have just defined on Tensorboard. We have installed a jupyter extension that makes connecting to Tensorboard very simple. To do this, \n",
    "\n",
    "In your Jupyter directory tree view, select the log directory for lesson 1 and click the <font color = \"red\">**Tensorboard**</font> button as shown in the picture.\n",
    "<img src = \"img/TensorboardInit1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, go to the <font color = \"red\">**Running**</font> tab, and choose the Tensorboard instance corresponding to the correct log directory as shown in the screenshot.\n",
    "<img src = \"img/TensorboardInit2.PNG\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
