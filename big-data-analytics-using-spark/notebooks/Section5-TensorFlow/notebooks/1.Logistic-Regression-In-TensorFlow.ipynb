{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) refers to a classifier that classifies an observation into one of two classes, and multinomial logistic regression is used when classifying into more than two classes, but the two terms are used interchangebly. We will look at an example where we want to classify handwritten digits into one of 10 classes: 0-9\n",
    "\n",
    "The logistic regression model works in a similar fashion to a linear regression model except that the final sum of the product between the weights and dependent variable is passed through a function that transforms the input to lie between 0 and 1. This function is called the logistic function, giving the model its name.\n",
    "\n",
    "We can create a logistic regressor in the same way as we created a linear regression computational graph.\n",
    "\n",
    "We will use the MNIST database of [handwritten digits](http://yann.lecun.com/exdb/mnist/) for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TensorFlow provides easy access to some sample data sets. We can access the mnist dataset a TensorFlow dataset that contains 60,000 training images and their corresponding labels as well as 10,000 testing images and their corresponding labels.\n",
    "- Each image is 28 pixels by 28 pixels\n",
    "- Each image represents a digit between 0 and 9\n",
    "- The labels are one-hot encoded => each label is a 1x10 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Logistic regrssion model is a classifier\n",
    "* Outputs a conditional probability: $p(\\ label\\ $| $\\ data)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "y = g(\\Theta \\cdot X) = g\\left( \\sum_{i=0}^{n} \\theta_{i} x_{i} \\right) \\text{ where } x_0 = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "g is the <a href = \"https://en.wikipedia.org/wiki/Softmax_function\"> softmax function</a>\n",
    "$$\\sigma\\left(z\\right)_j = \\frac{e^{z_j}}{\\sum_{k=1}^K{e^{z_k}}}$$ \n",
    "where $K$ is the dimension of the real valued vector $z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It squashes the output of the linear function $\\sum_{i=0}^{n} \\theta_{i} x_{i}$ to vary between zero and one.\n",
    "\n",
    "The sigmoid function is softmax function with just $K$ = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pylab\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "x = np.arange(-8, 8, 0.1)\n",
    "y = [sigmoid(f) for f in x]\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The logistic regression model is trained by finding the $\\theta$ that maximizes the conditional log likelihood:\n",
    "$$ LL(\\Theta) = \\sum_{j=1}^m \\log g(\\Theta \\cdot X_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Flow Diagram\n",
    "<img src=\"../resources/img/TensorFlow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Logistic Regression in Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "rng = np.random\n",
    "logs_path = 'logs/lesson1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-2f44eee0991e>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Processing Input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First we will build the **computational graph** for logistic regression.\n",
    "\n",
    "We will use two new TensorFlow concepts:\n",
    "\n",
    "* **Placeholders** are entry points into the graph allowing for training data to be passed into the graph.\n",
    "\n",
    "* **Variables** are used to represent parameters of the graph which need to retain their value between runs (iterations) while training in a session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# Placeholder1: flattened images of dimension 28*28 = 784\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputData\") \n",
    "# Placeholder2: one-hot encoded labels for the 10 classes\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\")\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For more information, see:\n",
    "\n",
    "1. **Placeholders**: https://www.tensorflow.org/api_guides/python/io_ops#Placeholders\n",
    "2. **Variables**: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Placeholders receive input\n",
    "    * Placeholder1: flattened images of dimension 28*28 = 784\n",
    "    * Placeholder2: one-hot encoded labels for the 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Allowing batch sizes that are not known a-priori\n",
    "\n",
    "* Specifying `None` as a dimension in a placeholder allows for variable batch sizes.\n",
    "* For example `[None, 784]` defines a sequence of vectors of dimension 784. The length of the sequence is not specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Variables change state\n",
    "    * `W` and `b` are the variables in logistic regression\n",
    "    * Initialized with random values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To quote [TensorFlow's programmer's guide](https://www.tensorflow.org/programmers_guide/variables):\n",
    ">A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    ">Variables are manipulated via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it. Unlike tf.Tensor objects, a tf.Variable exists outside the context of a single session.run call.\n",
    "\n",
    ">Internally, a tf.Variable stores a persistent tensor. Specific ops allow you to read and modify the values of this tensor. These modifications are visible across multiple tf.Sessions, so multiple workers can see the same values for a tf.Variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Each placeholder or variable is an edge in the TensorFlow computation graph\n",
    "    * Edges represent Tensors\n",
    "    * Tensors are n-dimensional arrays (Matrices are 2 dimensional tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Each operation on Tensors is a node in the TensorFlow graph\n",
    "    * Nodes take Tensors as input\n",
    "    * Return Tensors as output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In particular, the operation in our code is:\n",
    "```python\n",
    "prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax\n",
    "```\n",
    "* it is a composition of:\n",
    "   * `tf.matmul(x, W))` : performs dot product between the input vector `x` and the weights vector `W`\n",
    "   * `tf.add(b, tf.matmul(x,W))` : returns the tensor sum between the tensors b and the output of the inner computation \n",
    "   * `tf.nn.softmax(A)` : applies the softmax function on each value of the input tensor (default is along the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets run the code we just described\n",
    "\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputFeatures\") # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\") # 0-9 digits recognition => 10 classes\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss and Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our model is complete, but our computational graph is not yet complete. To complete the computational graph, we need to define a <b>loss function</b> and an <b>optimization strategy</b> to allow for the training of the free variables, `b` and `W` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use **Cross-entropy loss** as our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Cross-Entropy Loss for binary classification:\n",
    "\\begin{equation*}\n",
    "loss = −\\left(y\\log{p} + \\left(1−y\\right)\\log\\left(1−p\\right)\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "This is the cross-entropy loss per example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sample and when the classes are zero and one, the above loss reduces to:\n",
    "\n",
    "$$\n",
    "batch\\_loss = -\\sum_{n=1}^{batch\\_size}y*\\log\\left({1-p}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Tensorflow provides various inbuilt optimizers that allow for the optimization of objective functions. \n",
    "\n",
    "These inbuilt optimizers are mostly directed toward neural network optimization, but a user can specify their own optimization functions by extending a base class. \n",
    "\n",
    "The base class provides access to various methods that calculate the gradients at all points in our computational graph. \n",
    "\n",
    "However, for most industrial projects the set of optimizers provided by TensorFlow are sufficient. \n",
    "\n",
    "To optimize this linear regressor, we will use the inbuilt **Gradient Descent Optimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "3. <b>reduce_sum</b> : https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "4. <b>Gradient Descent Optimizer</b> : https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "with tf.name_scope('Loss'):\n",
    "    #Loss is the cross entropy loss between actual label and predicted value\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('SGD'):\n",
    "    #use gradient descent to train our linear model\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Most common loss functions are already implemented by TensorFlow\n",
    "    - ex. cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* `GradientDescentOptimizer` automatically computes the gradients of the function being optimized\n",
    "* Autograd - Big innovation that makes software like Tensorflow and PyTorch possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 5\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1))\n",
    "                          \n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "                          \n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We now have a complete computational graph. \n",
    "\n",
    "Each run of the optimizer takes a group of  Xs as input and makes a prediction. \n",
    "\n",
    "The prediction is compared against the inputted Ys to get the loss. \n",
    "\n",
    "The optimizer updates the free variables in its loss function based on the loss for that input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Executing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "TensorFlow uses the **`tf.Session`** class to represent a connection between the client program---typically a Python program, although a similar interface is available in other languages---and the C++ runtime. \n",
    "\n",
    "A `tf.Session` object provides access to devices in the local machine, and remote devices using the distributed TensorFlow runtime. \n",
    "\n",
    "It also caches information about your `tf.Graph` so that you can efficiently run the same computation multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use **Batch Gradient Descent** to optimize our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Training the computational graph in a session\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    #initialize all variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    for each epoch in range(training_epochs):\n",
    "        for each batch in range(total_batches):\n",
    "            # Runs the parts of the graph needed to return the variables in fetches\n",
    "            # using batch_xs and batch_ys as input\n",
    "            sess.run(fetches=[optimizer,loss], feed_dict={x: batch_xs, y: batch_ys})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "The `run` method runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every Operation and evaluate every Tensor in fetches, substituting the values in feed_dict for the corresponding input values\n",
    "\n",
    "Notice that we pass a batch of Xs and Ys to the feed_dict parameter when we run the optimizer in a session.\n",
    "\n",
    "The `feed_dict` parameter in the `run` function of a session accepts key, value entries where the value is a Python scalar, string, list, or numpy ndarray that can be converted to the same dtype as that tensor represented by the key. Additionally, if the key is a tf.placeholder, the shape of the value will be checked for compatibility with the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 loss= 0.465512747\n",
      "Epoch: 0010 loss= 0.392430843\n",
      "Epoch: 0015 loss= 0.362738007\n",
      "Epoch: 0020 loss= 0.345414007\n",
      "Epoch: 0025 loss= 0.333736597\n",
      "Optimization Finished!\n",
      "Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path + \"/logistic\", graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # there would be 600 batches\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_loss += c / total_batch\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # Test using the same session\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The advantages of Tensorflow lie in its extensibility and ease of prototyping more complex machine learning models easily.\n",
    "\n",
    "For example, we can modify the above logistic regression model into a regularized logistic regression model with the modification of the loss function to include the regularization term.  \n",
    "\n",
    "```python\n",
    "lamb = 0.01 #This is the hyperparameter that controls the strength of the regularization\n",
    "\n",
    "# Minimize error using cross entropy loss\n",
    "# reduce_mean calculates the mean across dimensions of a tensor\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1)  + lamb * (tf.nn.l2_loss(W) + tf.nn.l2_loss(b)))\n",
    "                     \n",
    "```\n",
    "We use an L2 regularizer by just applying TF's inbuilt L2 regularizer on the parameters of our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorboard to View Graph Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the computational graph that we have just defined on Tensorboard. We have installed a jupyter extension that makes connecting to Tensorboard very simple. To do this, \n",
    "\n",
    "In your Jupyter directory tree view, select the log directory for lesson 1 and click the <font color = \"red\">**Tensorboard**</font> button as shown in the picture.\n",
    "<img src = \"../resources/img/TensorboardInit1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, go to the <font color = \"red\">**Running**</font> tab, and choose the Tensorboard instance corresponding to the correct log directory as shown in the screenshot.\n",
    "<img src = \"../resources/img/TensorboardInit2.PNG\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
