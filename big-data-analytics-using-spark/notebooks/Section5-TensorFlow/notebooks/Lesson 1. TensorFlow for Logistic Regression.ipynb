{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Logistic regression refers to a classifier that classifies an observation into one of two classes, and multinomial logistic regression is used when classifying into more than two classes, but the two terms are used interchangebly. We will look at an example where we want to classify handwritten digits into one of 10 classes: 0-9\n",
    "\n",
    "The logistic regression model works in a similar fashion to a linear regression model except that the final sum of the product between the weights and dependent variable is passed through a function that transforms the input to lie between 0 and 1. This function is called the logistic function, giving the model its name.\n",
    "\n",
    "We can create a logistic regressor in the same way as we created a linear regression computational graph.\n",
    "\n",
    "We will use the MNIST database of <a href=\"http://yann.lecun.com/exdb/mnist/\"> handwritten digits</a> for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TensorFlow provides easy access to some sample data sets. We can access the mnist dataset a TensorFlow dataset that contains 60,000 training images and their corresponding labels as well as 10,000 testing images and their corresponding labels.\n",
    "- Each image is 28 pixels by 28 pixels\n",
    "- Each image represents a digit between 0 and 9\n",
    "- The labels are one-hot encoded => each label is a 1x10 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Logistic regrssion model is a classifier\n",
    "* Aims to model  $p(\\ label\\ $| $\\ data)$\n",
    "* Train the classifier to predict $Y_i = 1\\  if\\  \\theta \\cdot X > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{equation*}\n",
    "y = g(\\Theta^{T} X) = g(\\sum_{i=0}^{n} \\theta_{i} x_{i}) \\text{ where } x_0 = 1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- g is the <a href = \"https://en.wikipedia.org/wiki/Softmax_function\"> softmax function</a>\n",
    "    - It squashes all dimensions of a vector input to lie between 0-1\n",
    "    - Ensures that the sum of the magnitudes of each dimension is 1\n",
    "\n",
    "- The logistic regression model is trained by finding the $\\theta$ that minimizes a cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../resources/img/LogRegDataflow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression in Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "rng = np.random\n",
    "logs_path = 'logs/lesson1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-2f44eee0991e>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Processing Input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "First we will build the <font color=\"red\"><b>computational graph</b></font> for linear regression based on the algebraic equation that the model is defined by. We will use two new TensorFlow concepts, <font color=\"red\">placeholders and variables</font>, to build our graph. \n",
    "\n",
    "<font color=\"red\">**Placeholders**</font> are entry points into the graph allowing for training data to be passed into the graph.\n",
    "\n",
    "<font color=\"red\">**Placeholders**</font> are used to represent parameters of the graph which need to retain their value between runs (iterations) while training in a session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputData\") \n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\")\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "New concepts:\n",
    "1. <font color=\"red\">**Placeholders**</font>: https://www.tensorflow.org/api_guides/python/io_ops#Placeholders\n",
    "2. <font color=\"red\">**Placeholders**</font>: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Placeholders receive input\n",
    "    * flattened images of dimension 28*28 = 784\n",
    "    * one-hot encoded labels for the 10 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Specifying `None` as a dimension in a placeholder allows for variable batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To quote [TensorFlow's programmer's guide](https://www.tensorflow.org/programmers_guide/variables):\n",
    ">A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    ">Variables are manipulated via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it. Unlike tf.Tensor objects, a tf.Variable exists outside the context of a single session.run call.\n",
    "\n",
    ">Internally, a tf.Variable stores a persistent tensor. Specific ops allow you to read and modify the values of this tensor. These modifications are visible across multiple tf.Sessions, so multiple workers can see the same values for a tf.Variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Variables change state\n",
    "    * W and b\n",
    "    * set to have random initial values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Each placeholder or variable is an edge in the TensorFlow computation graph\n",
    "    * Edges represent Tensors\n",
    "    * Tensors are n-dimensional arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Each operation on Tensors is a node in the TensorFlow graph\n",
    "    * Nodes take tensors as input\n",
    "    * Return tensors as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = \"inputFeatures\") # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = \"actualLabel\") # 0-9 digits recognition => 10 classes\n",
    "\n",
    "W = tf.Variable(initial_value = tf.zeros([784, 10]), name = \"weight\")\n",
    "b = tf.Variable(initial_value = tf.zeros([10]), name = \"bias\")\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    prediction = tf.nn.softmax(tf.add(b, tf.matmul(x, W))) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss and Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our model is complete, but our computational graph is not yet complete. To complete the computational graph, we need to define a <font color=\"blue\"><b>loss function</b></font> and an <font color=\"blue\"><b>optimization strategy</b></font> to allow for the training of the free variables, $b$ and $w$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use <font color=\"blue\"><b>Mean Squared Error</b></font> as our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* MSE Loss\n",
    "\\begin{equation*}\n",
    "loss = \\frac{\\sum{(\\hat{y}-y)}^2}{2n}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Tensorflow provides various inbuilt optimizers that allow for the optimization of objective functions. These inbuilt optimizers are mostly directed toward neural network optimization, but a user can specify their own optimization functions by extending a base class. The base class provides access to various methods that calculate the gradients at all points in our computational graph. However, for most industrial projects the set of optimizers provided by TensorFlow are sufficient. \n",
    "\n",
    "To optimize this linear regressor, we will use the inbuilt <font color=\"blue\"><b>Gradient Descent Optimizer</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "New concepts:\n",
    "3. <font color=\"blue\"><b>reduce_sum</b></font> : https://www.tensorflow.org/api_docs/python/tf/reduce_sum\n",
    "4. <font color=\"blue\"><b>Gradient Descent Optimizer</b></font> : https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_sum( input_tensor = tf.pow(prediction-y, 2))/(2*n_samples)\n",
    "\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Loss'):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(prediction), axis=1))\n",
    "                          \n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "                          \n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We now have a complete computational graph. Each run of the optimizer takes a group of  Xs as input and makes a prediction. The prediction is compared against the inputted Ys to get the loss. The optimizer updates the free variables in its loss function based on the loss for that input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Executing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "TensorFlow uses the <font color=\"red\">**`tf.Session`**</font> class to represent a <font color=\"red\">connection between the client program</font>---typically a Python program, although a similar interface is available in other languages---and the <font color = \"red\">C++ runtime</font>. \n",
    "\n",
    "A `tf.Session` object provides access to devices in the local machine, and remote devices using the distributed TensorFlow runtime. \n",
    "\n",
    "It also caches information about your `tf.Graph` so that you can efficiently run the same computation multiple times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Training the computational graph in a session\n",
    "\n",
    "```python\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for each epoch in range(training_epochs):\n",
    "        for each batch in range(total_batches):\n",
    "            sess.run(fetches=[optimizer,loss], feed_dict={x: batch_xs, y: batch_ys})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use <font color = \"red\">**Batch Gradient Descent**</font> to optimize our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "The `run` method runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every Operation and evaluate every Tensor in fetches, substituting the values in feed_dict for the corresponding input values\n",
    "\n",
    "Notice that we pass a batch of Xs and Ys to the feed_dict parameter when we run the optimizer in a session.\n",
    "\n",
    "The `feed_dict` parameter in the `run` function of a session accepts key, value entries where the value is a Python scalar, string, list, or numpy ndarray that can be converted to the same dtype as that tensor represented by the key. Additionally, if the key is a tf.placeholder, the shape of the value will be checked for compatibility with the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 loss= 0.465482926\n",
      "Epoch: 0010 loss= 0.392333548\n",
      "Epoch: 0015 loss= 0.362695539\n",
      "Epoch: 0020 loss= 0.345441683\n",
      "Epoch: 0025 loss= 0.333685642\n",
      "Optimization Finished!\n",
      "Accuracy: 0.889333\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logs_path + \"/logistic\", graph=tf.get_default_graph())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # there would be 600 batches\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_loss += c / total_batch\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorboard to View Graph Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the computational graph that we have just defined on Tensorboard. We have installed a jupyter extension that makes connecting to Tensorboard very simple. To do this, \n",
    "\n",
    "In your Jupyter directory tree view, select the log directory for lesson 1 and click the <font color = \"red\">**Tensorboard**</font> button as shown in the picture.\n",
    "<img src = \"../resources/img/TensorboardInit1.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, go to the <font color = \"red\">**Running**</font> tab, and choose the Tensorboard instance corresponding to the correct log directory as shown in the screenshot.\n",
    "<img src = \"../resources/img/TensorboardInit2.PNG\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
