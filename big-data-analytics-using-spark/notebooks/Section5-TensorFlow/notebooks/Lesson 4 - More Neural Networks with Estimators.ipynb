{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Network Take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Build a fully connected neural network with 2 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using TensorFlow's higher-level operations and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- High level TensorFlow API that encapsulates\n",
    "    - Training\n",
    "    - Testing\n",
    "    - Prediction\n",
    "    - Export for model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Two kinds of estimators\n",
    "    - Pre-made built-in estimators: DNNClassifier, LinearRegressor\n",
    "    - Custom estimators: written using Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../../resources/img/estimator_types.png\" alt=\"nn\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Benefits of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Estimator-based models are independent of operating environment\n",
    "    - local host\n",
    "    - GPUs\n",
    "    - CPU clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Simplify model sharing between deveopers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- State of the art models with high-level intuitive code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Built using tf.layers\n",
    "- Estimators build TF Graph automatically given a series of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Please go through https://www.tensorflow.org/programmers_guide/estimators for more advantages of using Estimators as described by the developers of TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the same parameters as when we built the neural network using the low-level TF API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using the Estimator API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Create an `input function`\n",
    "\n",
    "   * Supplies data for training, evaluation, prediction\n",
    "   * Yields tuples of:\n",
    "        - Python dict `features`: key = name of feature, value = array of feature values\n",
    "        - Array `label` : label for every example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "How the `input function` generates the `features` and `label` is up to the developer. TensorFlow developers recommend the use of the  TensorFlow's Dataset API. The Dataset API can parse in all kinds of data using a high level specification. This includes:\n",
    "- reading lines from a file\n",
    "- reading records from binary files\n",
    "- iterators\n",
    "- Initialize Dataset from  in-memory data\n",
    "\n",
    "The Dataset API can even read in large files in parallel and join them into a single stream. It is highly versatile and should definitely be used when training from data that resides on the disk. \n",
    "For more information, refer: https://www.tensorflow.org/get_started/datasets_quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To define our input function, we will use an inbuilt function `tf.estimator.inputs.numpy_input_fn` which can take in Numpy arrays and returns an input function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "From the documentation:\n",
    "\n",
    ">Returns input function that would feed dict of numpy arrays into the model.\n",
    "This returns a function outputting features and targets based on the dict of numpy arrays. The dict features has the same keys as the x. The dict targets has the same keys as the y if y is a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_input_fn(mode):\n",
    "    if mode == 'train':\n",
    "        return tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "            batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "    elif mode == \"evaluation\":\n",
    "        return tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "            batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create Feature Columns\n",
    "\n",
    "    If we had read the data from files, we need to create feature columns that describe how to use the input. \n",
    "    \n",
    "    Is it numeric? Categorical? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define the Model Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Model function is the beating heart of Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Has the following signature:\n",
    "```\n",
    "def my_model_fn(\n",
    "       features, # This is batch_features from input_fn\n",
    "       labels,   # This is batch_labels from input_fn\n",
    "       mode,     # An instance of tf.estimator.ModeKeys\n",
    "       params):  # Additional configuration\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "ModeKeys specify whether the model is being called for training, evaluation or prediction. This is useful for neural network architectures that vary between training and prediction, such as for models that make use of dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Workflow**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specify additional calculations for:\n",
    "    - Training\n",
    "    - Prediction\n",
    "    - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Defining a neural network function to use later\n",
    "- Input: feature dict\n",
    "- Output: Array of 10 logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When using the Estimator API, it is recommended to use mid-level APIs such as Layers and Metrics in order to simplify the specification of our model. Fully connected hidden layers are implemented in `tf.layers` as a `Dense` layer. The Dense layer takes as input an incoming tensor, and the number of nodes in the Dense layer that we are specifying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def neural_net(x_dict):\n",
    "    x = x_dict['images']\n",
    "    layer_1 = tf.layers.dense(x, n_hidden_1)\n",
    "    layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n",
    "    out_layer = tf.layers.dense(layer_2, num_classes)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Benefits of high level API:\n",
    "- Can encapsulate calculations as pre-defined layers. Other layers include:\n",
    "    - Convolution, Dropout, Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "def neural_net(x_dict):\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    x = x_dict['images']\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(x, n_hidden_1)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, n_hidden_2)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.layers.dense(layer_2, num_classes)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing the Model function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, prepare the output and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "logits = neural_net(features)\n",
    "pred_classes = tf.argmax(logits, axis=1)\n",
    "pred_probabilties = tf.nn.softmax(logits)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The model function returns an instance of `tf.estimator.EstimatorSpec`\n",
    "```\n",
    "class EstimatorSpec(\n",
    "    collections.namedtuple('EstimatorSpec', [\n",
    "        'mode', 'predictions', 'loss', 'train_op', 'eval_metric_ops',\n",
    "        'export_outputs', 'training_chief_hooks', 'training_hooks', 'scaffold',\n",
    "        'evaluation_hooks'\n",
    "    ])):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we are only interested in prediction or inference from the neural network, we do not need to backpropagate errors. In fact, we would not have the actual labels to calculate errors. \n",
    "\n",
    "In this case, we do not require the backpropagation part of the TF Graph. We can return an EstimatorSpec as soon as we make our predictions. \n",
    "\n",
    "For more complex DNN's there may be different model architectures for training as compared to evaluation or prediction. It is important to fully account for these cases in the specification of our model function by handling the branching cases. An example of a more complex DNN is a DNN that uses dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Handle branching cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Prediction\n",
    "```\n",
    "if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training and Evaluation\n",
    "\n",
    "```\n",
    "estim_specs = tf.estimator.EstimatorSpec(\n",
    "  mode=mode,\n",
    "  predictions=pred_classes,\n",
    "  loss=loss_op,\n",
    "  train_op=train_op,\n",
    "  eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "return estim_specs\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    logits = neural_net(features)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build and Use the Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: D:\\Temp\\tmpxvj6xfyw\n",
      "INFO:tensorflow:Using config: {'_master': '', '_is_chief': True, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_model_dir': 'D:\\\\Temp\\\\tmpxvj6xfyw', '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002EF9D924908>, '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_task_id': 0, '_service': None, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker'}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train the Model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The model can be trained using the Estimator's `train` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into D:\\Temp\\tmpxvj6xfyw\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 2.4518557\n",
      "INFO:tensorflow:global_step/sec: 99.4813\n",
      "INFO:tensorflow:step = 101, loss = 0.33296615 (1.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.966\n",
      "INFO:tensorflow:step = 201, loss = 0.36804593 (0.990 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.652\n",
      "INFO:tensorflow:step = 301, loss = 0.3458166 (0.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.528\n",
      "INFO:tensorflow:step = 401, loss = 0.12781583 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.074\n",
      "INFO:tensorflow:step = 501, loss = 0.33862978 (0.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.84\n",
      "INFO:tensorflow:step = 601, loss = 0.301341 (0.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.889\n",
      "INFO:tensorflow:step = 701, loss = 0.3442419 (0.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.168\n",
      "INFO:tensorflow:step = 801, loss = 0.33781174 (0.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.845\n",
      "INFO:tensorflow:step = 901, loss = 0.38626903 (0.919 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into D:\\Temp\\tmpxvj6xfyw\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.21108991.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x2ef9d9245c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn = get_input_fn('train')\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model can be trained using the Estimator's `evaluate` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Remember, we need the evaluation input function when we test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-23-06:23:20\n",
      "INFO:tensorflow:Restoring parameters from D:\\Temp\\tmpxvj6xfyw\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-23-06:23:21\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9181, global_step = 1000, loss = 0.2829952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9181, 'global_step': 1000, 'loss': 0.2829952}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_input_fn = get_input_fn('evaluation')\n",
    "model.evaluate(evaluate_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predict using the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\Temp\\tmpxvj6xfyw\\model.ckpt-1000\n",
      "Model predictions are  [7, 2]\n",
      "Model predictions are  [1, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2efa0046908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_images = 4\n",
    "\n",
    "actual_labels = mnist.test.images[:n_images]\n",
    "predictions = list(model.predict(evaluate_input_fn))\n",
    "\n",
    "# Display\n",
    "f = plt.figure(figsize=(6,3))\n",
    "for i in range(n_images):\n",
    "    sp = f.add_subplot(2, 2, i+1)\n",
    "    sp.axis('Off')\n",
    "    plt.imshow(np.reshape(actual_labels[i], [28, 28]), cmap='gray')\n",
    "print('Model predictions are ' ,predictions[:2])\n",
    "print('Model predictions are ' ,predictions[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks Using Estimators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>![CNN](http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Parameters for our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.25 # Dropout, probability to drop a unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 1** - Implement a ConvNet function using Layers API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 2** - Define model in terms of ConvNet function and input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 3** - Build Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Step 4** - Train, Evaluate, Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## ConvNet Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Input: \n",
    "- x_dict -- dict\n",
    "- n_classes -- int\n",
    "- dropout -- boolean\n",
    "- reuse -- boolean\n",
    "- is_training -- boolean\n",
    "\n",
    "Output: logits -- array of `n_classes` logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<font color=\"blue\">What is dropout?</font>\n",
    "\n",
    "Dropout is a technique developed in 2014 to \"learn better by learning less\". A dropout layer between two Dense layers would not permit the forward or backward flow of information with a pre-defined probability at training time. In other words, Dropout randomly drops neural units in the layers that sandwich the Dropout layer during training. The learned wisdom behind Dropout is that it aids in regularization by forcing the Neural network to learn without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Randomly drop neurons in layers surrounding a dropout layer when training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Aids in regularization by forcing NN to learn without overfitting\n",
    "\n",
    "<center><img src=\"../../resources/img/Dropout.png\" alt=\"nn\" style=\"width: 400px;\"/>\n",
    "Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from\n",
    "overfitting”, JMLR 2014</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Testing with Dropout**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use all neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Different DataFlow Graph for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Retain all weights learned during Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Inbuilt TensorFlow support for Dropout -- `reuse` flag in `variable_scope`:\n",
    "```\n",
    "with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At test time, all neurons in the network are active. So the computational graph varies between the training and testing phase, however the weights applied at each neuron remain the same. \n",
    "\n",
    "TensorFlow provides support to maintain the weights between computational graphs using a `reuse` flag when we enclose a Graph in a predefined scope.\n",
    "```\n",
    "with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Resize Input to match picture format\n",
    "- Picture format = 28 x 28, 1 channel\n",
    "\n",
    "```x = tf.reshape(x, shape=[-1, 28, 28, 1])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our input tensor has the shape `784 x batch_size`.\n",
    "\n",
    "We should reshape the input tensor so that the ConvNet can learn filters that are spatially aware. We'll reshape this input tensor to match the picture's format: `Height x Width x Channel`.\n",
    "\n",
    "```x = tf.reshape(x, shape=[-1, 28, 28, 1])```\n",
    " \n",
    "Tensor input is now 4-D: `Batch Size, Height, Width, Channel`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolution and Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"../../resources/img/ConvNet_Layers.png\" alt=\"nn\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2D Convolution**\n",
    "\n",
    "- Element wise multiplication of 2D filter as it slides across the length and breadth of input image  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**2D Max Pooling**\n",
    "\n",
    "- Reduce the spatial size of input by subsampling every `n x n` slice of a Convolved 2D Tensor \n",
    "- Replace each slice with max value in the slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "ConvNets are built using the fundamental operations of Convolution and Pooling.\n",
    "\n",
    "The 2D Convolution operation takes a large input 2D Tensor and multiplies each element, the weight, in a smaller 2D window, canonically called the Convolutional Filter.\n",
    "\n",
    "The Pooling operation's function is to reduce the spatial size of the input image, reducing the number of parameters and avoiding overfitting. The most commonly used Pooling operation for images is a 2D MAX Pooling operation that downsamples every `n x n` slice of the Convolved Tensor by replacing the slice with the Maximum value in the `n x n` slice.\n",
    "\n",
    "For a more detailed, but probably the clearest writing about ConvNets, check out Andrej Karpathy's CSE231n lesson notes about the layers in ConvNets: http://cs231n.github.io/convolutional-networks/#pool   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TensorFlow provides support for Convolution and Pooling through the Layers API\n",
    "\n",
    "- Convolution:\n",
    "```\n",
    "conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "```\n",
    "Specifies a Convolutional layer with 32 filters, and shape `5 x 5`\n",
    "\n",
    "- Pooling:\n",
    "```\n",
    "conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "```\n",
    "Specifies a Max Pooling layer with a stride of 2, and shape `2 x 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we need to flatten the convolved and downsampled image into a 1 dimensional Tensor so that we can pass it through Dense layers and output an array of 10 logits.\n",
    "\n",
    "```\n",
    "fc1 = tf.contrib.layers.flatten(conv2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lastly, we add Dense layers that sandwich a Dropout layer\n",
    "```\n",
    "fc1 = tf.layers.dense(fc1, 1024)\n",
    "fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "out = tf.layers.dense(fc1, n_classes)\n",
    "```\n",
    "\n",
    "The `training` flag in the Dropout layer is a toggle to enable the layer when we are Training the model, and disable it at test time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Putting it all together,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input becomes 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer 2 with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our model function is very similar to the model function we wrote for the simple feed-forward Neural Network. Except that we use the ConvNet function to get our logits.\n",
    "\n",
    "We also need to call the ConvNet function in two different ways because Dropout has different behavior at training and testing time:\n",
    "- Training:\n",
    "`logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)`\n",
    "\n",
    "- Testing:\n",
    "`logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: D:\\Temp\\tmpkht_pku2\n",
      "INFO:tensorflow:Using config: {'_is_chief': True, '_num_ps_replicas': 0, '_tf_random_seed': None, '_service': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023999646FD0>, '_session_config': None, '_save_checkpoints_steps': None, '_model_dir': 'D:\\\\Temp\\\\tmpkht_pku2', '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 1, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_task_type': 'worker', '_save_summary_steps': 100, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We still need an input function. Our input function is actually the same as when we built a feedforward neural network. This is the thing about the Estimator API. It makes the model independent of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_fn = get_input_fn('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into D:\\Temp\\tmpkht_pku2\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3265784, step = 1\n",
      "INFO:tensorflow:global_step/sec: 4.41244\n",
      "INFO:tensorflow:loss = 0.0964202, step = 101 (22.666 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.2523\n",
      "INFO:tensorflow:loss = 0.09626232, step = 201 (23.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.39865\n",
      "INFO:tensorflow:loss = 0.032961518, step = 301 (22.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.01977\n",
      "INFO:tensorflow:loss = 0.23971115, step = 401 (24.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.17903\n",
      "INFO:tensorflow:loss = 0.053734824, step = 501 (23.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.0423\n",
      "INFO:tensorflow:loss = 0.035259385, step = 601 (24.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.21675\n",
      "INFO:tensorflow:loss = 0.02497337, step = 701 (23.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.4661\n",
      "INFO:tensorflow:loss = 0.028952029, step = 801 (22.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.33811\n",
      "INFO:tensorflow:loss = 0.005173399, step = 901 (23.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.99294\n",
      "INFO:tensorflow:loss = 0.053166352, step = 1001 (25.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.27659\n",
      "INFO:tensorflow:loss = 0.04607383, step = 1101 (23.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.25174\n",
      "INFO:tensorflow:loss = 0.01202877, step = 1201 (23.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.25246\n",
      "INFO:tensorflow:loss = 0.06419534, step = 1301 (23.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.07192\n",
      "INFO:tensorflow:loss = 0.030215802, step = 1401 (24.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.17653\n",
      "INFO:tensorflow:loss = 0.05805909, step = 1501 (23.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.19887\n",
      "INFO:tensorflow:loss = 0.029637396, step = 1601 (23.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.25021\n",
      "INFO:tensorflow:loss = 0.04556966, step = 1701 (23.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.29415\n",
      "INFO:tensorflow:loss = 0.0062331967, step = 1801 (23.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.27746\n",
      "INFO:tensorflow:loss = 0.006417631, step = 1901 (23.379 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into D:\\Temp\\tmpkht_pku2\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.004071021.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x23999646ef0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_fn = get_input_fn('evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-23-19:34:26\n",
      "INFO:tensorflow:Restoring parameters from D:\\Temp\\tmpkht_pku2\\model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-23-19:34:33\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9919, global_step = 2000, loss = 0.028276833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9919, 'global_step': 2000, 'loss': 0.028276833}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:\\Temp\\tmpkht_pku2\\model.ckpt-2000\n",
      "Model predictions are  [1, 3]\n",
      "Model predictions are  [1, 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23999521fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_images = 4\n",
    "offset = random.randint(0, 10000)\n",
    "actual_labels = mnist.test.images[offset:offset + n_images]\n",
    "predictions = list(model.predict(input_fn))[offset: offset + n_images]\n",
    "\n",
    "# Display\n",
    "f = plt.figure(figsize=(8,4))\n",
    "for i in range(n_images):\n",
    "    sp = f.add_subplot(2, 2, i+1)\n",
    "    sp.axis('Off')\n",
    "    plt.imshow(np.reshape(actual_labels[i], [28, 28]), cmap='gray')\n",
    "print('Model predictions are ' ,predictions[:2])\n",
    "print('Model predictions are ' ,predictions[2:4])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "f4c581da-5b5e-4d34-9209-caa24a86894c",
    "theme": {}
   }
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "263px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "502px",
    "left": "0px",
    "right": "1154px",
    "top": "135px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
