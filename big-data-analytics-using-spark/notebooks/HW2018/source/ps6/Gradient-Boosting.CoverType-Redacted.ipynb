{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bae3377213022d8c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  CSE 255 Programming Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa1806c15bc34f91",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Problem Statement\n",
    "\n",
    "In this programming assignment, your task is to classify geographical locations according to their predicted tree cover using Gradient Boosting and Random Forest classifiers. You are expected to fill in functions that would complete this task. All of the necessary helper code is included in this notebook. However, we advise you to go over the lecture material, the EdX videos and the corresponding notebooks before you attempt this Programming Assignment. You can find information about the dataset to be used in the following links:\n",
    "\n",
    "* **Dataset:** http://archive.ics.uci.edu/ml/datasets/Covertype \n",
    "\n",
    "* **Dataset description:** http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import os\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Cover Types: {1.0: 'Spruce/Fir', 2.0: 'Lodgepole Pine', 3.0: 'Ponderosa Pine', 4.0: 'Cottonwood/Willow', 5.0: 'Aspen', 6.0: 'Douglas-fir', 7.0: 'Krummholz'}\n"
     ]
    }
   ],
   "source": [
    "#define a dictionary of cover types\n",
    "CoverTypes={1.0: 'Spruce/Fir',\n",
    "            2.0: 'Lodgepole Pine',\n",
    "            3.0: 'Ponderosa Pine',\n",
    "            4.0: 'Cottonwood/Willow',\n",
    "            5.0: 'Aspen',\n",
    "            6.0: 'Douglas-fir',\n",
    "            7.0: 'Krummholz' }\n",
    "print('Tree Cover Types:', CoverTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    %cd $file_path\n",
    "    if not exists('covtype'):\n",
    "        print(\"creating directory covtype\")\n",
    "        !mkdir covtype\n",
    "    %cd covtype\n",
    "    if not exists('covtype.data'):\n",
    "        if not exists('covtype.data.gz'):\n",
    "            print('downloading covtype.data.gz')\n",
    "            !curl -O http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
    "        print('decompressing covtype.data.gz')\n",
    "        !gunzip -f covtype.data.gz\n",
    "    !ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/jovyan/work/notebooks/HW2018/source/ps6/'\n",
      "/home/jovyan/work/HW/HW7\n",
      "/home/jovyan/work/HW/HW7/covtype\n",
      "total 85800\n",
      "-rw-r--r-- 1 jovyan users     1264 May 20 22:42 BoostedTreesResults.pkl\n",
      "drwxr-xr-x 4 jovyan users      128 May 21 04:26 covtype\n",
      "-rw-r--r-- 1 jovyan users 75169317 May 17 18:35 covtype.data\n",
      "-rw-r--r-- 1 jovyan users      293 May 20 22:47 GradientBoostingResults.pkl\n"
     ]
    }
   ],
   "source": [
    "get_data(\"/home/jovyan/work/notebooks/HW2018/source/ps6/\") #Change according to necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up features that are made out of several binary features.\n",
    "def get_columns(cols_txt):\n",
    "    cols=[a.strip() for a in cols_txt.split(',')]\n",
    "    colDict={a:[a] for a in cols}\n",
    "    colDict['Soil_Type (40 binary columns)'] = ['ST_'+str(i) for i in range(40)]\n",
    "    colDict['Wilderness_Area (4 binarycolumns)'] = ['WA_'+str(i) for i in range(4)]\n",
    "    columns=[]\n",
    "    for item in cols:\n",
    "        columns = columns + colDict[item]\n",
    "    return columns\n",
    "    #print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature names\n",
    "cols_txt=\"\"\"\n",
    "Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology,\n",
    "Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways,\n",
    "Hillshade_9am, Hillshade_Noon, Hillshade_3pm,\n",
    "Horizontal_Distance_To_Fire_Points, Wilderness_Area (4 binarycolumns), \n",
    "Soil_Type (40 binary columns), Cover_Type\n",
    "\"\"\"\n",
    "columns = get_columns(cols_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'WA_0', 'WA_1', 'WA_2', 'WA_3', 'ST_0', 'ST_1', 'ST_2', 'ST_3', 'ST_4', 'ST_5', 'ST_6', 'ST_7', 'ST_8', 'ST_9', 'ST_10', 'ST_11', 'ST_12', 'ST_13', 'ST_14', 'ST_15', 'ST_16', 'ST_17', 'ST_18', 'ST_19', 'ST_20', 'ST_21', 'ST_22', 'ST_23', 'ST_24', 'ST_25', 'ST_26', 'ST_27', 'ST_28', 'ST_29', 'ST_30', 'ST_31', 'ST_32', 'ST_33', 'ST_34', 'ST_35', 'ST_36', 'ST_37', 'ST_38', 'ST_39', 'Cover_Type']\n"
     ]
    }
   ],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\r\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the first two lines of the data file\n",
    "!head -1 covtype.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read the file into an RDD\n",
    "# When using sc.textRead you need to use an absolute path.\n",
    "# If doing this on a real cluster, you need the file to be available on all nodes, ideally in HDFS.\n",
    "path='covtype/covtype.data'\n",
    "inputRDD=sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4c2f0cbc5f9c9c5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def label_RDD(inputRDD):\n",
    "    '''\n",
    "    Transform the text RDD into an RDD of Labeled Points\n",
    "    \n",
    "    Input: inputRDD \n",
    "    type: RDD\n",
    "    \n",
    "    Returns: Data\n",
    "    type: RDD \n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    Data=inputRDD.map(lambda line: [float(x.strip()) for x in line.split(',')])\\\n",
    "     .map(lambda V:LabeledPoint(V[-1],V[:-1]))\n",
    "        \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = label_RDD(inputRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(5.0, [2596.0,51.0,3.0,258.0,0.0,510.0,221.0,232.0,148.0,6279.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "Data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad446944695ea686",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def count_examples(Data):\n",
    "    '''\n",
    "    Count the number of examples of each type\n",
    "    \n",
    "    Input: Data\n",
    "    type: RDD \n",
    "    \n",
    "    Returns: counts\n",
    "    type: list of tuples, where each tuple is (covertype(int), count(int))\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    counts=Data.map(lambda lp:(lp.label,1)).reduceByKey(lambda x,y:x+y).collect()\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    counts.sort(key=lambda x:x[1],reverse=True)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a678d8b03134f4bf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "counts = count_examples(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-87e7badb92204cff",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(counts) == list, 'Incorrect return type'\n",
    "assert type(counts[0]) == tuple, 'Incorrect return type'\n",
    "assert type(counts[0][0]) == float, 'Incorrect return type'\n",
    "assert type(counts[0][1]) == int, 'Incorrect return type'\n",
    "assert counts[0][0] == 2, 'Incorrect return value'\n",
    "assert counts[0][1] == 283301, 'Incorrect return value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size= 581012\n",
      "              type (label):   percent of total\n",
      "---------------------------------------------------------\n",
      "      Lodgepole Pine (2.0):\t48.76\n",
      "          Spruce/Fir (1.0):\t36.46\n",
      "      Ponderosa Pine (3.0):\t6.15\n",
      "           Krummholz (7.0):\t3.53\n",
      "         Douglas-fir (6.0):\t2.99\n",
      "               Aspen (5.0):\t1.63\n",
      "   Cottonwood/Willow (4.0):\t0.47\n"
     ]
    }
   ],
   "source": [
    "total=Data.cache().count()\n",
    "print('total data size=',total)\n",
    "print('              type (label):   percent of total')\n",
    "print('---------------------------------------------------------')\n",
    "print('\\n'.join(['%20s (%3.1f):\\t%4.2f'%(CoverTypes[a[0]],a[0],100.0*a[1]/float(total)) for a in counts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Making the problem binary\n",
    "\n",
    "The implementation of BoostedGradientTrees in MLLib supports only binary problems. the `CovTYpe` problem has\n",
    "7 classes. To make the problem binary we choose the `Lodgepole Pine` (label = 2.0). We therefor transform the dataset to a new dataset where the label is `1.0` is the class is `Lodgepole Pine` and is `0.0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68e335dc015340e5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def labels_to_binary(Data):\n",
    "    '''\n",
    "    Transform the dataset to a new dataset \n",
    "    such that new label is 1 if current label is 2, else new label is 0\n",
    "    \n",
    "    Input: Data\n",
    "    type: RDD\n",
    "    \n",
    "    Returns: Data\n",
    "    type: RDD\n",
    "    '''\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    Label=2.0\n",
    "    Data=inputRDD.map(lambda line: [float(x) for x in line.split(',')])\\\n",
    "    .map(lambda V:LabeledPoint(1.0*(V[-1]==Label),V[:-1]))\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, [2596.0,51.0,3.0,258.0,0.0,510.0,221.0,232.0,148.0,6279.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = labels_to_binary(Data)\n",
    "Data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reducing data size\n",
    "In order to see the effects of overfitting more clearly, we reduce the size of the data by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Data1=Data.sample(False,0.1).cache()\n",
    "(trainingData,testData)=Data1.randomSplit([0.7,0.3])\n",
    "(trainingData_all, testData_all)=Data.randomSplit([0.7, 0.3])\n",
    "\n",
    "import pickle\n",
    "pickle.dump(trainingData.collect(), open('training10p.pkl', 'wb'))\n",
    "pickle.dump(testData.collect(), open('test10p.pkl', 'wb'))\n",
    "pickle.dump(trainingData_all.collect(), open('training_all.pkl', 'wb'))\n",
    "pickle.dump(testData_all.collect(), open('test_all.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = sc.parallelize(pickle.load(open('training10p.pkl', 'rb')))\n",
    "testData = sc.parallelize(pickle.load(open('test10p.pkl', 'rb')))\n",
    "trainingData_all = sc.parallelize(pickle.load(open('training_all.pkl', 'rb')))\n",
    "testData_all = sc.parallelize(pickle.load(open('test_all.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: Data1=57963, trainingData=40758, testData=17205\n"
     ]
    }
   ],
   "source": [
    "print('Sizes: Data1=%d, trainingData=%d, testData=%d'%(Data1.count(),trainingData.cache().count(),testData.cache().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 8749), (1.0, 8456)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts=testData.map(lambda lp:(lp.label,1)).reduceByKey(lambda x,y:x+y).collect()\n",
    "counts.sort(key=lambda x:x[1],reverse=True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosted Trees\n",
    "\n",
    "* Following [this example](http://spark.apache.org/docs/latest/mllib-ensembles.html#classification) from the mllib documentation\n",
    "\n",
    "* [pyspark.mllib.trees documentation](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.tree)\n",
    "\n",
    "#### Main classes and methods\n",
    "\n",
    "* `GradientBoostedTrees` is the class that implements the learning trainClassifier,\n",
    "   * It's main method is `trainClassifier(trainingData)` which takes as input a training set and generates an instance of `GradientBoostedTreesModel`\n",
    "   * The main parameter from train Classifier are:\n",
    "      * **data** – Training dataset: RDD of LabeledPoint. Labels should take values {0, 1}.\n",
    "      * categoricalFeaturesInfo – Map storing arity of categorical features. E.g., an entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.\n",
    "      * **loss** – Loss function used for minimization during gradient boosting. Supported: {“logLoss” (default), “leastSquaresError”, “leastAbsoluteError”}.\n",
    "      * **numIterations** – Number of iterations of boosting. (default: 100)\n",
    "      * **learningRate** – Learning rate for shrinking the contribution of each estimator. The learning rate should be between in the interval (0, 1]. (default: 0.1)\n",
    "      * **maxDepth** – Maximum depth of the tree. E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 3)\n",
    "      * **maxBins** – maximum number of bins used for splitting features (default: 32) DecisionTree requires maxBins >= max categories\n",
    "      \n",
    "      \n",
    "* `GradientBoostedTreesModel` represents the output of the boosting process: a linear combination of classification trees. The methods supported by this class are:\n",
    "   * `save(sc, path)` : save the tree to a given filename, sc is the Spark Context.\n",
    "   * `load(sc,path)` : The counterpart to save - load classifier from file.\n",
    "   * `predict(X)` : predict on a single datapoint (the `.features` field of a `LabeledPont`) or an RDD of datapoints.\n",
    "   * `toDebugString()` : print the classifier in a human readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9bdfd4c81113b03",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def Classify_GB(trainingData, testData, maxDepth):\n",
    "    '''\n",
    "    Train and test GradientBoostedTrees classifier using given training and test data\n",
    "    Repeat the procedure with different number of training iterations\n",
    "    \n",
    "    Input: trainingData, testData\n",
    "    type: RDD, RDD\n",
    "    \n",
    "    Returns: errors\n",
    "    type: dict with key = number of iterations, value = train & test error\n",
    "    \n",
    "    example output:\n",
    "    errors = {1:{'train':0.2, 'test':0.3}, 3:{'train':0.15, 'test':0.16}}\n",
    "    '''\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    from time import time\n",
    "    errors={}\n",
    "    start=time()\n",
    "    model=GradientBoostedTrees.trainClassifier(trainingData,categoricalFeaturesInfo={},numIterations=i,maxDepth=maxDepth)\n",
    "    #print model.toDebugString()\n",
    "    errors[i]={}\n",
    "    dataSets={'test':testData}\n",
    "    for name in list(dataSets.keys()):  # Calculate errors on train and test sets\n",
    "        data=dataSets[name]\n",
    "        Predicted=model.predict(data.map(lambda x: x.features))\n",
    "        LabelsAndPredictions=data.map(lambda lp: lp.label).zip(Predicted)\n",
    "        Err = LabelsAndPredictions.filter(lambda v_p:v_p[0] != v_p[1]).count()/float(data.count())\n",
    "        errors[i][name]=Err\n",
    "        print(i,errors[i],int(time()-start),'seconds')\n",
    "    print(errors)\n",
    "            \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 {'test': 0.27829119442022665} 13 seconds\n",
      "20 {'test': 0.27335077012496367} 23 seconds\n",
      "{10: {'test': 0.27829119442022665}, 20: {'test': 0.27335077012496367}}\n",
      "10 {'test': 0.25510026155187443} 17 seconds\n",
      "20 {'test': 0.23940714908456845} 32 seconds\n",
      "{10: {'test': 0.25510026155187443}, 20: {'test': 0.23940714908456845}}\n",
      "10 {'test': 0.2122638767800058} 24 seconds\n",
      "20 {'test': 0.202615518744551} 48 seconds\n",
      "{10: {'test': 0.2122638767800058}, 20: {'test': 0.202615518744551}}\n",
      "10 {'test': 0.17210113339145597} 44 seconds\n",
      "20 {'test': 0.15948852077884335} 74 seconds\n",
      "{10: {'test': 0.17210113339145597}, 20: {'test': 0.15948852077884335}}\n"
     ]
    }
   ],
   "source": [
    "#Train with 10% of the dataset\n",
    "B_10p_1 = Classify_GB(trainingData, testData, 1)\n",
    "B_10p_3 = Classify_GB(trainingData, testData, 3)\n",
    "B_10p_6 = Classify_GB(trainingData, testData, 6)\n",
    "B_10p_10 = Classify_GB(trainingData, testData, 10)\n",
    "#Store Results\n",
    "Results={ID:globals()[ID] for ID in ['B_10p_1','B_10p_3', 'B_10p_6', 'B_10p_10']}\n",
    "import pickle\n",
    "pickle.dump(Results, open('GradientBoostingResults.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results=pickle.load(open('GradientBoostingResults.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "* Following [this example](http://spark.apache.org/docs/latest/mllib-ensembles.html#classification) from the mllib documentation\n",
    "\n",
    "* [pyspark.mllib.trees.RandomForest documentation](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.tree.RandomForest)\n",
    "\n",
    "**trainClassifier**`(data, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy='auto', impurity='gini', maxDepth=4, maxBins=32, seed=None)`   \n",
    "Method to train a decision tree model for binary or multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**  \n",
    "* *data* – Training dataset: RDD of LabeledPoint. Labels should take values {0, 1, ..., numClasses-1}.  \n",
    "* *numClasses* – number of classes for classification.  \n",
    "* *categoricalFeaturesInfo* – Map storing arity of categorical features. E.g., an entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.  \n",
    "* *numTrees* – Number of trees in the random forest.  \n",
    "* *featureSubsetStrategy* – Number of features to consider for splits at each node. Supported: “auto” (default), “all”, “sqrt”, “log2”, “onethird”. If “auto” is set, this parameter is set based on numTrees: if numTrees == 1, set to “all”; if numTrees > 1 (forest) set to “sqrt”.\n",
    "* *impurity* – Criterion used for information gain calculation. Supported values: “gini” (recommended) or “entropy”.  \n",
    "* *maxDepth* – Maximum depth of the tree. E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 4)  \n",
    "* *maxBins* – maximum number of bins used for splitting features (default: 32)\n",
    "* *seed* – Random seed for bootstrapping and choosing feature subsets.  \n",
    "\n",
    "**Returns:**\t\n",
    "RandomForestModel that can be used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1772af5d3c6c0273",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def Classify_RF(trainingData, testData, depth):\n",
    "    '''\n",
    "    Train and test RandomForest classifier using given training and test data\n",
    "    Repeat the procedure with different tree depths\n",
    "    \n",
    "    Input: trainingData, testData\n",
    "    type: RDD, RDD\n",
    "    \n",
    "    Returns: errors\n",
    "    type: dict with key = number of iterations, value = train & test error\n",
    "    \n",
    "    example output:\n",
    "    errors = {1:{'train':0.2, 'test':0.3}, 3:{'train':0.15, 'test':0.16}}\n",
    "    '''\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    from time import time\n",
    "    errors={}\n",
    "    start=time()\n",
    "    model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                 numTrees=numTrees, featureSubsetStrategy=\"auto\",\n",
    "                                 impurity='gini', maxDepth=depth, maxBins=32)\n",
    "    #print model.toDebugString()\n",
    "    errors[depth]={}\n",
    "    dataSets={'test':testData}\n",
    "    for name in list(dataSets.keys()):  # Calculate errors on train and test sets\n",
    "        data=dataSets[name]\n",
    "        Predicted=model.predict(data.map(lambda x: x.features))\n",
    "        LabelsAndPredictions=data.map(lambda lp: lp.label).zip(Predicted)\n",
    "        Err = LabelsAndPredictions.filter(lambda v_p:v_p[0] != v_p[1]).count()/float(data.count())\n",
    "        errors[depth][name]=Err\n",
    "    print(depth,errors[depth],int(time()-start),'seconds')\n",
    "    print(errors)\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 {'test': 0.2967160709096193} 1 seconds\n",
      "6 {'test': 0.2760244115082825} 1 seconds\n",
      "10 {'test': 0.23109561174077303} 2 seconds\n",
      "{3: {'test': 0.2967160709096193}, 6: {'test': 0.2760244115082825}, 10: {'test': 0.23109561174077303}}\n",
      "3 {'test': 0.2864864864864865} 1 seconds\n",
      "6 {'test': 0.2662016855565243} 1 seconds\n",
      "10 {'test': 0.22063353676256903} 3 seconds\n",
      "{3: {'test': 0.2864864864864865}, 6: {'test': 0.2662016855565243}, 10: {'test': 0.22063353676256903}}\n"
     ]
    }
   ],
   "source": [
    "#Train with 10% of the dataset\n",
    "RF_10p_3 = Classify_RF(trainingData, testData, 3)\n",
    "RF_10p_6 = Classify_RF(trainingData, testData, 6)\n",
    "RF_10p_8 = Classify_RF(trainingData, testData, 8)\n",
    "RF_10p_10 = Classify_RF(trainingData, testData, 10)\n",
    "#Store Results\n",
    "Results_RF={ID:globals()[ID] for ID in ['RF_10p_3','RF_10p_6', 'RF_10p_8', 'RF_10p_10']}\n",
    "import pickle\n",
    "pickle.dump(Results_RF, open('RandomForestResults.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "Results=pickle.load(open('BoostedTreesResults.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure([Results['RF_10p_10'],Results['RF_10p_100']],['10Trees','100Trees'],Title='Random Forest using 10% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_figure([Results['RF_all_10'],Results['RF_all_100']],['10Trees','100Trees'],Title='Random Forest using entire dataset')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
