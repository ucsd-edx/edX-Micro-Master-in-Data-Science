{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 255 Programming Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Problem Statement\n",
    "\n",
    "In this programming assignment, you will estimate intrinsic dimension by calculating the Mean Squared Distance of the entire dataset to their representative centers. You will use the K-Means API in spark to find representative centers.\n",
    "All of the necessary helper code is included in this notebook. However, we advise you to go over the lecture material, the EdX videos and the corresponding notebooks before you attempt this Programming Assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the Theory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the intrinsic dimension of a data set\n",
    "Recall from class that given any $d$ dimensional dataset, we can divide it into $n$ cells of diameter $\\epsilon$ each. The relationship between $n, d, \\epsilon$ is then given by:\n",
    "$$\n",
    "n = \\frac{C}{\\epsilon^d}\n",
    "$$\n",
    "Where $C \\in I\\!R$\n",
    "\n",
    "Alternately, we may write this as:\n",
    "$$\n",
    "\\log{n} = \\log{C} + d \\times \\log{\\frac{1}{\\epsilon}}\n",
    "$$\n",
    "\n",
    "Given this expression, we can then compute the dimensionality of a dataset using:\n",
    "$$\n",
    "d = \\frac{\\log{n_2} - \\log{n_1}}{\\log{\\epsilon_1} - \\log{\\epsilon_2}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Where $(n_1,\\epsilon_1)$, $(n_2, \\epsilon_2)$ are the number of cells and diameter of each cell at 2 different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-Means to estimate intrinsic dimension\n",
    "We can use K-Means to approximate the value of intrinsic dimension of a data-set. In this case, the K centers represent the cells, each with a diameter equal to the Mean Squared Distance of the entire dataset to their representative centers. The estimate for intrinsic dimension then becomes:\n",
    "$$\n",
    "d = \\frac{\\log{n_2} - \\log{n_1}}{\\log{\\sqrt{\\epsilon_1}} - \\log{\\sqrt{\\epsilon_2}}} = 2 \\times \\frac{\\log{n_2} - \\log{n_1}}{\\log{\\epsilon_1} - \\log{\\epsilon_2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from math import log\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the homework server\n",
    "On EdX, we provide a test submission and a final submission box. The test submission evaluates your code for the test dataset provided with the homework. The final submission is evaluated on a dataset that is much larger.\n",
    "\n",
    "For submission and for local testing, make sure to read the path of the file you want to operate with from `./hw5-files.txt`. Otherwise your program will receive no points.\n",
    "\n",
    "## Local test\n",
    "\n",
    "For local testing, please create your own `hw5-files.txt` file, which contains a single file path on the local disk, e.g.\n",
    "`file://<absolute_path_to_current_directory>/hw5-small.parquet`. For final submission, we will create this file on our server for testing with the appropriate file path. If your implementation is correct, you should not worry about which file system (i.e. local file system or HDFS) Spark will read data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./hw5-files.txt') as f:\n",
    "    file_path =  f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: runKmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "The function <font color=\"blue\">runKmeans</font> takes as input the complete dataset rdd, the sample of the rdd on which k-means needs to be run on, the k value and the count of elements in the complete data-set. It outputs the Mean Squared Distance(MSD) of all the points in the dataset from their closest centers, where the centers are calculated using k-means.\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Code</font>**\n",
    "``` python\n",
    "rdd = sc.parallelize([(1,1),(2,2),(3,3),(4,4),(5,5)])\n",
    "runKmeans(rdd, sc.parallelize([(1,1),(2,2),(3,3)]), 3, rdd.count())\n",
    "```\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "``` python\n",
    "2.0\n",
    "```\n",
    "\n",
    "<font color=\"red\">**Hint : **</font> You might find [K-Means API](https://spark.apache.org/docs/2.2.0/mllib-clustering.html#k-means) useful. Ensure that the initializationMode parameter is set to kmeans++. The computeCost function gives you the sum of squared distances. You might want to tweak maxIterations to compute centers faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runKmeans(data, sample_dataset, k, count):\n",
    "    clusters = KMeans.train(sample_dataset, k, maxIterations=1)\n",
    "    avg_dists = []\n",
    "    for i in range(1):\n",
    "        avg_dists.append(clusters.computeCost(data.map(tuple))/count)\n",
    "    return sum(avg_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: computeIntrinsicDimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "The function <font color=\"blue\">computeIntrinsicDimension</font> takes as input a pair of values $(n1, e1)$, $(n2, e2)$ and computes the intrinsic dimension as output. $e1, e2$ are the mean squared distances of data-points from their closest center\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Code</font>**\n",
    "``` python\n",
    "n1 = 10\n",
    "n2 = 100\n",
    "e1 = 10000\n",
    "e2 = 100\n",
    "computeIntrinsicDimension(n1, e1, n2, e2)\n",
    "```\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "``` python\n",
    "1.0\n",
    "```\n",
    "<font color=\"red\">**Hint : **</font> Use the last formula in the theory section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIntrinsicDimension(n1, e1, n2, e2):\n",
    "    return 2*(log(n2)-log(n1))/(log(e1)-log(e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Now we run K-means for various values of k and use these to estimate the intrinsic dimension of the dataset. Since the dataset might be very large, running kmeans on the entire dataset to find k representative centers may take a very long time. To overcome this, we sample a subset of points from the complete dataset and run Kmeans only on these subsets. We will run Kmeans on 2 different subset sizes: 10000, 20000 points. We will be estimating the MSD for K values of 10, 200, 700, 2000.\n",
    "\n",
    "\n",
    "The function <font color=\"blue\">run</font> takes a dataframe containing the complete data-set as input and needs to do the following:\n",
    "* For each sample size S\n",
    " * Take the first S elements from the dataframe\n",
    " * For each value of K (number of centroids)\n",
    "  * Call runKmeans(data,S,K,data_count)\n",
    "* Use the MSD values generated to calculate the intrinsic dimension where $(n_1, n_2) \\in \\{(10,200),(200,700),(700,2000)\\}$.  \n",
    "\n",
    "You need to sample a subset of datapoints from the dataset and call the <font color=\"blue\">runKmeans</font> function to generate k centroids for this sample, and use them to compute MSD. <b>The values for $k$ that you need to use are 10, 200, 700, 2000</b>. <b>The sample sizes you will use are 10000, 20000</b>. Once you have found the MSD for the two different sample sizes, you need to estimate the intrinsic dimensionality using <font color=\"blue\">computeIntrinsicDimension</font>. The $n1, n2$ values are the k values $10, 200, 700, 2000$ respectively, and the $e1, e2$ values are the MSD values returned by <font color=\"blue\">runKmeans</font> for the respective value of $k$. You will observe that the value for intrinsic dimension changes as $k$ values are changed. Return a dictionary that contains the calculated intrinsic dimension for the pairs of $(k1, k2) = \\{(10,200),(200,700),(700,2000)\\}$.\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Code</font>**\n",
    "``` python\n",
    "\n",
    "df = spark.read.parquet(file_path)\n",
    "run(df)\n",
    "```\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "``` python\n",
    "{'ID_10000_10_200': 8.11414760371264, 'ID_10000_200_700': 1.2674020060274105, 'ID_10000_700_2000': 1.1024849544099415, 'ID_20000_10_200': 8.179836687458343, 'ID_20000_200_700': 1.2367113721963798, 'ID_20000_700_2000': 1.0511066601714645}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(df):\n",
    "    sample_sizes = [10000, 20000]\n",
    "    kvalues = [10, 200, 700, 2000]\n",
    "    count = df.rdd.count()\n",
    "    res = {}\n",
    "    msd_dict = {10000: [], 20000:[]}\n",
    "    for sample_size in sample_sizes:\n",
    "        samples = df.limit(sample_size)\n",
    "        for k in kvalues:\n",
    "            msd_dict[sample_size].append(runKmeans(df.rdd.map(tuple).cache(), samples.rdd.map(tuple).cache(), k, count))\n",
    "            print(msd_dict)\n",
    "        \n",
    "    for size in sample_sizes:\n",
    "        for i in range(len(kvalues)-1):\n",
    "            res[\"ID_\" + str(size) + \"_\" + str(kvalues[i]) + \"_\" + str(kvalues[i+1])] = computeIntrinsicDimension(kvalues[i], msd_dict[size][i], kvalues[i+1], msd_dict[size][i+1])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10000: [0.05680604060252533], 20000: []}\n",
      "{10000: [0.05680604060252533, 0.0011752311536031563], 20000: []}\n",
      "{10000: [0.05680604060252533, 0.0011752311536031563, 0.0001636547407431133], 20000: []}\n",
      "{10000: [0.05680604060252533, 0.0011752311536031563, 0.0001636547407431133, 2.475402662203251e-05], 20000: []}\n",
      "{10000: [0.05680604060252533, 0.0011752311536031563, 0.0001636547407431133, 2.475402662203251e-05], 20000: [0.05643556762094318]}\n",
      "{10000: [0.05680604060252533, 0.0011752311536031563, 0.0001636547407431133, 2.475402662203251e-05], 20000: [0.05643556762094318, 0.0011586461447486603]}\n",
      "{10000: [0.05680604060252533, 0.0011752311536031563, 0.0001636547407431133, 2.475402662203251e-05], 20000: [0.05643556762094318, 0.0011586461447486603, 0.00015367059912027638]}\n",
      "{10000: [0.05680604060252533, 0.0011752311536031563, 0.0001636547407431133, 2.475402662203251e-05], 20000: [0.05643556762094318, 0.0011586461447486603, 0.00015367059912027638, 2.3489319639280127e-05]}\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(file_path)\n",
    "res = run(df)\n",
    "with open('results.pkl', 'wb') as output:\n",
    "    pickle.dump(res, output, 2, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
