{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercises for Spark Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "Excercise1",
     "locked": true,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write a function called `mapcos` that has a single paramater: an RDD of numbers. Use `map` to return an RRD that that is the `cos()` (cosine) of the input.\n",
    "\n",
    "`mapcos(A)` should produce some output approximately like:\n",
    "    \n",
    "```\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "```\n",
    "`mapcos(A).collect` should produce:\n",
    "```\n",
    "    [1.0, 0.54030..., -0.41614...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mapcos(A):\n",
    "    # Write code that will perform the task outlined in exercise 1\n",
    "    return \"returns a spark RDD, so to get a list do: mapcos(A).collect() \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics1\n",
    "SparkBasics1.exercise1_1(pickleFile, mapcos ,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Excercise 2\n",
    "\n",
    "Write a function called `mapwords` that has a single paramater: an RDD of strings, and returns an RDD that contains a list of words for each string.\n",
    "\n",
    "`stringRDD=sc.parallelize([\"Spring quarter\", \"Learning spark basics\", \"Big data analytics with Spark\"])`\n",
    "\n",
    "`mapwords(stringRDD).collect()` \n",
    "```\n",
    "output: \n",
    "[['Spring', 'quarter'], ['Learning', 'spark', 'basics'], ['Big', 'data', 'analytics', 'with', 'Spark']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mapwords(stringRDD):\n",
    "    # Write code that will perform the task outlined in exercise 2\n",
    "    return \"return spark RDD, so to get a list run: mapwords(stringRDD).collect() \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "SparkBasics1.exercise1_2(pickleFile, mapwords, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "Write a function `getMax` that uses `reduce` to find the maximum number from a list of numbers. Your command should produce the following:\n",
    "\n",
    "`\n",
    "RDD=sc.parallelize([0,2,1])\n",
    "`\n",
    "\n",
    "`\n",
    "getMax(RDD)\n",
    "`\n",
    "\n",
    "```\n",
    "Output:  2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise 3: Teacher Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def getMax(C):\n",
    "    return C.reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "inputs = [ sc.parallelize([0,4,2,3,1]),\n",
    "           sc.parallelize([-3.2,-3.233,-3.1,-3.9]),\n",
    "           sc.parallelize([2,2,2,2,2,2]) ]\n",
    "\n",
    "Tester.GenPickle(getMax, inputs, pickleFile, \"ex3\", isRDD=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# after creating the code in the SparkBasics1_Teacher, we check that it works belows\n",
    "SparkBasics1_Teacher.exercise3(testPath, getMax, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Student Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def getMax(C):\n",
    "    # Write the code that will perform the task outlined in exercise 3\n",
    "    return \"return spark NUMBER here, example, to get a number run mapwords(C)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "SparkBasics1.exercise3(pickleFile, getMax, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "Write a function called `reducewords` that uses `reduce` to create a single string which is the concatenation of all the strings in stringRDD(with a space between each string). Example:\n",
    "\n",
    "\n",
    "`stringRDD=sc.parallelize([\"Spring quarter\", \"Learning spark basics\", \"Big data analytics with Spark\"])`\n",
    "\n",
    "`reducewords(stringRDD)`\n",
    "```\n",
    "Output: 'Spring quarter Learning spark basics Big data analytics with Spark'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Teacher Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def reducewords(A):\n",
    "    return A.reduce(lambda x,y: x+\" \"+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "inputs = [ sc.parallelize([\"Spring quarter\", \"Learning spark basics\", \"Big data analytics with Spark\"]),\n",
    "           sc.parallelize([\"Do not go gentle\", \"into that good night\", \"old age should burn and rave\"]),\n",
    "           sc.parallelize([\"do\",\"I dare disturb\",\"the universe\",\"there will be time there will be\",\"time\"]) ]\n",
    "\n",
    "Tester.GenPickle(reducewords, inputs, pickleFile, \"ex4\", isRDD=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "SparkBasics1_Teacher.exercise4(testPath, reducewords, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Student Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def reducewords(mapwords):\n",
    "    # This function should take in as it's input the previous mapwords function you wrote\n",
    "    # you should then apply 1 spark function to it to it so that you return a single list\n",
    "    return \"return something like: mapwords.somefunction(...)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "SparkBasics1.exercise4(pickleFile, reducewords, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercise 5\n",
    "\n",
    "Write a non-Spark function `maxFunc` that when called by the `reduce` command outputs the maximum element from a set of lists. example:\n",
    "\n",
    "\n",
    "`listRDD=sc.parallelize([[3,4],[2,1],[7,9]])`\n",
    "\n",
    "`listRDD.reduce(maxFunc)`\n",
    "\n",
    "```\n",
    "Output: [9]\n",
    "```\n",
    "     \n",
    "     (Note: The output is a list containing a single number rather than just a single number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Teacher Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def maxFunc(x,y):\n",
    "    return [max(x+y)]\n",
    "def func5_true(A):\n",
    "    return A.reduce(maxFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "inputs = [ sc.parallelize([[15,20],[21,14],[18,4,20]]),\n",
    "           sc.parallelize([[3,4,5,-3,19],[19.1],[7,-11]]),\n",
    "           sc.parallelize([[-3.2,-3.233,-3.9],[-4],[-3,-5]]) ]\n",
    "\n",
    "Tester.GenPickle(func5_true, inputs, pickleFile, \"ex5\", isRDD=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "SparkBasics1_Teacher.exercise5(testPath, maxFunc, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Student Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def maxFunc(x,y):\n",
    "    # x,y are lists of numbers\n",
    "    # write code here for exercise 5\n",
    "    return \"returns a list to be used possibly again by the reduce command\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "SparkBasics1.exercise5(pickleFile, maxFunc, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Spark Basics 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 1.\n",
    "Consider the following RDD\n",
    "    \n",
    "    listRDD=sc.parallelize([[3,4],[2,1],[7,9]]) \n",
    "    \n",
    "Find the sum of maximum numbers of all lists. Your output should be:\n",
    "\n",
    "    Output:  15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxSum(A):\n",
    "    # Write code that will perform the task outlined in exercise 1\n",
    "    return \"returns the sum of maximum numbers of all lists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkBasics2.exercise3_1(pickleFile, maxSum,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Write a `filter` command to output elements whose cosine is positive. On the input \n",
    "\n",
    "    RDD=sc.parallelize([0,2,1]):\n",
    "\n",
    "positiveCos(RDD) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "positiveCos(RDD).collect() should produce:\n",
    "\n",
    "     [0,1] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def positiveCos(A):\n",
    "    # Write code that will perform the task outlined in exercise 2\n",
    "    return \"returns a spark RDD, so to get a list do: positiveCos(A).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkBasics2.exercise3_2(pickleFile, SparkBasics2_Teacher.func_ex3_2,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "Write a `filter` command to output all words whose length is greater than or equal to 4. On the input: ` wordRDD=sc.parallelize(['this','is','the','best','mac','ever']) `\n",
    "\n",
    "longWords(RDD) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "longWords(RDD).collect() should produce:\n",
    "\n",
    "    ['this', 'best', 'ever']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longWords(A):\n",
    "    # Write code that will perform the task outlined in exercise 3\n",
    "    return \"returns a spark RDD, so to get a list do: longWords(A).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkBasics2.exercise3_3(pickleFile, longWords,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Write a `flatMap` command to collect all the elements from 1 to x for each element x in a list. On input `RDD=sc.parallelize([2,3,5])`\n",
    "\n",
    "collectRange(RDD) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "collectRange(RDD).collect() should produce:\n",
    "\n",
    "     [1, 2, 1, 2, 3, 1, 2, 3, 4, 5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collectRange(A):\n",
    "    # Write code that will perform the task outlined in exercise 4\n",
    "    return \"returns a spark RDD, so to get a list do: collectRange(A).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkBasics2.exercise3_4(pickleFile, collectRange,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Consider the following RDDs: \n",
    "\n",
    "` RDD1=sc.parallelize([\"spark basics\", \"big data analysis\", \"spring\"]) `\n",
    "\n",
    "` RDD2=sc.parallelize([\"spark using pyspark\", \"big data\"]) `\n",
    "\n",
    "Use the set operations for the following exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "transform1(RDD1,RDD2) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "transform1(RDD1,RDD2).collect() should produce:\n",
    "\n",
    "* ` ['spark', 'basics', 'big', 'data', 'analysis', 'spring', 'spark', 'using', 'pyspark', 'big', 'data'] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform1(RDD1,RDD2):\n",
    "    # Write code that will perform the task outlined in exercise 5\n",
    "    return \"returns a spark RDD, so to get a list do: transform1(RDD1,RDD2).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkBasics2.exercise3_5(pickleFile, transform1,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "transform2(RDD1,RDD2) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "transform2(RDD1,RDD2).collect() should produce:\n",
    "\n",
    "* ` ['data', 'big', 'spark'] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform2(RDD1,RDD2):\n",
    "    # Write code that will perform the task outlined in exercise 6\n",
    "    return \"returns a spark RDD, so to get a list do: transform2(RDD1,RDD2).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkBasics2.exercise3_6(pickleFile, transform2,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "transform3(RDD1,RDD2) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "transform3(RDD1,RDD2).collect() should produce:\n",
    "\n",
    "* ` ['spring', 'analysis', 'basics'] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform3(RDD1,RDD2):\n",
    "    # Write code that will perform the task outlined in exercise 7\n",
    "    return \"returns a spark RDD, so to get a list do: transform3(RDD1,RDD2).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkBasics2.exercise3_7(pickleFile, transform3,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 8\n",
    "\n",
    "transform4(RDD1,RDD2) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "transform4(RDD1,RDD2).collect() should produce:\n",
    "\n",
    "* ` [('spark', 'spark'), ('spark', 'using'), ('spark', 'pyspark'), ('basics', 'spark'), ('basics', 'using'), ('basics', 'pyspark'), ('spark', 'big'), ('spark', 'data'), ('basics', 'big'), ('basics', 'data'), ('big', 'spark'), ('big', 'using'), ('big', 'pyspark'), ('data', 'spark'), ('analysis', 'spark'), ('data', 'using'), ('data', 'pyspark'), ('analysis', 'using'), ('analysis', 'pyspark'), ('spring', 'spark'), ('spring', 'using'), ('spring', 'pyspark'), ('big', 'big'), ('big', 'data'), ('data', 'big'), ('analysis', 'big'), ('data', 'data'), ('analysis', 'data'), ('spring', 'big'), ('spring', 'data')]\n",
    " `    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform4(RDD1,RDD2):\n",
    "    # Write code that will perform the task outlined in exercise 8\n",
    "    return \"returns a spark RDD, so to get a list do: transform4(RDD1,RDD2).collect()\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
