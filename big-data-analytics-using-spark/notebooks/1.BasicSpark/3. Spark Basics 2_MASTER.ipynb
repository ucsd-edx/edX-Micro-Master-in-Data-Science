{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Basics 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combining operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The method `map` takes as input an RDD and returns an RDD. Similarly the method `reduce` takes as input an RDD and returns a single element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can combine map and reduce operations to perform more complex operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we want to compute the sum of the squares\n",
    "$$ \\sum_{i=1}^n x_i^2 $$\n",
    "where the elements $x_i$ are stored in an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Traditional syntax: \n",
    "* perform the map\n",
    "* store the intermediate result in a variable\n",
    "* perform the reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "testPath = '/'.join(os.getcwd().split('/')[:-1]) + \"/Tester\"\n",
    "sys.path.insert(0, testPath )\n",
    "pickleFile = testPath+ \"/SparkBasics2.pkl\"\n",
    "\n",
    "import SparkBasics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Teacher Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we generate the student's test code below, we can check that it works by running the student's tests\n",
    "import SparkBasics2_MASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SparkBasics2_MASTER.gen_exercise3_1(pickleFile, sc)\n",
    "SparkBasics2_MASTER.gen_exercise3_2(pickleFile, sc)\n",
    "SparkBasics2_MASTER.gen_exercise3_3(pickleFile, sc)\n",
    "SparkBasics2_MASTER.gen_exercise3_4(pickleFile, sc)\n",
    "SparkBasics2_MASTER.gen_exercise3_5(pickleFile, sc)\n",
    "SparkBasics2_MASTER.gen_exercise3_6(pickleFile, sc)\n",
    "SparkBasics2_MASTER.gen_exercise3_7(pickleFile, sc)\n",
    "SparkBasics2_MASTER.gen_exercise3_8(pickleFile, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SparkBasics2_MASTER.exercise3_1(pickleFile, SparkBasics2_MASTER.func_ex3_1,sc)\n",
    "SparkBasics2_MASTER.exercise3_2(pickleFile, SparkBasics2_MASTER.func_ex3_2,sc)\n",
    "SparkBasics2_MASTER.exercise3_3(pickleFile, SparkBasics2_MASTER.func_ex3_3,sc)\n",
    "SparkBasics2_MASTER.exercise3_4(pickleFile, SparkBasics2_MASTER.func_ex3_4,sc)\n",
    "SparkBasics2_MASTER.exercise3_5(pickleFile, SparkBasics2_MASTER.func_ex3_5,sc)\n",
    "SparkBasics2_MASTER.exercise3_6(pickleFile, SparkBasics2_MASTER.func_ex3_6,sc)\n",
    "SparkBasics2_MASTER.exercise3_7(pickleFile, SparkBasics2_MASTER.func_ex3_7,sc)\n",
    "SparkBasics2_MASTER.exercise3_8(pickleFile, SparkBasics2_MASTER.func_ex3_8,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B=sc.parallelize(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#separate commands\n",
    "Squares=B.map(lambda x:x*x)\n",
    "Squares.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or we can combine them into a single cascaded command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#cascaded commands\n",
    "B.map(lambda x:x*x)\\\n",
    "   .reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "These two expressions are equivalent, and we might expect that the more basic one is the first, where the commands \n",
    "are separate, and that the python compiler translates the cascaded commands into machine code that corresponds to the separate commands.\n",
    "\n",
    "It turns out that the opposite is true, it is the cascaded form that is closer to the machine code, and spark identifies cascading operations even when they are expressed in a non-cascaded way.\n",
    "\n",
    "The explanation of this surprising behaviour is related to the notion of lazy evaluation in scala and is explained in [spark programming guide/RDD operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An instructive mistake\n",
    "Here is another way to compute the sum of the squares using a single reduce command. What is wrong with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "C=sc.parallelize([1,1,1])\n",
    "C.reduce(lambda x,y: x*x+y*y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise 1:\n",
    "Consider the following RDD\n",
    "    \n",
    "    listRDD=sc.parallelize([[3,4],[2,1],[7,9]]) \n",
    "    \n",
    "Find the sum of maximum numbers of all lists. Your output should be:\n",
    "\n",
    "    Output:  15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def maxSum(A):\n",
    "    # Write code that will perform the task outlined in exercise 1\n",
    "    return \"returns the sum of maximum numbers of all lists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics2\n",
    "SparkBasics2.exercise3_1(pickleFile, maxSum,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### getting information about an RDD\n",
    "RDD's typically have hundreds of thousands of elements. It usually makes no sense to print out the content of a whole RDD. Here are some ways to get manageable amounts of information about an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n=1000000\n",
    "B=sc.parallelize([0,0,1,0]*(n/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#find the number of elements in the RDD\n",
    "B.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# get the first few elements of an RDD\n",
    "print 'first element=',B.first()\n",
    "print 'first 5 elements = ',B.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sampling an RDD\n",
    "* RDDs are often very large.\n",
    "* Aggregates, such as averages, can be approximated efficiently by using a sample.\n",
    "* Sampling is done in parallel and it keeps the data local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The method `RDD.sample(withReplacement,p)` generates a sample of the elements of the RDD. where\n",
    "- `withReplacement` is a boolean flag indicating whether or not a an element in the RDD can be sampled more than once.\n",
    "- `p` is the probability of accepting each element into the sample. Note that as the sampling is performed independently in each partition, the number of elements in the sample changes from sample to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get a sample whose expected size is m\n",
    "m=5.\n",
    "B.sample(False,m/n).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#compute the average exactly:\n",
    "exact=B.reduce(lambda x,y:x+y)/(n+0.0)\n",
    "print 'exact average', exact\n",
    "#compute the average by sampling 1% of the elements\n",
    "p=0.01\n",
    "approx=B.sample(False,p).reduce(lambda x,y:x+y)/(n*p)\n",
    "print 'approximate average',approx\n",
    "print 'error=',approx-exact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Things to note and think about\n",
    "* Each time you run the previous cell, you get a different estimate\n",
    "* The accuracy of the estimate is determined by the size of the sample $n*p$\n",
    "* See how the error changes as you vary $p$\n",
    "* Can you give a formula that relates the variance of the estimate to $(p*n)$ ? (The answer is in the Probability and statistics course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### filtering an RDD\n",
    "The method `RDD.filter(func)` Return a new dataset formed by selecting those elements of the source on which func returns true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# How many positive numbers?\n",
    "B.filter(lambda n: n > 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Write a `filter` command to output elements whose cosine is positive. On the input \n",
    "\n",
    "    RDD=sc.parallelize([0,2,1]):\n",
    "\n",
    "positiveCos(RDD) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "positiveCos(RDD).collect() should produce:\n",
    "\n",
    "     [0,1] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "math.floor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SparkBasics2.exercise3_1(someshit, shomeshit2, shomeshit3):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def positiveCos(A):\n",
    "    # Write code that will perform the task outlined in exercise 2\n",
    "    return \"returns a spark RDD, so to get a list do: positiveCos(A).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics2\n",
    "SparkBasics2.exercise3_2(pickleFile, SparkBasics2_Teacher.func_ex3_2,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "Write a `filter` command to output all words whose length is greater than or equal to 4. On the input: ` wordRDD=sc.parallelize(['this','is','the','best','mac','ever']) `\n",
    "\n",
    "longWords(RDD) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "longWords(RDD).collect() should produce:\n",
    "\n",
    "    ['this', 'best', 'ever']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def longWords(A):\n",
    "    # Write code that will perform the task outlined in exercise 3\n",
    "    return \"returns a spark RDD, so to get a list do: longWords(A).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics2\n",
    "SparkBasics2.exercise3_3(pickleFile, longWords,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Remove duplicate elements in RDD\n",
    "The method `RDD.distinct(numPartitions=None)` Returns a new dataset that contains the distinct elements of the source dataset \n",
    "\n",
    "* The number of partitions is specified through the **numPartitions** argument. Each of this partitions is potentially on different machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate element in DuplicateRDD, we get distinct RDD\n",
    "DuplicateRDD = sc.parallelize([1,1,2,2,3,3])\n",
    "DistinctRDD = DuplicateRDD.distinct()\n",
    "DistinctRDD.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### flatmap an RDD\n",
    "The method `RDD.flatMap(func)` is similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text=[\"you are my sunshine\",\"my only sunshine\"]\n",
    "text_file = sc.parallelize(text)\n",
    "# map each line in text to a list of words\n",
    "print 'map:',text_file.map(lambda line: line.split(\" \")).collect()\n",
    "# create a single list of words by combining the words from all of the lines\n",
    "print 'flatmap:',text_file.flatMap(lambda line: line.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Write a `flatMap` command to collect all the elements from 1 to x for each element x in a list. On input `RDD=sc.parallelize([2,3,5])`\n",
    "\n",
    "collectRange(RDD) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "collectRange(RDD).collect() should produce:\n",
    "\n",
    "     [1, 2, 1, 2, 3, 1, 2, 3, 4, 5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def collectRange(A):\n",
    "    # Write code that will perform the task outlined in exercise 4\n",
    "    return \"returns a spark RDD, so to get a list do: collectRange(A).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics2\n",
    "SparkBasics2.exercise3_4(pickleFile, collectRange,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Set operations\n",
    "In this part, we explore set operations including **union**,**intersection**,**subtract**, **cartesian** in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1, 1, 2, 3])\n",
    "rdd2 = sc.parallelize([1, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. union(other)\n",
    " * Return the union of this RDD and another one.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " rdd1.union(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. intersection(other)\n",
    " * Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.Note that this method performs a shuffle internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. subtract(other, numPartitions=None)\n",
    " * Return each value in self that is not contained in other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd1.subtract(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. cartesian(other)\n",
    " * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where **a** is in **self** and **b** is in **other**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print rdd1.cartesian(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise \n",
    "\n",
    "Consider the following RDDs: \n",
    "\n",
    "` RDD1=sc.parallelize([\"spark basics\", \"big data analysis\", \"spring\"]) `\n",
    "\n",
    "` RDD2=sc.parallelize([\"spark using pyspark\", \"big data\"]) `\n",
    "\n",
    "Use the set operations for the following exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 5\n",
    "\n",
    "transform1(RDD1,RDD2) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "transform1(RDD1,RDD2).collect() should produce:\n",
    "\n",
    "* ` ['spark', 'basics', 'big', 'data', 'analysis', 'spring', 'spark', 'using', 'pyspark', 'big', 'data'] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform1(RDD1,RDD2):\n",
    "    # Write code that will perform the task outlined in exercise 5\n",
    "    return \"returns a spark RDD, so to get a list do: transform1(RDD1,RDD2).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics2\n",
    "SparkBasics2.exercise3_5(pickleFile, transform1,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 6\n",
    "\n",
    "transform2(RDD1,RDD2) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "transform2(RDD1,RDD2).collect() should produce:\n",
    "\n",
    "* ` ['data', 'big', 'spark'] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform2(RDD1,RDD2):\n",
    "    # Write code that will perform the task outlined in exercise 6\n",
    "    return \"returns a spark RDD, so to get a list do: transform2(RDD1,RDD2).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics2\n",
    "SparkBasics2.exercise3_6(pickleFile, transform2,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 7\n",
    "\n",
    "transform3(RDD1,RDD2) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "transform3(RDD1,RDD2).collect() should produce:\n",
    "\n",
    "* ` ['spring', 'analysis', 'basics'] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform3(RDD1,RDD2):\n",
    "    # Write code that will perform the task outlined in exercise 7\n",
    "    return \"returns a spark RDD, so to get a list do: transform3(RDD1,RDD2).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_7",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics2\n",
    "SparkBasics2.exercise3_7(pickleFile, transform3,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Exercise 8\n",
    "\n",
    "transform4(RDD1,RDD2) should produce some output approximately like:\n",
    "\n",
    "    PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "transform4(RDD1,RDD2).collect() should produce:\n",
    "\n",
    "* ` [('spark', 'spark'), ('spark', 'using'), ('spark', 'pyspark'), ('basics', 'spark'), ('basics', 'using'), ('basics', 'pyspark'), ('spark', 'big'), ('spark', 'data'), ('basics', 'big'), ('basics', 'data'), ('big', 'spark'), ('big', 'using'), ('big', 'pyspark'), ('data', 'spark'), ('analysis', 'spark'), ('data', 'using'), ('data', 'pyspark'), ('analysis', 'using'), ('analysis', 'pyspark'), ('spring', 'spark'), ('spring', 'using'), ('spring', 'pyspark'), ('big', 'big'), ('big', 'data'), ('data', 'big'), ('analysis', 'big'), ('data', 'data'), ('analysis', 'data'), ('spring', 'big'), ('spring', 'data')]\n",
    " `    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform4(RDD1,RDD2):\n",
    "    # Write code that will perform the task outlined in exercise 8\n",
    "    return \"returns a spark RDD, so to get a list do: transform4(RDD1,RDD2).collect()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import SparkBasics2\n",
    "SparkBasics2.exercise3_8(pickleFile, transform4,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
