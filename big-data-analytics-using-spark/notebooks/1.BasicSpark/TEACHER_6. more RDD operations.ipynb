{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of spark operations\n",
    "\n",
    "There are Three types of operations on RDDs: Transformations, Actions and Shuffles.\n",
    "\n",
    "* The most expensive operations are those the require communication between nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Transformations:** RDD $\\to$ RDD.\n",
    "  * **Examples** map, filter, sample, [More](http://spark.apache.org/docs/latest/programming-guide.html#transformations)\n",
    "  * **No** communication needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Actions:** RDD $\\to$ Python-object in head node.\n",
    "  * **Examples:** reduce, collect, count, take, [More](http://spark.apache.org/docs/latest/programming-guide.html#actions)\n",
    "  * **Some** communication needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Shuffles:** RDD $\\to$ RDD, **shuffle** needed\n",
    "  * **Examples:** repartition, sortByKey, reduceByKey, join [More](http://spark.apache.org/docs/latest/programming-guide.html#shuffle-operations)\n",
    "  * **A LOT** of communication needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key/value pairs\n",
    "\n",
    "* A python dictionary is a collection of *key/value* pairs.\n",
    "* The **key** is used to find a set of pairs with the particular key.\n",
    "* The **value** can be anything.\n",
    "* Spark has a set of special operations for *(key,value)* RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Spark provides specific functions to deal with RDDs in which each element is a key/value pair. Key/value RDDs expose new operations (e.g. aggregating and grouping together data with the same key and grouping together two different RDDs.) Such RDDs are also called pair RDDs. **In python, each element of a pair RDD is a pair tuple.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#start the SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating (key,value) RDDS\n",
    "\n",
    "**Method 1:** `parallelize` a list of pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pair_rdd = sc.parallelize([(1,2), (3,4)])\n",
    "print pair_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Method 2:** `map()` a function that returns a key/value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "regular_rdd = sc.parallelize([1, 2, 3, 4, 2, 5, 6])\n",
    "pair_rdd = regular_rdd.map( lambda x: (x, x*x) )\n",
    "print pair_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For this first exercise, we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million **nework interactions**. First, download and read the gzip file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")\n",
    "data_file = \"./kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "print raw_data.count()\n",
    "print raw_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise 1\n",
    "##### create a function: \n",
    "```python\n",
    "def csv_to_kv(raw_data):\n",
    "    \\\\ raw_data: an RDD of lines as defined above.\n",
    "    return KV\n",
    "    \\\\ Key-Value RDD\n",
    "```\n",
    "Map each row in `raw_data` to a key-value pair where the key is the last element in the raw (network interaction type). and the value is a list of all of the elements in the row. Return the resulting RDD. Example:\n",
    "\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "csv_to_kv(raw_data).take(1)\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "`(u'normal.', [u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.'])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_kv(raw_data):\n",
    "    return raw_data.map(lambda x: (x.split(\",\")[-1], x.split(\",\")) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "r1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "testPath = '/'.join(os.getcwd().split('/')[:-1]) + \"/Tester\"\n",
    "sys.path.insert(0, testPath )\n",
    "\n",
    "from miniTester import isSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sys.stdout.write( \"Checking Spark Format: \" )\n",
    "initDebugStr = ' '.join(raw_data.toDebugString().split(' ')[1:])\n",
    "newDebugStr  = csv_to_kv(raw_data).toDebugString()\n",
    "assert initDebugStr in newDebugStr\n",
    "sys.stdout.write(\"Correct!\\n\")\n",
    "\n",
    "sys.stdout.write(\"Checking Problem: \")\n",
    "assert csv_to_kv(raw_data).take(10) == [(u'normal.', [u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'239', u'486', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'19', u'19', u'1.00', u'0.00', u'0.05', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'235', u'1337', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'29', u'29', u'1.00', u'0.00', u'0.03', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'219', u'1337', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'6', u'6', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'39', u'39', u'1.00', u'0.00', u'0.03', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'217', u'2032', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'6', u'6', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'49', u'49', u'1.00', u'0.00', u'0.02', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'217', u'2032', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'6', u'6', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'59', u'59', u'1.00', u'0.00', u'0.02', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'212', u'1940', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'1', u'2', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'1.00', u'1', u'69', u'1.00', u'0.00', u'1.00', u'0.04', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'159', u'4087', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'5', u'5', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'11', u'79', u'1.00', u'0.00', u'0.09', u'0.04', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'210', u'151', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'8', u'89', u'1.00', u'0.00', u'0.12', u'0.04', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']), (u'normal.', [u'0', u'tcp', u'http', u'SF', u'212', u'786', u'0', u'0', u'0', u'1', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'8', u'99', u'1.00', u'0.00', u'0.12', u'0.05', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.'])]\n",
    "sys.stdout.write(\"Correct!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some important Key-Value Transformations\n",
    "#### 1. reduceByKey(func): Apply the reduce function on the values with the same key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print \"Original RDD :\", rdd.collect()\n",
    "print \"After transformation : \", rdd.reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that although it is similar to the reduce function, it is implemented as a transformation and not as an action because the dataset can have very large number of keys. So, it does not return values to the driver program. Instead, it returns a new RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. sortByKey(): \n",
    "Sort RDD by keys in ascending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(2,2), (1,4), (3,6)])\n",
    "print \"Original RDD :\", rdd.collect()\n",
    "print \"After transformation : \", rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note:** The output of sortByKey() is an RDD. This means that  RDDs do have a meaningful order, which extends between partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3. mapValues(func):\n",
    "Apply func to each value of RDD without changing the key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print \"Original RDD :\", rdd.collect()\n",
    "print \"After transformation : \", rdd.mapValues(lambda x: x*2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4. groupByKey(): \n",
    "Returns a new RDD of `(key,<iterator>)` pairs where the iterator iterates over the values associated with the key.\n",
    "\n",
    "[Iterators](http://anandology.com/python-practice-book/iterators.html) are python objects that generate a sequence of values. Writing a loop over `n` elements as \n",
    "```python\n",
    "for i in range(n):\n",
    "    ##do something\n",
    "```\n",
    "is inefficient because it first allocates a list of `n` elements and then iterates over it.\n",
    "Using the iterator `xrange(n)` achieves the same result without materializing the list. Instead, elements are generated on the fly.\n",
    "\n",
    "To materialize the list of values returned by an iterator we will use the list comprehension command:\n",
    "```python\n",
    "[a for a in <iterator>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print \"Original RDD :\", rdd.collect()\n",
    "print \"After transformation : \", rdd.groupByKey().mapValues(lambda x:[a for a in x]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise 2\n",
    "Continue with the function created in last exercise. Create a new function orderNet which returns the sorted network interaction types(with their total durations) which have the largest total durations. Duration is the first column of x, i.e. x[0].\n",
    "\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "RDD = csv_to_kv(raw_data)\n",
    "orderNet.take(5)\n",
    "\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "`\n",
    "[(u'normal.', 21075991.0), (u'portsweep.', 1991911.0), (u'warezclient.', 627563.0), (u'buffer_overflow.', 2751.0), (u'multihop.', 1288.0)]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orderedNet(RDD):\n",
    "    return RDD.mapValues(lambda x: int(x[0]) ) \\\n",
    "            .reduceByKey(lambda x,y: x+y)  \\\n",
    "            .map(lambda x: (x[1],x[0]) ) \\\n",
    "            .sortByKey(False) \\\n",
    "            .map(lambda x: (x[1],x[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Student Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from miniTester import isSpark\n",
    "\n",
    "RDD = csv_to_kv(raw_data)\n",
    "\n",
    "sys.stdout.write( \"Checking Spark Format: \" )\n",
    "initDebugStr = RDD.toDebugString() .split('|  ')[1:]\n",
    "newDebugStr  = orderedNet(RDD).toDebugString()\n",
    "assert all([phrase in newDebugStr for phrase in initDebugStr]) == True\n",
    "sys.stdout.write(\"Correct!\\n\")\n",
    "\n",
    "sys.stdout.write( \"Checking Problem: \" )\n",
    "assert orderedNet(RDD).take(50) == [(u'normal.', 21075991), (u'portsweep.', 1991911), (u'warezclient.', 627563), (u'buffer_overflow.', 2751), (u'multihop.', 1288), (u'rootkit.', 1008), (u'spy.', 636), (u'loadmodule.', 326), (u'warezmaster.', 301), (u'back.', 284), (u'ftp_write.', 259), (u'guess_passwd.', 144), (u'perl.', 124), (u'imap.', 72), (u'satan.', 64), (u'ipsweep.', 43), (u'phf.', 18), (u'nmap.', 0), (u'smurf.', 0), (u'pod.', 0), (u'neptune.', 0), (u'teardrop.', 0), (u'land.', 0)]\n",
    "sys.stdout.write(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5. flatMapValues(func): \n",
    "`func` is a function that takes as input a single value and returns an itrator that generates a sequence of values.\n",
    "The application of flatMapValues operates on a key/value RDD. It applies `func` to each value, and gets an list (generated by the iterator) of values. It then combines each of the values with the original key to produce a list of key-value pairs. These lists are concatenated as in `flatMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "print \"Original RDD :\", rdd.collect()\n",
    "# the lambda function generates for each number i, an iterator that produces i,i+1\n",
    "print \"After transformation : \", rdd.flatMapValues(lambda x: xrange(x,x+2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### (Advanced) 6. combineByKey(createCombiner, mergeValue, mergeCombiner): \n",
    "Combine values with the same key using a different result type.\n",
    "\n",
    "This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it. \n",
    "\n",
    "The elements of the original RDD are considered here *values*\n",
    "\n",
    "Values are converted into *combiners* which we will refer to here as \"accumulators\". An example of such a mapping is the mapping of the value *word* to the accumulator (*word*,1) that is done in WordCount.\n",
    "\n",
    "accumulators are then combined with values and the other combiner to generate a result for each key.\n",
    "\n",
    "For example, we can use it to calculate per-activity average durations as follows. Consider an RDD of key/value pairs where keys correspond to different activities and values correspond to duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"Sleep\", 7), (\"Work\",5), (\"Play\", 3), \n",
    "                      (\"Sleep\", 6), (\"Work\",4), (\"Play\", 4),\n",
    "                      (\"Sleep\", 8), (\"Work\",5), (\"Play\", 5)])\n",
    "\n",
    "sum_counts = rdd.combineByKey(\n",
    "    (lambda x: (x, 1)), # createCombiner maps each value into a  combiner (or accumulator)\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)),\n",
    "#mergeValue defines how to merge a accumulator with a value (saves on mapping each value to an accumulator first)\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "print sum_counts.collect()\n",
    "duration_means_by_activity = sum_counts.mapValues(lambda value:\n",
    "                                                  value[0]*1.0/value[1]) \\\n",
    "                                            .collect()\n",
    "print duration_means_by_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To understand combineByKey(), it’s useful to think of how it handles each element it processes. As combineByKey() traverses through the elements in a partition, each element either has a key it hasn’t seen before or has the same key as a previous element.\n",
    "\n",
    "If it’s a new key, createCombiner() is called to create the initial value for the accumulator on that key. In the above example, the accumulator is a tuple initialized as (x, 1) where x is a value in original RDD. Note that createCombiner() is called only when a key is seen for the first time in **each partition.**\n",
    "\n",
    "If it is a key we have seen before while processing that partition, it will instead use the provided function, mergeValue(), with the current value for the accumulator for that key and the new value.\n",
    "\n",
    "Since each partition is processed independently, we can have multiple accumulators for the same key. When we are merging the results from each partition, if two or more partitions have an accumulator for the same key, we merge the accumulators using the user-supplied mergeCombiners() function. In the above example, we are just adding the 2 accumulators element-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformations on two Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(1,2),(2,1),(2,2)])\n",
    "rdd2 = sc.parallelize([(2,5),(3,1)])\n",
    "a = rdd1.collect()\n",
    "b = rdd2.collect()\n",
    "print a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1. subtractByKey: \n",
    "Remove from RDD1 all elements whose key is present in RDD2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print \"RDD1:\", a\n",
    "print \"RDD2:\", b\n",
    "print \"Result:\", rdd1.subtractByKey(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. join: \n",
    "* A fundamental operation in relational databases.\n",
    "* assumes two tables have a **key** column in common. \n",
    "* merges rows with the same key.\n",
    "\n",
    "Suppose we have two `(key,value)` datasets \n",
    "\n",
    "\n",
    "|**dataset 1**|                                     |..........| **dataset 2** | \t       \t     |\n",
    "|-------------|-------------------------------------|   |-------------|-----------------|\n",
    "| **key=name**   |   **(gender,occupation,age)**    |   |  **key=name**   |   **hair color**    |\n",
    "| John   |  (male,cook,21)                          |   | Jill   |  blond |\n",
    "| Jill   |  (female,programmer,19)                  |   | Grace  |  brown |         \n",
    "| John   |  (male, kid, 2)                          |   | John   |  black |\n",
    "| Kate   |  (female, wrestler, 54)                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When `Join` is called on datasets of type `(Key, V)` and `(Key, W)`, it  returns a dataset of `(Key, (V, W))` pairs with all pairs of elements for each key. Joining the 2 datasets above yields:\n",
    "\n",
    "|   key = name | (gender,occupation,age),haircolor |\n",
    "|--------------|-----------------------------------|\n",
    "| John         | ((male,cook,21),black)             |\n",
    "| John         | ((male, kid, 2),black)             |\n",
    "| Jill         | ((female,programmer,19),blond)     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print \"RDD1:\", a\n",
    "print \"RDD2:\", b\n",
    "print \"Result:\", rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Variants of join.\n",
    "There are four variants of `join` which differ in how they treat keys that appear in one dataset but not the other.\n",
    "* `join` is an *inner* join which means that keys that appear only in one dataset are eliminated.\n",
    "* `leftOuterJoin` keeps all keys from the left dataset even if they don't appear in the right dataset. The result of leftOuterJoin in our example will contain the keys `John, Jill, Kate`\n",
    "* `rightOuterJoin` keeps all keys from the right dataset even if they don't appear in the left dataset. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John`\n",
    "* `FullOuterJoin` keeps all keys from both datasets. The result of leftOuterJoin in our example will contain the keys `Jill, Grace, John, Kate`\n",
    "\n",
    "In outer joins, if the element appears only in one dataset, the element in `(K,(V,W))` that does not appear in the dataset is represented bye `None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### 3. rightOuterJoin: \n",
    "Perform a right join between two RDDs. Every key in the right/second RDD will be present at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print \"RDD1:\", a\n",
    "print \"RDD2:\", b\n",
    "print \"Result:\", rdd1.rightOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### 4. leftOuterJoin: Perform a left join between two RDDs. Every key in the left RDD will be present at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print \"RDD1:\", a\n",
    "print \"RDD2:\", b\n",
    "print \"Result:\", rdd1.leftOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Actions on Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (2,4), (2,6)])\n",
    "a = rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1. countByKey(): Count the number of elements for each key. Returns a dictionary for easy access to keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print \"RDD: \", a\n",
    "result = rdd.countByKey()\n",
    "print \"Result:\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2. collectAsMap(): \n",
    "Collect the result as a dictionary to provide easy lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print \"RDD: \", a\n",
    "result = rdd.collectAsMap()\n",
    "print \"Result:\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 3. lookup(key): \n",
    "Return all values associated with the provided key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print \"RDD: \", a\n",
    "result = rdd.lookup(2)\n",
    "print \"Result:\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "Continue with the function created in exercise 2. Use any of the above transformations/actions to create a function, **avgDuration**, that calculates and returns the average duration for each of the network interaction types. Return the final dataset as a dictionary. You are encouraged to use **combineByKey()**.\n",
    "\n",
    "\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "RDD =  csv_to_kv(raw_data) \n",
    "avgDuration(RDD)\n",
    "\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "`\n",
    "{u'guess_passwd.': 2.717, u'nmap.': 0.0, u'loadmodule.': 36.222, u'rootkit.': 100.8, u'warezclient.': 615.258, u'smurf.': 0.0, u'pod.': 0.0, u'neptune.': 0.0, u'normal.': 216.657, u'spy.': 318.0, u'ftp_write.': 32.375, u'phf.': 4.5, u'portsweep.': 1915.299, u'teardrop.': 0.0, u'buffer_overflow.': 91.7, u'land.': 0.0, u'imap.': 6.0, u'warezmaster.': 15.05, u'perl.': 41.333, u'multihop.': 184.0, u'back.': 0.129, u'ipsweep.': 0.034, u'satan.': 0.04}\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RDD1 =  csv_to_kv(raw_data) \n",
    "RDD2 =  orderedNet( csv_to_kv(raw_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_counts = RDD1.combineByKey(\n",
    "    (lambda x: (x, 1)),\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)),\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) )\n",
    "\n",
    "duration_means_by_activity = sum_counts.mapValues(lambda value: value[0]*1.0/value[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "checkit = duration_means_by_activity.collectAsMap()\n",
    "print checkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RDD1.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RDD2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkit2 = csv_to_kv(raw_data)\n",
    "for i in checkit2.take(10):\n",
    "    print i\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
