{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting via nearest neighbor methods\n",
    "\n",
    "In this notebook we will look at the problem of predicting the digit that an image is supposed to represent from its pixel values. We will look at a particularly simple strategy for this problem known as the _nearest neighbor classifier_.\n",
    "\n",
    "\n",
    "To run this notebook you should have the following Python packages installed:\n",
    "* numpy\n",
    "* matplotlib\n",
    "* sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "`MNIST` is a classical dataset in machine learning, consisting of 28x28 gray-scale images handwritten digits. The original training set contains 60,000 examples and the test set contains 10,000 examples. In this notebook we will be working with smaller data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "\n",
    "## Load the training set\n",
    "train_data = np.load('MNIST/train_data.npy')\n",
    "train_labels = np.load('MNIST/train_labels.npy')\n",
    "\n",
    "## Load the testing set\n",
    "test_data = np.load('MNIST/test_data.npy')\n",
    "test_labels = np.load('MNIST/test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset dimensions:  (7500, 784) \n",
      "\n",
      "Number of training labels:  7500 \n",
      "\n",
      "Testing dataset dimensions:  (1000, 784) \n",
      "\n",
      "Number of testing labels:  1000\n"
     ]
    }
   ],
   "source": [
    "## Print out their dimensions\n",
    "print(\"Training dataset dimensions: \", np.shape(train_data), \"\\n\")\n",
    "print(\"Number of training labels: \", len(train_labels), \"\\n\")\n",
    "print(\"Testing dataset dimensions: \", np.shape(test_data), \"\\n\")\n",
    "print(\"Number of testing labels: \", len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "{0: 750, 1: 750, 2: 750, 3: 750, 4: 750, 5: 750, 6: 750, 7: 750, 8: 750, 9: 750} \n",
      "\n",
      "Test set distribution:\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100, 6: 100, 7: 100, 8: 100, 9: 100}\n"
     ]
    }
   ],
   "source": [
    "## Compute the number of examples of each digit\n",
    "train_digits, train_counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Training set distribution:\")\n",
    "print(dict(zip(train_digits, train_counts)), \"\\n\")\n",
    "\n",
    "test_digits, test_counts = np.unique(test_labels, return_counts=True)\n",
    "print(\"Test set distribution:\")\n",
    "print(dict(zip(test_digits, test_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "Each data point is stored as 784-dimensional vector. To visualize a data point, we need to reshape it to be a 28x28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define a function that displays a digit given its vector representation\n",
    "def show_digit(x):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "## Define a function that takes an index into a particular data set (\"train\" or \"test\") and displays that image.\n",
    "def vis_image(index, dataset=\"train\"):\n",
    "    if(dataset==\"train\"): \n",
    "        show_digit(train_data[index,])\n",
    "        label = train_labels[index]\n",
    "    else:\n",
    "        show_digit(test_data[index,])\n",
    "        label = test_labels[index]\n",
    "    print(\"Label \" + str(label))\n",
    "    return\n",
    "\n",
    "## View the first data point in the training set\n",
    "vis_image(0, \"train\")\n",
    "\n",
    "## Now view the first data point in the test set\n",
    "vis_image(0, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Computing squared Euclidean distance\n",
    "\n",
    "To compute nearest neighbors in our data set, we need to first be able to compute distances between data points. A natural distance function is _Euclidean distance_: for two vectors $x, y \\in \\mathbb{R}^d$, their Euclidean distance is defined as \n",
    "$$\\|x - y\\| = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.$$\n",
    "Often we omit the square root, and simply compute _squared Euclidean distance_:\n",
    "$$\\|x - y\\|^2 = \\sum_{i=1}^d (x_i - y_i)^2.$$\n",
    "For the purposes of nearest neighboor computations, the two are equivalent. For three vectors $x, y, z \\in \\mathbb{R}^d$, we have $\\|x - y\\| \\leq \\|x - z\\|$ if and only if $\\|x - y\\|^2 \\leq \\|x - z\\|^2$.\n",
    "\n",
    "Now we just need to be able to compute squared Euclidean distance. The following function does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Computes squared Euclidean distance between two vectors.\n",
    "def squared_dist(x,y):\n",
    "    return np.sum(np.square(x-y))\n",
    "\n",
    "## Compute distance between a seven and a one in our training set.\n",
    "print(\"Distance from 7 to 1: \", squared_dist(train_data[4,],train_data[5,]))\n",
    "\n",
    "## Compute distance between a seven and a two in our training set.\n",
    "print(\"Distance from 7 to 2: \", squared_dist(train_data[4,],train_data[1,]))\n",
    "\n",
    "## Compute distance between two seven's in our training set.\n",
    "print(\"Distance from 7 to 7: \", squared_dist(train_data[4,],train_data[7,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing nearest neighbors\n",
    "\n",
    "Now that we have a distance function defined, we can now turn to nearest neighbor classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Takes a vector x and returns the index of its nearest neighbor in train_data\n",
    "def find_NN(x):\n",
    "    # Compute distances from x to every row in train_data\n",
    "    distances = [squared_dist(x,train_data[i,]) for i in range(len(train_labels))]\n",
    "    # Get the index of the smallest distance\n",
    "    return np.argmin(distances)\n",
    "\n",
    "## Takes a vector x and returns the class of its nearest neighbor in train_data\n",
    "def NN_classifier(x):\n",
    "    # Get the index of the the nearest neighbor\n",
    "    index = find_NN(x)\n",
    "    # Return its class\n",
    "    return train_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A success case:\n",
    "print(\"A success case:\")\n",
    "print(\"NN classification: \", NN_classifier(test_data[0,]))\n",
    "print(\"True label: \", test_labels[0], \"\\n\")\n",
    "print(\"The test image:\")\n",
    "vis_image(0, \"test\")\n",
    "print(\"The corresponding nearest neighbor image:\")\n",
    "vis_image(find_NN(test_data[0,]), \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A failure case:\n",
    "print(\"A failure case:\")\n",
    "print(\"NN classification: \", NN_classifier(test_data[39,]))\n",
    "print(\"True label: \", test_labels[39])\n",
    "print(\"The test image:\")\n",
    "vis_image(39, \"test\")\n",
    "print(\"The corresponding nearest neighbor image:\")\n",
    "vis_image(find_NN(test_data[39,]), \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the full test set\n",
    "\n",
    "Now let's apply our nearest neighbor classifier over the full data set. \n",
    "\n",
    "Note that to classify each test point, our code takes a full pass over each of the 7500 training examples. Thus we should not expect testing to be very fast. The following code takes about 100-150 seconds on 2.6 GHz Intel Core i5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Predict on each test data point (and time it!)\n",
    "t_before = time.time()\n",
    "test_predictions = [NN_classifier(test_data[i,]) for i in range(len(test_labels))]\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute the error\n",
    "err_positions = np.not_equal(test_predictions, test_labels)\n",
    "error = np.sum(err_positions)/len(test_labels)\n",
    "\n",
    "print(\"Error of nearest neighbor classifier: \", error)\n",
    "print(\"Classification time: \", t_after - t_before, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster nearest neighbor methods\n",
    "\n",
    "Performing nearest neighbor classification in the way we have presented requires a full pass through the training set in order to classify a single point. Given $N$ training points and $M$ testing points in $\\mathcal{R}^d$, performing classification at test time takes $\\mathcal{O}(N M d)$ time.\n",
    "\n",
    "Fortunately, there are faster methods to perform nearest neighbor look up if we are willing to spend some time preprocessing the training set. `scikit-learn` has fast implementations of two useful nearest neighbor data structures: the _ball tree_ and the _KD tree_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "## Build nearest neighbor structure on training data\n",
    "t_before = time.time()\n",
    "ball_tree = BallTree(train_data)\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute training time\n",
    "t_training = t_after - t_before\n",
    "print(\"Training time: \", t_training)\n",
    "\n",
    "## Get nearest neighbor predictions on testing data\n",
    "t_before = time.time()\n",
    "test_neighbors = np.squeeze(ball_tree.query(test_data, k=1, return_distance=False))\n",
    "ball_tree_predictions = train_labels[test_neighbors]\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute testing time\n",
    "t_testing = t_after - t_before\n",
    "print(\"Testing time: \", t_testing)\n",
    "\n",
    "## Verify that the predictions are the same\n",
    "print(\"Ball tree produces same predictions as above? \", np.array_equal(test_predictions, ball_tree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "## Build nearest neighbor structure on training data\n",
    "t_before = time.time()\n",
    "kd_tree = KDTree(train_data)\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute training time\n",
    "t_training = t_after - t_before\n",
    "print(\"Training time: \", t_training)\n",
    "\n",
    "## Get nearest neighbor predictions on testing data\n",
    "t_before = time.time()\n",
    "test_neighbors = np.squeeze(kd_tree.query(test_data, k=1, return_distance=False))\n",
    "kd_tree_predictions = train_labels[test_neighbors]\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute testing time\n",
    "t_testing = t_after - t_before\n",
    "print(\"Testing time: \", t_testing)\n",
    "\n",
    "## Verify that the predictions are the same\n",
    "print(\"KD tree produces same predictions as above? \", np.array_equal(test_predictions, kd_tree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:3.4]",
   "language": "python",
   "name": "conda-env-3.4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
